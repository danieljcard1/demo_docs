---
title: "Wk 06: Tracking ngrams in TC Research"
---

## Overview

This week we explore the techniques used in Majdik (2019) and Graham (2021) to track ngrams in our dataset of TC articles. The techniques that follow are similar to the exploratory work that both authors advocate. That is, you might build on this work as you develop a more nuanced approach to your data. 

While we could use Quanteda or a number of other approaches, we'll draw on Chapter 4 of [Text Mining with R: A Tidy Approach](https://www.tidytextmining.com/ngrams). 

## Preparation

Load libraries

```{r}
library(tidyverse)

#install.packages("tidytext")
library(tidytext)
```

Load data

```{r}
raw_data <- read_csv("data/tc_journals.csv")

glimpse(raw_data)
```

## Exploring ngrams in title+abstracts

### Create a "text" column that is title and abstract combined

```{r}
raw_data$text <- paste(raw_data$article_title, raw_data$abstract, sep = " . ")

head(raw_data$text, n = 2)
```

### Break "text" into a list of bigrams

We use unnest_tokens from tidytext to break our "text" column into individual observations of bigrams.

Note that the "text" column will be replaced by a "bigram" column.

```{r}

tc_bigrams <- raw_data %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

```
Notice that we've transformed our 2002 articles into 227,304 bigrams. 

We can then count the most common bigrams in the dataset

```{r}
tc_bigrams %>%
  count(bigram, sort = TRUE)
```
### filter stopwords

```{r}
library(tidyr)

# transform "bigram" column into two columns: "word1" and "word2"
bigrams_separated <- tc_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# filter to keep only rows where both word1 and word2 are not stopwords
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)


# new bigram counts
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigram_counts


```
### Custom stopwords

```{r}
custom_stopwords <- c("copyright",
                      "sage",
                      "holder's",
                      "express",
                      "permission",
                      "download")

# add custom stopwords to filter process
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)%>%
  filter(!word1 %in% custom_stopwords) %>%
  filter(!word2 %in% custom_stopwords)

# new bigram counts
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)


bigram_counts
```


Now we can reunite our two columns into a single bigram column

```{r}
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```

### Trigrams (all at once)

```{r}
tc_trigrams <- raw_data %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  filter(!is.na(trigram)) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word,
         !word1 %in% custom_stopwords,
         !word2 %in% custom_stopwords,
         !word3 %in% custom_stopwords) %>%
  unite(trigram, word1, word2, word3, sep = " ")


tc_trigrams %>%
  count(trigram, sort = TRUE)
```

## Tracking Counts over time

### Filtering bigrams of interest

We can use `filter()` to retain ngrams of interest, which we can then visualize...

```{r}
# define bigram of interest
sj <-  "social justice"


# filter for presence of bigram
sj_bigrams <- bigrams_united %>%
  filter(bigram == sj)

# check result
glimpse(sj_bigrams)

# Calculate the frequency of the bigram by year
sj_bigram_freq <- sj_bigrams %>%
  group_by(publication_year) %>%
  summarize(frequency = n()) %>%
  ungroup()

# Plot the frequency of the bigram by year
ggplot(sj_bigram_freq, aes(x = publication_year, y = frequency)) +
  geom_line() +
  ggtitle("Frequency of 'social justice' Bigram by Year") +
  xlab("Publication Year") +
  ylab("Frequency") +
  scale_x_continuous(breaks = seq(min(sj_bigram_freq$publication_year), max(sj_bigram_freq$publication_year), by = 1))
```

We can also add other dimensions to the analysis, e.g. frequency by year and journal

```{r}

# Calculate the frequency of the bigram by year and journal
sj_bigram_freq <- sj_bigrams %>%
  group_by(publication_year, abbreviation) %>%
  summarize(frequency = n()) %>%
  ungroup()

# Plot the frequency of the "social justice" bigram by year and journal
ggplot(sj_bigram_freq, aes(x = publication_year, y = frequency)) +
  geom_line() +
  ggtitle("Frequency of 'social justice' Bigram by Year and Journal") +
  xlab("Publication Year") +
  ylab("Frequency") +
  facet_wrap(~abbreviation, scales = "free_x", ncol = 2)

```

And we can make that look a little nicer. 

```{r}
#install.packages("ggthemes")
library(ggthemes)

# Calculate the frequency of the bigram by year and journal
sj_bigram_freq <- sj_bigrams %>%
  group_by(publication_year, abbreviation) %>%
  summarize(frequency = n()) %>%
  ungroup()

# Plot the frequency of the "social justice" bigram by year and journal
ggplot(sj_bigram_freq, aes(x = publication_year, y = frequency, color = abbreviation)) +
  geom_line() +
  ggtitle("Frequency of 'social justice' by Year and Journal") +
  xlab("Publication Year") +
  ylab("Frequency") +
  scale_x_continuous(breaks = seq(min(sj_bigram_freq$publication_year), max(sj_bigram_freq$publication_year), by = 1)) +
  scale_y_continuous(breaks = seq(0, max(sj_bigram_freq$frequency), by = 2)) +
  theme_fivethirtyeight()

```
### From target bigram to more complex constructs

We can also create a list of bigrams as a proxy for a construct. 

```{r}

# List of bigrams you want to analyze
construct_list <- c("bigram1", "bigram2", "bigram3")  # Add your list of bigrams here

# Filter for the specific bigrams in the list
filtered_data <- bigrams_united %>%
  filter(bigram %in% construct_list)

# calculate frequency

# plot


```

Alternatively, if you wanted to track complex constructions a la Majdik you might use the [stringr package to develop some regular expressions](https://stringr.tidyverse.org/articles/regular-expressions.html).


## Some other exploratory analyses


### TF-IDF

We can use term frequency inverse document frequency (TF-IDF). Similar to "keyness" in Quanteda, tf-idf can help us identify ngrams that are distinctive of a particular subset of the corpus. 

```{r}
bigram_tf_idf <- bigrams_united %>%
  count(abbreviation, bigram) %>%
  bind_tf_idf(bigram, abbreviation, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```
### Bigram tf-idf, by journal 

```{r, fig.height=8}
library(ggplot2)

bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  group_by(abbreviation) %>%
  slice_max(tf_idf, n = 10) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(tf_idf, bigram, fill = abbreviation)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ abbreviation, ncol = 2, scales = "free") +
  labs(x = "tf-idf of bigram", y = NULL)
```
### Bigram tf-idf, by year

```{r, fig.height=12}
# get tf-idf by year
bigram_tf_idf_year <- bigrams_united %>%
  count(publication_year, bigram) %>%
  bind_tf_idf(bigram, publication_year, n) %>%
  arrange(desc(tf_idf))

# plot tf-idf by year
bigram_tf_idf_year %>%
  arrange(desc(tf_idf)) %>%
  group_by(publication_year) %>%
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(tf_idf, bigram, fill = publication_year)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ publication_year, ncol = 3, scales = "free") +
  labs(x = "tf-idf of bigram", y = NULL)
```

```{r}
library(igraph)

bigram_counts


bigram_graph <- bigram_counts %>%
  filter(n > 25) %>%
  graph_from_data_frame()

bigram_graph


```



```{r}
#install.packages("ggraph")
library(ggraph)
set.seed(999)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)


a <- grid::arrow(type = "closed", length = unit(.1, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.05, 'inches')) +
  geom_node_point(color = "lightblue", size = 3) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

