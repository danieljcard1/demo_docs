[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Rhetorics",
    "section": "",
    "text": "This site is for use in WRIT 8520: Computational Rhetorics. The code and documentation found here is offered as is with no guarantee of utility for anyone in any context. Enjoy!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 02: Exploring TC journals (pt 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 03: Exploring TC journals, (pt 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 03: Exploring TC journals, (pt 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 05: Titles in TC Journals, pt. 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 05: Titles in TC Journals, pt. 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 06: Tracking ngrams in TC Research\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 08: Sentiment Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 09: Network Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWk 09: Network Analysis, pt 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "demos/week09/wk09_networks_pt2.html",
    "href": "demos/week09/wk09_networks_pt2.html",
    "title": "Wk 09: Network Analysis, pt 2",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(bibliometrix)\nlibrary(stringr)\nRead in combined data\nCodeM &lt;- read_rds(\"data/comb.rds\")"
  },
  {
    "objectID": "demos/week09/wk09_networks_pt2.html#create-subsets-for-analysis",
    "href": "demos/week09/wk09_networks_pt2.html#create-subsets-for-analysis",
    "title": "Wk 09: Network Analysis, pt 2",
    "section": "Create subsets for analysis",
    "text": "Create subsets for analysis\nWe can manipulate the data here and then explore further with bibliometrix.\nHere is a list of WOS/Bibliometrix tags, for reference. Some key tags:\n\nPY: publication year\nTI: document title\nAU: authors\nDE: author-supplied keywords\nAB: abstract\nCR: list of cited references\nNR: count of cited references\n\nFilter articles by publication year\n\nCode# Only articles after 1992\n\npost92 &lt;- M %&gt;%\n  filter(PY &gt; 1992)\n\n\nFilter articles by “word” in title\n\nCodeword &lt;- \"ethics\"\n\nethics &lt;- M %&gt;%\n  filter(str_detect(TI, regex(word, ignore_case = TRUE)))\n\n\nFilter articles by one or more words in abstract\n\nCodesearch_terms &lt;- c(\"Aristotle\", \"Plato\", \"Socrates\")  # List of words to check for\n\n# create a regular expression pattern by pasting the search terms together and separating them with \\\\b|\\\\b. The \\\\b is a word boundary anchor, ensuring that we match whole words or phrases rather than substrings \npattern &lt;- paste0(\"\\\\b\", paste(search_terms, collapse = \"\\\\b|\\\\b\"), \"\\\\b\")\n\ngreeks &lt;- M %&gt;%\n  filter(str_detect(AB, regex(pattern, ignore_case = TRUE)))\n\ngreeks$AB\n\n[1] \"THIS ARTICLE USES THE CROSS-CULTURAL CONCEPTS OF CONTEXT AND TIME TO EXAMINE THE RHETORIC OF GERMAN UNIVERSITY STUDENTS IN AN ENGLISH BUSINESS WRITING COURSE. THIS PARTICIPANT-OBSERVER ACCOUNT, WHICH INCLUDES NUMEROUS STUDENT EXAMPLES AND OBSERVATIONS, PROVIDES AFRESH PERSPECTIVE FOR AMERICAN TEACHERS IN INCREASINGLY MULTINATIONAL, MULTICULTURAL CLASSROOMS. IT ALSO SUGGESTS HOW ARISTOTLE'S CONCEPTS OF ETHOS, LOGOS, AND PATHOS TOGETHER WITH THE CASE METHOD AND GROUP WORK CAN HELP TEACHERS RESPOND TO THE CHALLENGES IN SUCH CLASSROOMS. THE ARTICLE CONCLUDES BY SUGGESTING THAT UNDERSTANDING THE RHETORIC OF CULTURE IS AN IMPORTANT STEP IN ACCEPTING AND NEGOTIATING CULTURAL DIFFERENCES.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n[2] \"THIS REVIEW OF THE RELATIONSHIP OF LAW AND ART IN THE LITIGATIVE CONTEXT EXPLORES WAYS IN WHICH THE METHODOLOGIES OF THE NOVELIST AND OTHER ARTISTS CAN BE INVOKED BY THE LAWYER IN STRUCTURING AND DEVELOPING A CASE AND PRESENTING IT TO A COURT. TO THE LITIGATORS WHO TRANSCEND THE FORM BOOKS AND STEREOTYPES AND SEE THEIR CASES WITH AFRESH EYE, NEITHER THE LAW NOR THE FACTS ARE FIXED IN STONE BUT RATHER CREATED TO MEET THE DEEPEST REALITIES OF THE CASE WITHIN THE CONTEXT OF OUR MOST FUNDAMENTAL VALUES AND BELIEFS. LITIGATORS, BY THE WAY THEY DEFINE AND PROJECT THE ISSUES, CAN AFFECT, EVEN DETERMINE, WHAT LAW AND FACTS ARE LEGALLY RELEVANT AND DISPOSITIVE. THEY MUST DEVISE AND WRITE THE STORY THAT THREADS THE CLIENT'S WAY OUR OF THE LABYRINTH. MASTERY OF THE FORMAL REQUIREMENTS OF LITIGATIVE WRITING IS ONLY A NECESSARY FIRST STEP. FREEWRITING; HEMINGWAYESQUE CHOICE OF WORDS AND SYNTAX; HARNESSING THE SYMBOLIC, OFTEN HIDDEN, POWER OF LANGUAGE; ACHIEVING THE DRAMATIC POTENTIAL OF CASE PRESENTATION-ALL THESE AND MORE FROM THE CREATIVE ARTIST'S REPERTOIRE EMPOWER LITIGATORS TO WIN THEIR CASES. RESORT IS MADE NOT ONLY TO THE APPLICABLE STATUTORY, REGULATORY, AND CASE LAW BUT ALSO TO THE PROCESSES OF THE LIKE OF CEZANNE, CONRAD, HEMINGWAY, TOLSTOY, JOYCE, ARISTOTLE, AND FAULKNER.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n[3] \"THE REPORT FOR DECISION MAKING SHARES SOME COMMON GROUND WITH THE PROPOSAL, THE REPORT OF SCIENTIFIC EXPERIMENT, AND EVEN THE PERSUASIVE ESSAY, YET THESE GENRES DIFFER. RECOGNIZING THESE DIFFERENCES IS NECESSARY FOR EFFECTIVE INQUIRY, PEDAGOGY, AND DECISION MAKING. THE GENRES ARE MEANS OF SOLVING DIFFERENT TYPES OF PROBLEMS: PRACTICAL, EMPIRICAL, AND THEORETICAL. THEY SERVE DIFFERENT AIMS: ACTION, DEMONSTRATION, AND CONVICTION. THE PROPOSAL, LIKE THE REPORT, MAY SOLVE PRACTICAL PROBLEMS, BUT THE PROPOSAL ADVOCATES, WHEREAS THE REPORT INQUIRES. THESE GENRES ALL EMBODY ASSUMPTIONS ABOUT PROBLEM SOLVING AND INQUIRY IN THEIR FORMS. APPLYING THE PROBLEM-SOLVING GOALS AND METHODS OF THE PROPOSAL, EXPERIMENTAL REPORT, OR ESSAY TO THE REPORT FOR DECISION-MAKING COMPROMISES THE QUALITY OF THE INQUIRY AND OF THE RESULTING DECISION. COMPLEX PROBLEMS FOR DECISION MAKING REQUIRE A RHETORICAL METHOD OF INQUIRY BASED ON ARISTOTLE'S SPECIAL TOPICS. THE REPORT GENRE REFLECTS THE INVENTION HEURISTICS AND ANALYSIS IN ITS FORM.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n[4] \"PURPOSE: TO IDENTIFY THE MAIN RHETORICAL TECHNIQUES ACTUALLY USED TO SECURE INVESTORS' SUPPORT IN SOME OF THE MOST SUCCESSFUL (MOST-FUNDED) WEB-BASED CROWD FUNDING PROJECTS. THE STUDY SERVES TO BRIDGE THE GAP BETWEEN THEORETICAL RESEARCH OF RHETORIC AND THE NEEDS OF BUSINESS COMMUNICATION PRACTITIONERS BY IDENTIFYING THE MEANS OF PERSUASION THAT CAN BE USED BY ONLINE CROWD FUNDING ENTREPRENEURS. METHOD: QUALITATIVE ANALYSIS OF THIRTEEN CROWD FUNDING PROJECT DESCRIPTIONS POSTED ON A MAJOR WEB SITE-WWW.KICKSTARTER.COM-WAS PERFORMED TO IDENTIFY SPECIFIC RHETORICAL TECHNIQUES VIA TEXT CODING. THE SAMPLE INCLUDED THE MOST-FUNDED PROJECTS TO DATE, ONE FROM EACH OF THE THIRTEEN PROJECT CATEGORIES ON KICKSTARTER. ARISTOTLE'S CONCEPT OF ETHOS, PATHOS, AND LOGOS SERVED AS A BASIC FRAMEWORK FOR DEVELOPING A MORE DETAILED CLASSIFICATION OF RHETORICAL MEANS OF PERSUASION USED IN THE PROJECTS. RESULTS: THE MOST-FUNDED PROJECTS HAVE BEEN FOUND TO CONTAIN ALL THREE TYPES OF RHETORICAL APPEALS (ETHOS, PATHOS, AND LOGOS), SUBDIVIDED INTO A TOTAL OF TWELVE SPECIFIC SUBTYPES MOST COMMONLY ENCOUNTERED IN THE DESCRIPTIONS FROM THE SAMPLE. THE SUBTYPE DEFINITIONS HAVE BEEN DEVELOPED AND REFINED OVER THE COURSE OF SEVERAL REVIEWS. CONCLUSION: THE RESEARCH DATA MADE IT POSSIBLE TO CREATE A ``RHETORICAL PROFILE'' OF A SUCCESSFUL CROWD FUNDING PROJECT DESCRIPTION REPRESENTING A SUMMARY OF THE RHETORICAL TECHNIQUES IDENTIFIED DURING THE STUDY. ALTHOUGH THIS SUMMARY REFLECTS A HYPOTHETICAL ALL-INCLUSIVE CASE, IT CAN BE USED AS A BENCHMARK WHEN DRAFTING CROWD FUNDING PROJECT DESCRIPTIONS. THE STUDY ALSO IDENTIFIED SPECIFIC DIRECTIONS FOR FUTURE RESEARCH THAT COULD DETERMINE THE INFLUENCE OF PROJECT DESCRIPTION RHETORIC ON DONOR DECISIONS.\"                                                                                                                                            \n[5] \"PURPOSE: I ARGUE THAT EMOTIONAL APPEALS, PREVALENT IN CHARTS AND GRAPHS DURING THE LATER NINETEENTH CENTURY BUT LARGELY DORMANT SINCE THEN, HAVE RAPIDLY RE-EMERGED IN CONTEMPORARY DATA VISUALIZATION. CHANGING THE RELATIONSHIP BETWEEN DESIGNER AND USER, THIS NEW FORM OF DATA DESIGN HAS INTENSIFIED THE AFFECTIVE IMPACT OF DATA DISPLAYS BY ELICITING EMOTIONS RANGING FROM EXCITEMENT AND EMPATHY TO ANXIETY AND FEAR. METHODS: THIS ARTICLE DRAWS ON HISTORICAL AND CONTEMPORARY SOURCES TO BUILD ITS CASE. IT GIVES AN OVERVIEW OF EMOTIONAL APPEALS IN THE RHETORICAL TRADITION, FROM ARISTOTLE TO MODERN THEORISTS LIKE GEORGE CAMPBELL, WHO EMPHASIZED SENSORY RESPONSES THROUGH PERSONALIZATION AND PROXIMITY. THE ARTICLE PROVIDES AN HISTORICAL OVERVIEW OF PATHOS APPEALS IN DATA DESIGN DURING THE LATER NINETEENTH CENTURY AND THE SHIFT TO MODERNIST MINIMALISM IN THE TWENTIETH CENTURY. CONTEMPORARY EXAMPLES FROM COMPANIES, NONPROFITS, GOVERNMENT AGENCIES, AND INDIVIDUAL DESIGNERS ILLUSTRATE HOW DATA VISUALIZATION AROUSES EMOTION. RESULTS: EMOTIONAL APPEALS DURING THE NINETEENTH CENTURY FOCUSED PRIMARILY ON COLOR AND DESIGN NOVELTY, WHICH, BY APPEALING LARGELY TO THE SENSES, FOSTERED EMOTIONAL RESPONSES SUCH AS EXCITEMENT AND CURIOSITY. CONTEMPORARY DATA VISUALIZATION MAKES SIMILAR EMOTIONAL APPEALS THROUGH THE USE OF COLOR, NOVELTY, AND MULTIMODAL FEATURES; HOWEVER, DIGITAL TECHNOLOGY ALSO ALLOWS DESIGNERS TO APPEAL TO THE EMOTIONS BY PERSONALIZING DISPLAYS THROUGH INTERACTIVITY, SPATIAL AND TEMPORAL PROXIMITY, AND AESTHETIC AND EXPRESSIVE ELEMENTS. CONCLUSION: PATHOS (EMOTIONAL) APPEALS HAVE BECOME AN INTEGRAL PART OF CONTEMPORARY DATA VISUALIZATION, LARGELY BECAUSE OF THE MULTIMODAL AND INTERACTIVE AFFORDANCES OF DIGITAL TECHNOLOGY. DESIGNERS WHO UNDERSTAND THIS DIMENSION OF DATA DESIGN CAN DEPLOY TECHNOLOGY TO MAKE THEIR DISPLAYS MORE ENGAGING, HUMANE, AND USABLE.\"\n[6] \"ANALOGICAL REASONING HAS LONG BEEN AN IMPORTANT TOOL IN THE PRODUCTION OF SCIENTIFIC KNOWLEDGE, YET MANY SCIENTISTS REMAIN HESITANT TO FULLY ENDORSE (OR EVEN ADMIT) ITS USE. AS THE TEACHERS OF SCIENTIFIC AND TECHNICAL WRITERS, WE HAVE AN OPPORTUNITY AND RESPONSIBILITY TO TEACH THEM TO USE ANALOGY WITHOUT THEIR WRITING BECOMING ``OVERLY INDUCTIVE,'' AS ARISTOTLE WARNED. TO THAT END, I HERE OFFER AN ANALYSIS OF AN EXAMPLE OF THE EFFECTIVE USE OF ANALOGY IN RODNEY BROOKS'S ``INTELLIGENCE WITHOUT REPRESENTATION.'' IN THIS ARTICLE, BROOKS PROVIDES A MODEL FOR INCORPORATING THESE TOOLS INTO AN ARGUMENT BY BUILDING FOUR OF THEM INTO AN ENTHYMEME THAT CLEARLY ORGANIZES HIS ARGUMENT. THIS COMBINATION OF INDUCTIVE AND DEDUCTIVE REASONING HELPED THE ARTICLE BECOME A VERY INFLUENTIAL PIECE OF SCHOLARSHIP IN ARTIFICIAL INTELLIGENCE RESEARCH, AND IT CAN HELP OUR STUDENTS LEARN TO USE ANALOGY IN THEIR OWN WRITING.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n\n\nFilter articles that contain one of the keywords in either title or abstract\n\nCodesearch_words &lt;- c(\"Intersectionality\", \"Intersectional\")\n\n# Filter the data frame to select rows with at least one word from the list in column1 or column2\nintersectional &lt;- M %&gt;% \n  rowwise() %&gt;%\n  filter(any(str_detect(c(TI, AB), regex(paste(search_words, collapse = \"|\"), ignore_case = TRUE))))\n\nintersectional$TI\n\n [1] \"BUILDING TOWARD MORE JUST DATA PRACTICES\"                                                                                                          \n [2] \"PRIORITIZING ACCESS AS A SOCIAL JUSTICE CONCERN: ADVOCATING FOR ABLEISM STUDIES AND DISABILITY JUSTICE IN TECHNICAL AND PROFESSIONAL COMMUNICATION\"\n [3] \"PLAIN LANGUAGE TO MINIMIZE COGNITIVE LOAD: A SOCIAL JUSTICE PERSPECTIVE\"                                                                           \n [4] \"LIVING TESTIMONIOS: HOW LATINX GRADUATE STUDENTS PERSIST AND ENACT SOCIAL JUSTICE WITHIN HIGHER EDUCATION\"                                         \n [5] \"EMBODYING PUBLIC FEMINISMS: COLLABORATIVE INTERSECTIONAL MODELS FOR ENGAGEMENT\"                                                                    \n [6] \"FEMINIST DIGITAL RESEARCH METHODOLOGY FOR RHETORICIANS OF HEALTH AND MEDICINE\"                                                                     \n [7] \"``CHANGING THE FACE OF TECHNOLOGY'': STORYTELLING AS INTERSECTIONAL FEMINIST PRACTICE IN CODING ORGANIZATIONS\"                                     \n [8] \"INCREASING INCLUSION IN TECHNICAL COMMUNICATION ACADEMIC PROGRAMS\"                                                                                 \n [9] \"RISKING DISCLOSURE: UNRULY RHETORICS AND QUEER(ING) HIV RISK COMMUNICATION ON GRINDR\"                                                              \n[10] \"SUPERDIVERSITY: AN AUDIENCE ANALYSIS PRAXIS FOR ENACTING SOCIAL JUSTICE IN TECHNICAL COMMUNICATION\"                                                \n[11] \"HEALTH AND WELLNESS AS RESISTANCE: TACTICAL FOLK MEDICINE\""
  },
  {
    "objectID": "demos/week09/wk09_networks_pt2.html#explore-new-data-in-bibliometrix",
    "href": "demos/week09/wk09_networks_pt2.html#explore-new-data-in-bibliometrix",
    "title": "Wk 09: Network Analysis, pt 2",
    "section": "Explore new data in bibliometrix",
    "text": "Explore new data in bibliometrix\nTake the result of your filtering and analyze it!\nStock descriptive analysis\n\nCode# run a stock analysis (generates a list of dataframes)\nresults &lt;- biblioAnalysis(intersectional, sep = \";\")\n\n# create a summary of the results\noptions(width=100)\nS &lt;- summary(object = results, k = 10, pause = FALSE)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              2017 : 2023 \n Sources (Journals, Books, etc)        4 \n Documents                             11 \n Annual Growth Rate %                  0 \n Document Average Age                  2 \n Average citations per doc             2.182 \n Average citations per year per doc    0.5476 \n References                            569 \n \nDOCUMENT TYPES                     \n article      11 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    28 \n Author's Keywords (DE)                74 \n \nAUTHORS\n Authors                               17 \n Author Appearances                    17 \n Authors of single-authored docs       7 \n \nAUTHORS COLLABORATION\n Single-authored docs                  7 \n Documents per Author                  0.647 \n Co-Authors per Doc                    1.55 \n International co-authorships %        0 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    2017        1\n    2018        1\n    2021        3\n    2022        5\n    2023        1\n\nAnnual Percentage Growth Rate 0 \n\n\nMost Productive Authors\n\n   Authors        Articles Authors        Articles Fractionalized\n1   ALEXANDER JJ         1  BENNETT KC                        1.0\n2   BENNETT KC           1  CARDINAL A                        1.0\n3   CARDINAL A           1  CHEUNG IW                         1.0\n4   CARLSON EB           1  DAYLEY C                          1.0\n5   CHEUNG IW            1  DE HERTOGH LB                     1.0\n6   DAVIS C              1  GREEN MK                          1.0\n7   DAYLEY C             1  REA EA                            1.0\n8   DE HERTOGH LB        1  ALEXANDER JJ                      0.5\n9   DELEON RL            1  CARLSON EB                        0.5\n10  EDENFIELD AC         1  DELEON RL                         0.5\n\n\nTop manuscripts per citations\n\n                                 Paper                                    DOI TC TCperYear   NTC\n1  DE HERTOGH LB, 2018, J. Bus. Tech. Commun.   10.1177/1050651918780188       7     1.167 1.000\n2  CHEUNG IW, 2017, IEEE Trans. Prof. Commun.   10.1109/TPC.2017.2759639       6     0.857 1.000\n3  ALEXANDER JJ, 2021, Tech. Commun. Q.         10.1080/10572252.2021.1930181  5     1.667 1.667\n4  GREEN MK, 2021, Tech. Commun. Q.             10.1080/10572252.2021.1930185  3     1.000 1.000\n5  BENNETT KC, 2022, IEEE Trans. Prof. Commun.  10.1109/TPC.2022.3140570       1     0.500 2.500\n6  MCKOY T, 2022, IEEE Trans. Prof. Commun.     10.1109/TPC.2022.3143352       1     0.500 2.500\n7  REA EA, 2021, Tech. Commun.                  NA                             1     0.333 0.333\n8  GOUGE CC, 2022, IEEE Trans. Prof. Commun.    10.1109/TPC.2021.3137675       0     0.000 0.000\n9  PHILLIPS LL, 2022, IEEE Trans. Prof. Commun. 10.1109/TPC.2022.3140569       0     0.000 0.000\n10 DAYLEY C, 2023, Tech. Commun.                10.55177/tc963195              0     0.000   NaN\n\n\nMost Relevant Sources\n\n                                   Sources        Articles\n1 IEEE TRANSACTIONS ON PROFESSIONAL COMMUNICATION        5\n2 TECHNICAL COMMUNICATION QUARTERLY                      3\n3 TECHNICAL COMMUNICATION                                2\n4 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION        1\n\n\nMost Relevant Keywords\n\n      Author Keywords (DE)      Articles  Keywords-Plus (ID)     Articles\n1  SOCIAL JUSTICE                      6 RACE                           4\n2  CULTURAL DIFFERENCES                3 TECHNICAL COMMUNICATION        3\n3  STATISTICS                          3 DESIGN                         2\n4  COMMUNICATION                       2 AIDS                           1\n5  EDUCATION                           2 BIG DATA                       1\n6  ETHICS                              2 CARE                           1\n7  GLOBAL COMMUNICATION                2 CHALLENGES                     1\n8  HEALTH/MEDICAL COMMUNICATION        2 CHOICE                         1\n9  RESEARCH METHODS                    2 COMMUNICATION                  1\n10 SOCIOLOGY                           2 DISCRIMINATION                 1\n\n\n\nCode#plot(x = results, k = 10, pause = FALSE)\n\n\nCitation analysis\nWhen writing about intersectionality, who do TC authors cite?\nMost cited articles\n\nCode# Get citations\nCR &lt;- citations(intersectional, field = \"article\", sep = \";\")\n\n# Top 50 most cited articles\ncbind(CR$Cited[1:50])\n\n                                                                                                                                                [,1]\nJONES NN, 2016, TECH COMMUN Q, V25, P211, DOI 10.1080/10572252.2016.1224655                                                                        7\nAGBOKA G. Y., 2014, J TECHNICAL WRITING, V44, P297                                                                                                 5\nANONYMOUS, 2011, PROGRAMMATIC PERSPEC                                                                                                              5\nWALTON R., 2019, TECHNICAL COMMUNICAT                                                                                                              5\nHAAS AM, 2012, J BUS TECH COMMUN, V26, P277, DOI 10.1177/1050651912439539                                                                          4\nSHELTON C, 2020, TECH COMMUN Q, V29, P18, DOI 10.1080/10572252.2019.1640287                                                                        4\nWILLIAMS M.F., 2014, COMMUNICATING RACE E                                                                                                          4\nAGBOKA GY, 2013, TECH COMMUN Q, V22, P28, DOI 10.1080/10572252.2013.730966                                                                         3\nJONES N. N., 2016, J TECH WRIT COMMUN, V46, P342, DOI DOI 10.1177/0047281616639472                                                                 3\nSALDA├A┬▒A J., 2016, CODING MANUAL QUALIT                                                                                                          3\nSHIVERS-MCNAIR A, 2017, TECH COMMUN-STC, V64, P97                                                                                                  3\nACHARYA, 2018, P 36 ACM INT C DES C, P1, DOI 10.1145/3233756.3233960, DOI 10.1145/3233756.3233960                                                  2\nAGBOKA G., 2012, J TECH WRIT COMMUN, V42, P159, DOI DOI 10.2190/TW.42.2.E                                                                          2\nALBERS M., 2003, J TECH WRIT COMMUN, V33, P263, DOI DOI 10.2190/6KJN-95QV-JMD3-E5EE                                                                2\nANONYMOUS, 2010, RHETORICA MOTION FEM                                                                                                              2\nANONYMOUS, 2012, FEMINIST RHETORICAL                                                                                                               2\nANONYMOUS, 2014, PROGRAMMATIC PERSPEC                                                                                                              2\nANONYMOUS, METHODOLOGIES RHETOR                                                                                                                    2\nCOLLINS PH, 2021, CONTEMP POLIT THEORY, V20, P690, DOI 10.1057/S41296-021-00490-0                                                                  2\nCRENSHAW K., 1989, UNIV CHICAGO LEG FOR, V140, P139                                                                                                2\nCRENSHAW K., 1989, UNIV CHICAGO LEG FOR, V1989, P139, DOI DOI 10.4324/9780429500480-5                                                              2\nDAYLEY C, 2020, TECH COMMUN Q, V29, P49, DOI 10.1080/10572252.2019.1635210                                                                         2\nEDENFIELD AC, 2019, TECH COMMUN Q, V28, P177, DOI 10.1080/10572252.2019.1607906                                                                    2\nEDENFIELD AC., 2019, J TECH WRIT COMMUN, V49, P433, DOI DOI 10.1177/0047281619871211                                                               2\nFROST EA, 2016, J BUS TECH COMMUN, V30, P3, DOI 10.1177/1050651915602295                                                                           2\nHAAS AM, 2018, KEY THEORETICAL FRAMEWORKS: TEACHING TECHNICAL COMMUNICATION IN THE TWENTY-FIRST CENTURY, P3, DOI 10.7330/9781607327585.C000        2\nJONES N. N., 2016, J TECH WRIT COMMUN, V46, P471, DOI 10.1177/0047281616653489, DOI 10.1177/0047281616653489                                       2\nJONES NN, 2017, BUS PROF COMMUN Q, V80, P6, DOI 10.1177/2329490616680360                                                                           2\nJONES NN, 2018, KEY THEORETICAL FRAMEWORKS: TEACHING TECHNICAL COMMUNICATION IN THE TWENTY-FIRST CENTURY, P241, DOI 10.7330/9781607327585.C010     2\nMILLER CR, 1979, COLL ENGL, V40, P610, DOI 10.2307/375964                                                                                          2\nMOORE KR, 2018, KEY THEORETICAL FRAMEWORKS: TEACHING TECHNICAL COMMUNICATION IN THE TWENTY-FIRST CENTURY, P185, DOI 10.7330/9781607327585.C008     2\nROSE EMMA, 2018, COMMUNICATION DESIGN QUARTERLY REVIEW, V6, P9, DOI 10.1145/3282665.3282667                                                        2\nSCOTT J. BLAKE, 2003, RISKY RHETORIC AIDS                                                                                                          2\nSHIVERS-MCNAIR ANN, 2019, COMPUTERS AND COMPOSITION, V51, P43, DOI 10.1016/J.COMPCOM.2018.11.005                                                   2\nWALTON R, 2019, J TECH WRIT COMMUN                                                                                                                 2\nWALTON R., 2016, J TECH WRIT COMMUN, V60, P402, DOI DOI 10.1177/0047281616653496                                                                   2\n*NAT BIOETH ADV CO, 2001, ETH POL ISS RES INV                                                                                                      1\nACEVEDO-GIL N, 2017, RACE ETHNIC EDUC-UK, V20, P829, DOI 10.1080/13613324.2017.1343294                                                             1\nAGARWAL P, 2020, FORBES                                                                                                                            1\nAGBOKA G. Y., 2013, CONNEXIONS, V1, P29                                                                                                            1\nAGBOKA G. Y., 2020, IEEE PROFESSIONAL CO                                                                                                           1\nAGBOKA G. Y., 2020, J TECH WRIT COMMUN, DOI 10.1177/0047281620901484, DOI 10.1177/0047281620901484                                                 1\nAGBOKA G. Y., 2020, J TECH WRIT COMMUN, V34, P159                                                                                                  1\nAGBOKA GY, 2018, KEY THEORETICAL FRAMEWORKS: TEACHING TECHNICAL COMMUNICATION IN THE TWENTY-FIRST CENTURY, P114, DOI 10.7330/9781607327585.C005    1\nAHMED AA, 2018, INTERACT COMPUT, V30, P53, DOI 10.1093/IWC/IWX018                                                                                  1\nAJANA B, 2017, DIGIT HEALTH, V3, DOI 10.1177/2055207616689509                                                                                      1\nALCOFF L, 1988, SIGNS, V13, P405, DOI 10.1086/494426                                                                                               1\nALFREY L, 2017, GENDER SOC, V31, P28, DOI 10.1177/0891243216680590                                                                                 1\nALLIANCE FOR BOARD DIVERSITY, 2017, MISS PIEC REP 2016 B                                                                                           1\nALLYN B., 2020, NPR                                                                                                                                1\n\n\nMost cited first authors\n\nCodeCR &lt;- citations(intersectional, field = \"author\", sep = \";\")\n\ncbind(CR$Cited[1:20])\n\n                 [,1]\nANONYMOUS          51\nJONES NN           14\nWALTON R           10\nAGBOKA G Y          9\nCRENSHAW K          8\nHAAS AM             7\nGONZALES L          5\nJONES N N           5\nSHELTON C           5\nAGBOKA GY           4\nEDENFIELD AC        4\nMOORE KR            4\nWILLIAMS M F        4\nCDC                 3\nCOLLINS PH          3\nFROST EA            3\nSALDA A A J         3\nSCOTT J BLAKE       3\nSHIVERS MCNAIR A    3\nACHARYA             2"
  },
  {
    "objectID": "demos/week06/wk06_tracking-frames.html",
    "href": "demos/week06/wk06_tracking-frames.html",
    "title": "Wk 06: Tracking ngrams in TC Research",
    "section": "",
    "text": "This week we explore the techniques used in Majdik (2019) and Graham (2021) to track ngrams in our dataset of TC articles. The techniques that follow are similar to the exploratory work that both authors advocate. That is, you might build on this work as you develop a more nuanced approach to your data.\nWhile we could use Quanteda or a number of other approaches, we’ll draw on Chapter 4 of Text Mining with R: A Tidy Approach."
  },
  {
    "objectID": "demos/week06/wk06_tracking-frames.html#overview",
    "href": "demos/week06/wk06_tracking-frames.html#overview",
    "title": "Wk 06: Tracking ngrams in TC Research",
    "section": "",
    "text": "This week we explore the techniques used in Majdik (2019) and Graham (2021) to track ngrams in our dataset of TC articles. The techniques that follow are similar to the exploratory work that both authors advocate. That is, you might build on this work as you develop a more nuanced approach to your data.\nWhile we could use Quanteda or a number of other approaches, we’ll draw on Chapter 4 of Text Mining with R: A Tidy Approach."
  },
  {
    "objectID": "demos/week06/wk06_tracking-frames.html#preparation",
    "href": "demos/week06/wk06_tracking-frames.html#preparation",
    "title": "Wk 06: Tracking ngrams in TC Research",
    "section": "Preparation",
    "text": "Preparation\nLoad libraries\n\nCodelibrary(tidyverse)\n\n#install.packages(\"tidytext\")\nlibrary(tidytext)\n\n\nLoad data\n\nCoderaw_data &lt;- read_csv(\"data/tc_journals.csv\")\n\nglimpse(raw_data)\n\nRows: 2,002\nColumns: 6\n$ source_title      &lt;chr&gt; \"JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\", \"…\n$ author_full_names &lt;chr&gt; \"Wickman, Chad\", \"DeJeu, Emily Barrow\", \"DeVasto, Da…\n$ article_title     &lt;chr&gt; \"Genre and Metagenre in Biomedical Research Writing\"…\n$ abstract          &lt;chr&gt; \"The use of reporting guidelines is an established y…\n$ publication_year  &lt;dbl&gt; 2023, 2022, 2016, 2009, 2009, 2020, 2016, 2012, 2011…\n$ abbreviation      &lt;chr&gt; \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBT…"
  },
  {
    "objectID": "demos/week06/wk06_tracking-frames.html#exploring-ngrams-in-titleabstracts",
    "href": "demos/week06/wk06_tracking-frames.html#exploring-ngrams-in-titleabstracts",
    "title": "Wk 06: Tracking ngrams in TC Research",
    "section": "Exploring ngrams in title+abstracts",
    "text": "Exploring ngrams in title+abstracts\nCreate a “text” column that is title and abstract combined\n\nCoderaw_data$text &lt;- paste(raw_data$article_title, raw_data$abstract, sep = \" . \")\n\nhead(raw_data$text, n = 2)\n\n[1] \"Genre and Metagenre in Biomedical Research Writing . The use of reporting guidelines is an established yet still-evolving practice in the field of biomedicine. These documents are often linked to common methodologies (e.g., randomized clinical trials); they include multiple textual artifacts (e.g., checklists, flow diagrams) and have a history that is coextensive with the emergence and ongoing development of evidence-based medicine (e.g., as an epistemological orientation to research and decision making). Drawing on the concept of metagenre, this article examines how practitioners use reporting guidelines to define and regulate the boundaries of biomedical research and writing activity. The analysis, focusing on one prominent set of guidelines, shows how practitioners use the genre-metagenre dynamic to promote strategic intervention while upholding traditional principles and standards for evidence-based research and communication.\"\n[2] \"The Ethics of Delivering Bad News: Evaluating Impression Management Strategies in Corporate Financial Reporting . Business communication textbooks offer impression management (IM) strategies to help students learn how to soften bad news. But corporations sometimes use these strategies in ethically questionable ways. This article analyzes IM strategies in a landmark case of ethically dubious corporate financial reporting. Findings suggest that the company, Ivax, manipulated three standard IM strategies by overamplifying its power to fix a financial crisis, substantially downplaying bad news, and concealing damaging information. Ivax also used a fourth, less familiar strategy: It buried contradictory information in legal disclaimers. Instructors need to help students become ethical writers who avoid questionable IM strategies like these.\"                                                                                                 \n\n\nBreak “text” into a list of bigrams\nWe use unnest_tokens from tidytext to break our “text” column into individual observations of bigrams.\nNote that the “text” column will be replaced by a “bigram” column.\n\nCodetc_bigrams &lt;- raw_data %&gt;%\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\n\nNotice that we’ve transformed our 2002 articles into 227,304 bigrams.\nWe can then count the most common bigrams in the dataset\n\nCodetc_bigrams %&gt;%\n  count(bigram, sort = TRUE)\n\n# A tibble: 105,848 × 2\n   bigram                      n\n   &lt;chr&gt;                   &lt;int&gt;\n 1 of the                   1232\n 2 technical communication  1197\n 3 in the                    960\n 4 this article              883\n 5 of technical              570\n 6 and the                   512\n 7 as a                      435\n 8 technical writing         427\n 9 to the                    424\n10 on the                    395\n# ℹ 105,838 more rows\n\n\nfilter stopwords\n\nCodelibrary(tidyr)\n\n# transform \"bigram\" column into two columns: \"word1\" and \"word2\"\nbigrams_separated &lt;- tc_bigrams %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")\n\n# filter to keep only rows where both word1 and word2 are not stopwords\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\n\n# new bigram counts\nbigram_counts &lt;- bigrams_filtered %&gt;%\n  count(word1, word2, sort = TRUE)\n\nbigram_counts\n\n# A tibble: 39,418 × 3\n   word1        word2             n\n   &lt;chr&gt;        &lt;chr&gt;         &lt;int&gt;\n 1 technical    communication  1197\n 2 technical    writing         427\n 3 technical    communicators   380\n 4 author       copyright       248\n 5 professional communication   241\n 6 writing      communication   237\n 7 sage         publications    212\n 8 multiple     sites           182\n 9 copyright    holder's        155\n10 holder's     express         146\n# ℹ 39,408 more rows\n\n\nCustom stopwords\n\nCodecustom_stopwords &lt;- c(\"copyright\",\n                      \"sage\",\n                      \"holder's\",\n                      \"express\",\n                      \"permission\",\n                      \"download\")\n\n# add custom stopwords to filter process\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)%&gt;%\n  filter(!word1 %in% custom_stopwords) %&gt;%\n  filter(!word2 %in% custom_stopwords)\n\n# new bigram counts\nbigram_counts &lt;- bigrams_filtered %&gt;%\n  count(word1, word2, sort = TRUE)\n\n\nbigram_counts\n\n# A tibble: 39,374 × 3\n   word1        word2             n\n   &lt;chr&gt;        &lt;chr&gt;         &lt;int&gt;\n 1 technical    communication  1197\n 2 technical    writing         427\n 3 technical    communicators   380\n 4 professional communication   241\n 5 writing      communication   237\n 6 multiple     sites           182\n 7 social       media           135\n 8 user         experience      102\n 9 social       justice          94\n10 email        articles         92\n# ℹ 39,364 more rows\n\n\nNow we can reunite our two columns into a single bigram column\n\nCodebigrams_united &lt;- bigrams_filtered %&gt;%\n  unite(bigram, word1, word2, sep = \" \")\n\nbigrams_united\n\n# A tibble: 58,563 × 7\n   source_title        author_full_names article_title abstract publication_year\n   &lt;chr&gt;               &lt;chr&gt;             &lt;chr&gt;         &lt;chr&gt;               &lt;dbl&gt;\n 1 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n 2 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n 3 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n 4 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n 5 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n 6 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n 7 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n 8 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n 9 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n10 JOURNAL OF BUSINES… Wickman, Chad     Genre and Me… The use…             2023\n# ℹ 58,553 more rows\n# ℹ 2 more variables: abbreviation &lt;chr&gt;, bigram &lt;chr&gt;\n\n\nTrigrams (all at once)\n\nCodetc_trigrams &lt;- raw_data %&gt;%\n  unnest_tokens(trigram, text, token = \"ngrams\", n = 3) %&gt;%\n  filter(!is.na(trigram)) %&gt;%\n  separate(trigram, c(\"word1\", \"word2\", \"word3\"), sep = \" \") %&gt;%\n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word,\n         !word3 %in% stop_words$word,\n         !word1 %in% custom_stopwords,\n         !word2 %in% custom_stopwords,\n         !word3 %in% custom_stopwords) %&gt;%\n  unite(trigram, word1, word2, word3, sep = \" \")\n\n\ntc_trigrams %&gt;%\n  count(trigram, sort = TRUE)\n\n# A tibble: 22,185 × 2\n   trigram                               n\n   &lt;chr&gt;                             &lt;int&gt;\n 1 technical writing communication     235\n 2 professional communication tpc       72\n 3 user centered design                 37\n 4 original published version           31\n 5 tactical technical communication     31\n 6 technical communication classroom    30\n 7 technical communication research     29\n 8 technical communication programs     24\n 9 covid 19 pandemic                    22\n10 user experience design               20\n# ℹ 22,175 more rows"
  },
  {
    "objectID": "demos/week06/wk06_tracking-frames.html#tracking-counts-over-time",
    "href": "demos/week06/wk06_tracking-frames.html#tracking-counts-over-time",
    "title": "Wk 06: Tracking ngrams in TC Research",
    "section": "Tracking Counts over time",
    "text": "Tracking Counts over time\nFiltering bigrams of interest\nWe can use filter() to retain ngrams of interest, which we can then visualize…\n\nCode# define bigram of interest\nsj &lt;-  \"social justice\"\n\n\n# filter for presence of bigram\nsj_bigrams &lt;- bigrams_united %&gt;%\n  filter(bigram == sj)\n\n# check result\nglimpse(sj_bigrams)\n\nRows: 94\nColumns: 7\n$ source_title      &lt;chr&gt; \"JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\", \"…\n$ author_full_names &lt;chr&gt; \"Frost, Erin A.\", \"Petersen, Emily January; Walton, …\n$ article_title     &lt;chr&gt; \"Apparent Feminism as a Methodology for Technical Co…\n$ abstract          &lt;chr&gt; \"This article introduces apparent feminism, which is…\n$ publication_year  &lt;dbl&gt; 2016, 2018, 2018, 2018, 2018, 2018, 2022, 2022, 2022…\n$ abbreviation      &lt;chr&gt; \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBT…\n$ bigram            &lt;chr&gt; \"social justice\", \"social justice\", \"social justice\"…\n\nCode# Calculate the frequency of the bigram by year\nsj_bigram_freq &lt;- sj_bigrams %&gt;%\n  group_by(publication_year) %&gt;%\n  summarize(frequency = n()) %&gt;%\n  ungroup()\n\n# Plot the frequency of the bigram by year\nggplot(sj_bigram_freq, aes(x = publication_year, y = frequency)) +\n  geom_line() +\n  ggtitle(\"Frequency of 'social justice' Bigram by Year\") +\n  xlab(\"Publication Year\") +\n  ylab(\"Frequency\") +\n  scale_x_continuous(breaks = seq(min(sj_bigram_freq$publication_year), max(sj_bigram_freq$publication_year), by = 1))\n\n\n\n\nWe can also add other dimensions to the analysis, e.g. frequency by year and journal\n\nCode# Calculate the frequency of the bigram by year and journal\nsj_bigram_freq &lt;- sj_bigrams %&gt;%\n  group_by(publication_year, abbreviation) %&gt;%\n  summarize(frequency = n()) %&gt;%\n  ungroup()\n\n# Plot the frequency of the \"social justice\" bigram by year and journal\nggplot(sj_bigram_freq, aes(x = publication_year, y = frequency)) +\n  geom_line() +\n  ggtitle(\"Frequency of 'social justice' Bigram by Year and Journal\") +\n  xlab(\"Publication Year\") +\n  ylab(\"Frequency\") +\n  facet_wrap(~abbreviation, scales = \"free_x\", ncol = 2)\n\n\n\n\nAnd we can make that look a little nicer.\n\nCode#install.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n# Calculate the frequency of the bigram by year and journal\nsj_bigram_freq &lt;- sj_bigrams %&gt;%\n  group_by(publication_year, abbreviation) %&gt;%\n  summarize(frequency = n()) %&gt;%\n  ungroup()\n\n# Plot the frequency of the \"social justice\" bigram by year and journal\nggplot(sj_bigram_freq, aes(x = publication_year, y = frequency, color = abbreviation)) +\n  geom_line() +\n  ggtitle(\"Frequency of 'social justice' by Year and Journal\") +\n  xlab(\"Publication Year\") +\n  ylab(\"Frequency\") +\n  scale_x_continuous(breaks = seq(min(sj_bigram_freq$publication_year), max(sj_bigram_freq$publication_year), by = 1)) +\n  scale_y_continuous(breaks = seq(0, max(sj_bigram_freq$frequency), by = 2)) +\n  theme_fivethirtyeight()\n\n\n\n\nFrom target bigram to more complex constructs\nWe can also create a list of bigrams as a proxy for a construct.\n\nCode# List of bigrams you want to analyze\nconstruct_list &lt;- c(\"bigram1\", \"bigram2\", \"bigram3\")  # Add your list of bigrams here\n\n# Filter for the specific bigrams in the list\nfiltered_data &lt;- bigrams_united %&gt;%\n  filter(bigram %in% construct_list)\n\n# calculate frequency\n\n# plot\n\n\nAlternatively, if you wanted to track complex constructions a la Majdik you might use the stringr package to develop some regular expressions."
  },
  {
    "objectID": "demos/week06/wk06_tracking-frames.html#some-other-exploratory-analyses",
    "href": "demos/week06/wk06_tracking-frames.html#some-other-exploratory-analyses",
    "title": "Wk 06: Tracking ngrams in TC Research",
    "section": "Some other exploratory analyses",
    "text": "Some other exploratory analyses\nTF-IDF\nWe can use term frequency inverse document frequency (TF-IDF). Similar to “keyness” in Quanteda, tf-idf can help us identify ngrams that are distinctive of a particular subset of the corpus.\n\nCodebigram_tf_idf &lt;- bigrams_united %&gt;%\n  count(abbreviation, bigram) %&gt;%\n  bind_tf_idf(bigram, abbreviation, n) %&gt;%\n  arrange(desc(tf_idf))\n\nbigram_tf_idf\n\n# A tibble: 43,753 × 6\n   abbreviation bigram                      n      tf   idf  tf_idf\n   &lt;chr&gt;        &lt;chr&gt;                   &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 JTWC         multiple sites            182 0.0135  1.61  0.0217 \n 2 JTWC         writing communication     235 0.0174  0.916 0.0160 \n 3 JTWC         email articles             92 0.00683 1.61  0.0110 \n 4 JTWC         copy users                 46 0.00342 1.61  0.00550\n 5 JTWC         original published         36 0.00267 1.61  0.00430\n 6 JTWC         editor's desk              31 0.00230 1.61  0.00370\n 7 JTWC         published version          31 0.00230 1.61  0.00370\n 8 CDQ          communication design       60 0.00624 0.511 0.00319\n 9 CDQ          communication designers    19 0.00197 1.61  0.00318\n10 CDQ          university press           12 0.00125 1.61  0.00201\n# ℹ 43,743 more rows\n\n\nBigram tf-idf, by journal\n\nCodelibrary(ggplot2)\n\nbigram_tf_idf %&gt;%\n  arrange(desc(tf_idf)) %&gt;%\n  group_by(abbreviation) %&gt;%\n  slice_max(tf_idf, n = 10) %&gt;%\n  ungroup() %&gt;%\n  mutate(bigram = reorder(bigram, tf_idf)) %&gt;%\n  ggplot(aes(tf_idf, bigram, fill = abbreviation)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ abbreviation, ncol = 2, scales = \"free\") +\n  labs(x = \"tf-idf of bigram\", y = NULL)\n\n\n\n\nBigram tf-idf, by year\n\nCode# get tf-idf by year\nbigram_tf_idf_year &lt;- bigrams_united %&gt;%\n  count(publication_year, bigram) %&gt;%\n  bind_tf_idf(bigram, publication_year, n) %&gt;%\n  arrange(desc(tf_idf))\n\n# plot tf-idf by year\nbigram_tf_idf_year %&gt;%\n  arrange(desc(tf_idf)) %&gt;%\n  group_by(publication_year) %&gt;%\n  slice_max(tf_idf, n = 5) %&gt;%\n  ungroup() %&gt;%\n  mutate(bigram = reorder(bigram, tf_idf)) %&gt;%\n  ggplot(aes(tf_idf, bigram, fill = publication_year)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ publication_year, ncol = 3, scales = \"free\") +\n  labs(x = \"tf-idf of bigram\", y = NULL)\n\n\n\n\n\nCodelibrary(igraph)\n\nbigram_counts\n\n# A tibble: 39,374 × 3\n   word1        word2             n\n   &lt;chr&gt;        &lt;chr&gt;         &lt;int&gt;\n 1 technical    communication  1197\n 2 technical    writing         427\n 3 technical    communicators   380\n 4 professional communication   241\n 5 writing      communication   237\n 6 multiple     sites           182\n 7 social       media           135\n 8 user         experience      102\n 9 social       justice          94\n10 email        articles         92\n# ℹ 39,364 more rows\n\nCodebigram_graph &lt;- bigram_counts %&gt;%\n  filter(n &gt; 25) %&gt;%\n  graph_from_data_frame()\n\nbigram_graph\n\nIGRAPH c310e27 DN-- 81 69 -- \n+ attr: name (v/c), n (e/n)\n+ edges from c310e27 (vertex names):\n [1] technical    -&gt;communication technical    -&gt;writing      \n [3] technical    -&gt;communicators professional -&gt;communication\n [5] writing      -&gt;communication multiple     -&gt;sites        \n [7] social       -&gt;media         user         -&gt;experience   \n [9] social       -&gt;justice       email        -&gt;articles     \n[11] article      -&gt;examines      content      -&gt;strategy     \n[13] web          -&gt;sites         covid        -&gt;19           \n[15] communication-&gt;tpc           communication-&gt;design       \n+ ... omitted several edges\n\n\n\nCode#install.packages(\"ggraph\")\nlibrary(ggraph)\nset.seed(999)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1)\n\n\n\nCodea &lt;- grid::arrow(type = \"closed\", length = unit(.1, \"inches\"))\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,\n                 arrow = a, end_cap = circle(.05, 'inches')) +\n  geom_node_point(color = \"lightblue\", size = 3) +\n  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n  theme_void()"
  },
  {
    "objectID": "demos/week05/wk05_tc-titles-pt2.html",
    "href": "demos/week05/wk05_tc-titles-pt2.html",
    "title": "Wk 05: Titles in TC Journals, pt. 2",
    "section": "",
    "text": "Here, we conduct some computational text analysis on a dataset consisting of article metadata for articles published in 5 TC journals between 2005 and 2023.\nIn this exercise, we’ll work adjacent to Boettger and Friess (2014) in that we are working with a similar dataset. That said, our dataset doesn’t include Intercom but does feature 2000+ articles across 5 journals and 18 years. As such, rather than compare word usage across academic and trade publications, we might be able to do some additional analysis to trace shifts over time."
  },
  {
    "objectID": "demos/week05/wk05_tc-titles-pt2.html#overview",
    "href": "demos/week05/wk05_tc-titles-pt2.html#overview",
    "title": "Wk 05: Titles in TC Journals, pt. 2",
    "section": "",
    "text": "Here, we conduct some computational text analysis on a dataset consisting of article metadata for articles published in 5 TC journals between 2005 and 2023.\nIn this exercise, we’ll work adjacent to Boettger and Friess (2014) in that we are working with a similar dataset. That said, our dataset doesn’t include Intercom but does feature 2000+ articles across 5 journals and 18 years. As such, rather than compare word usage across academic and trade publications, we might be able to do some additional analysis to trace shifts over time."
  },
  {
    "objectID": "demos/week05/wk05_tc-titles-pt2.html#quantitative-analysis-of-textual-data",
    "href": "demos/week05/wk05_tc-titles-pt2.html#quantitative-analysis-of-textual-data",
    "title": "Wk 05: Titles in TC Journals, pt. 2",
    "section": "Quantitative Analysis of Textual Data",
    "text": "Quantitative Analysis of Textual Data\nQuantitative analysis of textual data has been a mainstay in corpus linguistics for some time. It’s been much less popular in RCWS, but can be a useful approach for working with large datasets.\nQuanteda\nWhile Ding and Kong (2019) and Boettger and Friess (2014) both use Laurence Anthony’s AntConc and there are a number of similar WYSIWYG corpus analysis tools out there, we’ll be using Quanteda. Quanteda is an R package designed to provide an open source alternative to expensive corpus tools that is simultaneously powerful and relatively easy to learn.\nHere are some concepts and vocabulary that are salient to corpus analysis with Quanteda:\n\n\nCorpus: A corpus is a collection of text documents. It’s the primary data structure in Quanteda, and it can consist of a single document or a large collection of documents.\n\nDocument-Term Matrix (DTM): A DTM is a table that represents the frequency of words (terms) in each document of a corpus. It’s a fundamental tool for text analysis and allows you to perform various operations on text data.\n\nTokenization: Tokenization is the process of breaking text into individual words or tokens. Here, a “token” refers to a single, meaningful unit of text. Tokens are the building blocks of textual data, and they are usually words, but they can also be phrases, subword units, punctuation marks, and more. Quanteda can tokenize text documents, which is the first step in many text analysis tasks.\n\nStop Words: Stop words are common words like “and,” “the,” “is,” etc., that are often removed from text before analysis. While most disciplines remove them without hesitation, scholars in rhetoric may ask questions for which stop words are significant.\n\nStemming and Lemmatization: These are techniques used to reduce words to their base or root forms. Stemming removes suffixes from words, while lemmatization maps words to their dictionary forms. Quanteda provides functions for both.\n\nDictionary: A dictionary is a list of words or phrases used to identify specific features or attributes in text. You can create custom dictionaries or use predefined ones in Quanteda.\n\nn-grams: N-grams are contiguous sequences of n items (usually words) from a given text. Quanteda can help you create n-grams to capture multi-word phrases and patterns.\n\nTidy Data Principles: Quanteda often follows the tidy data principles, which emphasize organizing data into a structured format that facilitates analysis. This includes using data frames and long-format data structures.\nInstall and Load Quanteda\nQuanteda is split into a series of modular packages. We’ll install each of them as well as a few others. For reference, read about the Quanteda family of packages.\n\nCode#install.packages(\"Rtools\") # you may need to install Rtools to install all the quanteda packages\n\n#install.packages(\"remotes\")\n\n#install.packages(\"quanteda\")\n#install.packages(\"readtext\")\n#install.packages(\"spacyr\")\n#install.packages(\"quanteda.textmodels\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"quanteda.textplots\")\n#remotes::install_github(\"kbenoit/quanteda.dictionaries\")\n\n# load quanteda family of packages\nlibrary(quanteda)\nlibrary(quanteda.dictionaries)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)\n\n# load other libraries\nlibrary(readtext)\nlibrary(spacyr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "demos/week05/wk05_tc-titles-pt2.html#quantitative-analysis-of-tc-titles-and-abstracts",
    "href": "demos/week05/wk05_tc-titles-pt2.html#quantitative-analysis-of-tc-titles-and-abstracts",
    "title": "Wk 05: Titles in TC Journals, pt. 2",
    "section": "Quantitative Analysis of TC Titles and Abstracts",
    "text": "Quantitative Analysis of TC Titles and Abstracts\nLoad data\n\nCode# define location of datafile\ndata_file &lt;- \"data/tc_journals.RData\"\n\n# load data\nload(data_file)\n\nglimpse(tc_journals)\n\nRows: 2,002\nColumns: 6\n$ source_title      &lt;chr&gt; \"JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\", \"…\n$ author_full_names &lt;chr&gt; \"Wickman, Chad\", \"DeJeu, Emily Barrow\", \"DeVasto, Da…\n$ article_title     &lt;chr&gt; \"Genre and Metagenre in Biomedical Research Writing\"…\n$ abstract          &lt;chr&gt; \"The use of reporting guidelines is an established y…\n$ publication_year  &lt;dbl&gt; 2023, 2022, 2016, 2009, 2009, 2020, 2016, 2012, 2011…\n$ abbreviation      &lt;chr&gt; \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBT…\n\n\nFor reference, here’s our data in a nifty table made with the reactable package. You can sort and filter by column, which may come in handy as you check your code results. For viewing purposes I’ve combined authors, journal, and year into a single column.\n\nCode#install.packages(\"reactable\")\nlibrary(reactable)\n\n# drop source title\ntable_data &lt;- tc_journals %&gt;%\n  mutate(\n    article = paste(author_full_names,\" (\",abbreviation,\", \",publication_year,\")\"\n    )\n  ) %&gt;%\n  select(article,\n         article_title,\n         abstract)\n\n# create table (define columns, add parameters & formatting)\nreactable(table_data,\n  columns = list(\n    article = colDef(name = \"Article\", sortable = TRUE, filterable = TRUE),\n    article_title = colDef(name = \"Title\", sortable = TRUE, filterable = TRUE, width = 175),\n    abstract = colDef(name = \"Abstract\", sortable = TRUE, filterable = TRUE, width = 500)\n  ),\n  defaultColDef = colDef(align = \"left\", width = 125),\n  searchable = TRUE, # Enable search\n  sortable = TRUE, # Enable sorting\n  defaultPageSize = 3,\n  highlight = TRUE,\n  outlined = TRUE,\n  striped = TRUE,\n  compact = TRUE\n)\n\n\n\n\n\n\nNow that we have packages installed, libraries loaded, and our data looks good, let’s try to answer a series of questions…\nWhat are the most common words in TC Journals?\nTo answer this question, we need to\n\ncreate a corpus object in which titles function as the texts\ntokenize each text (title) in the corpus\nremove stopwords (or decide not to)\ncreate a document-feature matrix\nsort and visualize the feature (token/word) counts\n\nCreate a corpus of titles\nCrete a corpus object.\n\nCode# Create a corpus object called \"title_corp\"\ntitle_corp &lt;- corpus(tc_journals, text_field = \"article_title\")\n\n# print the corpus\nprint(title_corp)\n\nCorpus consisting of 2,002 documents and 5 docvars.\ntext1 :\n\"Genre and Metagenre in Biomedical Research Writing\"\n\ntext2 :\n\"The Ethics of Delivering Bad News: Evaluating Impression Man...\"\n\ntext3 :\n\"Stasis and Matters of Concern: The Conviction of the L'Aquil...\"\n\ntext4 :\n\"Integrating Social Media Into Existing Work Environments The...\"\n\ntext5 :\n\"Practitioner Research Instruction A Neglected Curricular Are...\"\n\ntext6 :\n\"Legally Minded Technical Communicators: A Case Study of a Le...\"\n\n[ reached max_ndoc ... 1,996 more documents ]\n\nCode# summary of the corpus (including metadata for the texts)\n#summary(title_corp, n = 2)\n\n\n\n\n\n\n\n\nCorpus summary\n\n\n\nYou can call summary(corpus_name) to get summary statistics about the corpus.\n\n\nCreate a tokens object\nNow that we have a corpus, we can tokenize our texts (article titles) so that each word in the title becomes on in a list of tokens associated with that article. We can view the first three results to better see what the object looks like.\n\nCode# create a tokens object without punctuation, separators, and numbers\ntitle_tokens &lt;- tokens(title_corp, remove_punct = TRUE,\n                 remove_separators = TRUE,\n                 remove_numbers = TRUE)\n\n# check the result\ntitle_tokens[1:3]\n\nTokens consisting of 3 documents and 5 docvars.\ntext1 :\n[1] \"Genre\"      \"and\"        \"Metagenre\"  \"in\"         \"Biomedical\"\n[6] \"Research\"   \"Writing\"   \n\ntext2 :\n [1] \"The\"        \"Ethics\"     \"of\"         \"Delivering\" \"Bad\"       \n [6] \"News\"       \"Evaluating\" \"Impression\" \"Management\" \"Strategies\"\n[11] \"in\"         \"Corporate\" \n[ ... and 2 more ]\n\ntext3 :\n [1] \"Stasis\"     \"and\"        \"Matters\"    \"of\"         \"Concern\"   \n [6] \"The\"        \"Conviction\" \"of\"         \"the\"        \"L'Aquila\"  \n[11] \"Seven\"     \n\n\nRemove stopwords\nFor this analysis, we might also remove stopwords. We can view the first 15 words in a standard list of stopwords.\n\nCode# see a list of stopwords\nhead(stopwords(\"en\"), 15)\n\n [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n[11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n\n\nNotice that “you” was a key word in B&G’s comparative analysis of academic and trade publications and also happens to be one of the stopwords in our list.\nWe’ll create a separate tokens object without stopwords so that we can compare the results with and without.\n\nCode# create a new tokens object by removing stopwords from the existing tokens object  \ntitle_tokens_nostop &lt;- title_tokens %&gt;%\n  tokens_remove(stopwords(\"en\"))\n\n# check the result\ntitle_tokens_nostop[1:3]\n\nTokens consisting of 3 documents and 5 docvars.\ntext1 :\n[1] \"Genre\"      \"Metagenre\"  \"Biomedical\" \"Research\"   \"Writing\"   \n\ntext2 :\n [1] \"Ethics\"     \"Delivering\" \"Bad\"        \"News\"       \"Evaluating\"\n [6] \"Impression\" \"Management\" \"Strategies\" \"Corporate\"  \"Financial\" \n[11] \"Reporting\" \n\ntext3 :\n[1] \"Stasis\"     \"Matters\"    \"Concern\"    \"Conviction\" \"L'Aquila\"  \n[6] \"Seven\"     \n\n\nCreate a document-feature matrix\nNext we can use a tokens object to create a document-feature matrix (dfm). For reference:\n\nDocument: In a DFM, each row represents a document or text unit. This can be a single document, a sentence, a paragraph, or any other defined text unit. In our case, it’s a title + abstract.\nFeature: Each column represents a feature, typically a word or a term that appears in the documents. These features are usually extracted from the text through tokenization, and they can be single words or multi-word phrases.\nMatrix: The DFM is a two-dimensional matrix where the rows correspond to documents, and the columns correspond to features. The values in the matrix represent the frequency of each feature in each document, but they can also be transformed into other measures such as term frequency-inverse document frequency (TF-IDF) scores.\n\nLet’s create two dfms: one that includes stopwords and another that doesn’t.\n\nCode# Below we create two dfms: one from each of our tokens objects (w/ stopwords and w/o stopwords)\n\n# syntax option 1 (with stopwords)\ntitles_dfm &lt;- title_tokens %&gt;%\n  dfm()\n\n\n# syntax option 2 (without stopwords)\ntitles_nostop_dfm &lt;- dfm(title_tokens_nostop)\n\n\nNow we can print each matrix.\n\nCode# dfm with stopwords\nprint(titles_dfm)\n\nDocument-feature matrix of: 2,002 documents, 4,523 features (99.76% sparse) and 5 docvars.\n       features\ndocs    genre and metagenre in biomedical research writing the ethics of\n  text1     1   1         1  1          1        1       1   0      0  0\n  text2     0   0         0  1          0        0       0   1      1  1\n  text3     0   1         0  0          0        0       0   2      0  2\n  text4     0   0         0  0          0        0       0   1      0  1\n  text5     0   0         0  1          0        1       0   0      0  0\n  text6     0   0         0  0          0        0       1   0      0  1\n[ reached max_ndoc ... 1,996 more documents, reached max_nfeat ... 4,513 more features ]\n\nCode# dfm without stopwords\nprint(titles_nostop_dfm)\n\nDocument-feature matrix of: 2,002 documents, 4,422 features (99.83% sparse) and 5 docvars.\n       features\ndocs    genre metagenre biomedical research writing ethics delivering bad news\n  text1     1         1          1        1       1      0          0   0    0\n  text2     0         0          0        0       0      1          1   1    1\n  text3     0         0          0        0       0      0          0   0    0\n  text4     0         0          0        0       0      0          0   0    0\n  text5     0         0          0        1       0      0          0   0    0\n  text6     0         0          0        0       1      0          0   0    0\n       features\ndocs    evaluating\n  text1          0\n  text2          1\n  text3          0\n  text4          0\n  text5          0\n  text6          0\n[ reached max_ndoc ... 1,996 more documents, reached max_nfeat ... 4,412 more features ]\n\n\n\n\n\n\n\n\nSparse matrices\n\n\n\nIn this context, “sparse” refers to a type of data structure used to efficiently work with large data. In a sparse matrix, the majority of the elements have a value of zero. If your dfm is 99.56% sparse, it means that only .04% of the entries are something other than 0.\n\n\nView top features (words)\n\nCode# 20 most frequent words in dfm (with stop)\ntopfeatures(titles_dfm, 20)\n\n          and           the            of            in communication \n         1058          1053          1025           820           615 \n            a     technical           for            to        design \n          599           546           378           266           197 \n      writing            an            on          from            as \n          183           173           137           132           129 \n     rhetoric  professional      research        online         study \n          120           113           106           100            95 \n\nCode# 20 most frequent words in dfm (with stop)\ntopfeatures(titles_nostop_dfm, 20)\n\ncommunication     technical        design       writing      rhetoric \n          615           546           197           183           120 \n professional      research        online         study        social \n          113           106           100            95            85 \n   rhetorical      analysis       content   information        review \n           83            80            79            76            72 \n         case    technology          work       science          user \n           69            69            68            67            64 \n\n\nVisualize top words (with stopwords)\n\nCode# get top n features\nfeatures_titles_dfm &lt;- textstat_frequency(titles_dfm, n = 50)\n\n# sort by reverse frequency order\nfeatures_titles_dfm$feature &lt;- with(features_titles_dfm, reorder(feature, -frequency))\n\nggplot(features_titles_dfm, aes(x = feature, y = frequency)) +\n    geom_point() + \n    theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  labs(title = \"Top words in TC Article Titles (2005-2023)\")\n\n\n\n\nVisualize top words (without stopwords)\n\nCode# get top n features\nfeatures_titles_nostop_dfm &lt;- textstat_frequency(titles_nostop_dfm, n = 50)\n\n# sort by reverse frequency order\nfeatures_titles_nostop_dfm$feature &lt;- with(features_titles_nostop_dfm, reorder(feature, -frequency))\n\nggplot(features_titles_nostop_dfm, aes(x = feature, y = frequency)) +\n    geom_point() + \n    theme(axis.text.x = element_text(angle = 45, hjust = 1))+\n  labs(title = \"Top words in TC Article Titles (2005-2023)\")\n\n\n\n\nReplicate B&G’s frequency table (no stop words)\nTop words in TC Article Titles (2005-2023)\n\nCodesorted_features &lt;- features_titles_nostop_dfm %&gt;%\n  arrange(desc(frequency))%&gt;%\n  select(feature, frequency, docfreq)%&gt;%\n  reactable(defaultPageSize = 20,\n  highlight = TRUE,\n  outlined = TRUE,\n  striped = TRUE,\n  compact = TRUE)\n\nsorted_features\n\n\n\n\n\n\nNice! But are “technical” and “communication” really top words? Or is “technical communication” the top phrase?\nWhat are the top words or phrases?\nHere, we can use Quanteda’s vignette Working with multi-word expressions.\nDiscover collocations\nHere, I start with the tokens object created earlier (no punctuation, separators, or numbers). We then remove stopwords, make the words lowercase, and\n\nCode# min count is a big parameter here. \ncol &lt;- title_tokens %&gt;% \n       tokens_remove(stopwords(\"en\")) %&gt;% \n       tokens_select(pattern = \"^[A-Z]\", valuetype = \"regex\", \n                     case_insensitive = TRUE, padding = TRUE) %&gt;% \n       textstat_collocations(min_count = 15, tolower = TRUE)\n\nhead(col)\n\n                 collocation count count_nested length   lambda        z\n1    technical communication   349            0      2 4.608652 41.40442\n2                 case study    43            0      2 6.221791 21.40411\n3              editor's desk    30            0      2 7.796537 17.94783\n4               social media    26            0      2 5.196207 17.94057\n5            user experience    23            0      2 6.069663 17.42236\n6 professional communication    55            0      2 3.148093 16.35628\n\n\nWe can then use that statistical scoring of word associations to automatically compound collocates into multi-word expressions.\n\nCodecomp_title_toks &lt;- tokens_compound(title_tokens, pattern = col)\n\nhead(comp_title_toks)\n\nTokens consisting of 6 documents and 5 docvars.\ntext1 :\n[1] \"Genre\"      \"and\"        \"Metagenre\"  \"in\"         \"Biomedical\"\n[6] \"Research\"   \"Writing\"   \n\ntext2 :\n [1] \"The\"        \"Ethics\"     \"of\"         \"Delivering\" \"Bad\"       \n [6] \"News\"       \"Evaluating\" \"Impression\" \"Management\" \"Strategies\"\n[11] \"in\"         \"Corporate\" \n[ ... and 2 more ]\n\ntext3 :\n [1] \"Stasis\"     \"and\"        \"Matters\"    \"of\"         \"Concern\"   \n [6] \"The\"        \"Conviction\" \"of\"         \"the\"        \"L'Aquila\"  \n[11] \"Seven\"     \n\ntext4 :\n [1] \"Integrating\"  \"Social_Media\" \"Into\"         \"Existing\"     \"Work\"        \n [6] \"Environments\" \"The\"          \"Case\"         \"of\"           \"Delicious\"   \n\ntext5 :\n [1] \"Practitioner\"            \"Research\"               \n [3] \"Instruction\"             \"A\"                      \n [5] \"Neglected\"               \"Curricular\"             \n [7] \"Area\"                    \"in\"                     \n [9] \"Technical_Communication\" \"Undergraduate\"          \n[11] \"Programs\"               \n\ntext6 :\n [1] \"Legally\"                 \"Minded\"                 \n [3] \"Technical_Communicators\" \"A\"                      \n [5] \"Case_Study\"              \"of\"                     \n [7] \"a\"                       \"Legal\"                  \n [9] \"Writing\"                 \"Course\"                 \n\n\nNow that we have a tokens object that includes multi-word expressions…\nFor reference:\n\nQuanteda Tutorials\nQuanteda Documentation"
  },
  {
    "objectID": "demos/week03/wk03_text_analysis.html",
    "href": "demos/week03/wk03_text_analysis.html",
    "title": "Wk 03: Exploring TC journals, (pt 3)",
    "section": "",
    "text": "We start with data from four journals"
  },
  {
    "objectID": "demos/week03/wk03_text_analysis.html#overview",
    "href": "demos/week03/wk03_text_analysis.html#overview",
    "title": "Wk 03: Exploring TC journals, (pt 3)",
    "section": "",
    "text": "We start with data from four journals"
  },
  {
    "objectID": "demos/week03/wk03_text_analysis.html#load-libraries-and-data",
    "href": "demos/week03/wk03_text_analysis.html#load-libraries-and-data",
    "title": "Wk 03: Exploring TC journals, (pt 3)",
    "section": "Load libraries and data",
    "text": "Load libraries and data\nFor this analysis, we’ll be using the R package Quanteda: Quantitative Analysis of Textual Data.\n\nCodelibrary(tidyverse)\n\n\n#install.packages(\"Rtools\") # you may need to install Rtools to install all the quanteda packages\n\n#install.packages(\"remotes\")\n\n#install.packages(\"quanteda\")\n#install.packages(\"readtext\")\n#install.packages(\"spacyr\")\n#install.packages(\"quanteda.textmodels\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"quanteda.textplots\")\n#remotes::install_github(\"kbenoit/quanteda.dictionaries\")\n\nlibrary(quanteda)\nlibrary(quanteda.dictionaries)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)"
  },
  {
    "objectID": "demos/week03/wk03_text_analysis.html#load-data",
    "href": "demos/week03/wk03_text_analysis.html#load-data",
    "title": "Wk 03: Exploring TC journals, (pt 3)",
    "section": "Load data",
    "text": "Load data\n\nCode#install.packages(\"here\")\nlibrary(here)\n\n# Use \"here\" to set the working directory \nhere::here()\n\n[1] \"C:/Users/dcard/Documents/GitHub/computational_rhetorics/demo_docs\"\n\nCodegetwd()\n\n[1] \"C:/Users/dcard/Documents/GitHub/computational_rhetorics/demo_docs/demos/week03\"\n\nCode# Use \"here\" to define the relative path to your data\ndata_file &lt;- here(\"demos/week03/data_out/full_data.RData\")\n\nload(data_file)\n\nglimpse(full_data)\n\nRows: 1,537\nColumns: 9\n$ source_title          &lt;chr&gt; \"JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION…\n$ author_full_names     &lt;chr&gt; \"Wickman, Chad\", \"DeJeu, Emily Barrow\", \"DeVasto…\n$ article_title         &lt;chr&gt; \"Genre and Metagenre in Biomedical Research Writ…\n$ abstract              &lt;chr&gt; \"The use of reporting guidelines is an establish…\n$ cited_references      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ cited_reference_count &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ publication_year      &lt;dbl&gt; 2023, 2022, 2016, 2009, 2009, 2020, 2016, 2012, …\n$ publication_type      &lt;chr&gt; \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\"…\n$ abbreviation          &lt;chr&gt; \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", …"
  },
  {
    "objectID": "demos/week03/wk03_text_analysis.html#creating-a-corpus-object",
    "href": "demos/week03/wk03_text_analysis.html#creating-a-corpus-object",
    "title": "Wk 03: Exploring TC journals, (pt 3)",
    "section": "Creating a Corpus object",
    "text": "Creating a Corpus object\nA Quanteda Corpus is a special form of a character vector that includes metadata about the corpus and the “documents” within the corpus. In this case, our corpus includes all the articles results in our CSV and each article is a document.\nCreate a “text” column for analysis\n\nCode# Creates a new column in full_data from the title and abstract (separated by a tilde)\nfull_data$text &lt;- paste(full_data$article_title, full_data$abstract, sep = \" ~ \")\n\n\n# check the column\nfull_data$text[2:3]\n\n[1] \"The Ethics of Delivering Bad News: Evaluating Impression Management Strategies in Corporate Financial Reporting ~ Business communication textbooks offer impression management (IM) strategies to help students learn how to soften bad news. But corporations sometimes use these strategies in ethically questionable ways. This article analyzes IM strategies in a landmark case of ethically dubious corporate financial reporting. Findings suggest that the company, Ivax, manipulated three standard IM strategies by overamplifying its power to fix a financial crisis, substantially downplaying bad news, and concealing damaging information. Ivax also used a fourth, less familiar strategy: It buried contradictory information in legal disclaimers. Instructors need to help students become ethical writers who avoid questionable IM strategies like these.\"                                                                                     \n[2] \"Stasis and Matters of Concern: The Conviction of the L'Aquila Seven ~ On October 22, 2012, six scientists and one civil servant were convicted of manslaughter for failing to properly warn the people of L'Aquila, Italy, of an impending earthquake that resulted in over 300 deaths and 1,500 injuries. This article investigates a key event leading up to this conviction: An emergency meeting of scientists, civil servants, and politicians to determine whether or not an advanced warning should be issued to the residents of L'Aquila. The following investigation of this emergency meeting uses functional stasis analysis to identify the primary breakdown in deliberation that ultimately led to a message of calm and reassurance immediately prior to the devastating earthquake. The results provide insights into not only the events in L'Aquila but also broader issues of risk, uncertainty, fact, and value in science-policy deliberation.\"\n\n\nCreate the corpus\n\nCode# creates the corpus object\ncorp &lt;- corpus(full_data)\n\n# summary of the corpus (including metadata for the texts)\nsummary(corp, n = 3)\n\nCorpus consisting of 1537 documents, showing 3 documents:\n\n  Text Types Tokens Sentences                                    source_title\n text1    97    149         4 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n text2    94    130         6 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n text3   102    158         4 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n                                       author_full_names\n                                           Wickman, Chad\n                                     DeJeu, Emily Barrow\n DeVasto, Danielle; Graham, S. Scott; Zamparutti, Louise\n                                                                                                   article_title\n                                                              Genre and Metagenre in Biomedical Research Writing\n The Ethics of Delivering Bad News: Evaluating Impression Management Strategies in Corporate Financial Reporting\n                                             Stasis and Matters of Concern: The Conviction of the L'Aquila Seven\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    abstract\n The use of reporting guidelines is an established yet still-evolving practice in the field of biomedicine. These documents are often linked to common methodologies (e.g., randomized clinical trials); they include multiple textual artifacts (e.g., checklists, flow diagrams) and have a history that is coextensive with the emergence and ongoing development of evidence-based medicine (e.g., as an epistemological orientation to research and decision making). Drawing on the concept of metagenre, this article examines how practitioners use reporting guidelines to define and regulate the boundaries of biomedical research and writing activity. The analysis, focusing on one prominent set of guidelines, shows how practitioners use the genre-metagenre dynamic to promote strategic intervention while upholding traditional principles and standards for evidence-based research and communication.\n                                                                                                                                                               Business communication textbooks offer impression management (IM) strategies to help students learn how to soften bad news. But corporations sometimes use these strategies in ethically questionable ways. This article analyzes IM strategies in a landmark case of ethically dubious corporate financial reporting. Findings suggest that the company, Ivax, manipulated three standard IM strategies by overamplifying its power to fix a financial crisis, substantially downplaying bad news, and concealing damaging information. Ivax also used a fourth, less familiar strategy: It buried contradictory information in legal disclaimers. Instructors need to help students become ethical writers who avoid questionable IM strategies like these.\n                              On October 22, 2012, six scientists and one civil servant were convicted of manslaughter for failing to properly warn the people of L'Aquila, Italy, of an impending earthquake that resulted in over 300 deaths and 1,500 injuries. This article investigates a key event leading up to this conviction: An emergency meeting of scientists, civil servants, and politicians to determine whether or not an advanced warning should be issued to the residents of L'Aquila. The following investigation of this emergency meeting uses functional stasis analysis to identify the primary breakdown in deliberation that ultimately led to a message of calm and reassurance immediately prior to the devastating earthquake. The results provide insights into not only the events in L'Aquila but also broader issues of risk, uncertainty, fact, and value in science-policy deliberation.\n cited_references cited_reference_count publication_year publication_type\n             &lt;NA&gt;                    NA             2023                J\n             &lt;NA&gt;                    NA             2022                J\n             &lt;NA&gt;                    NA             2016                J\n abbreviation\n         JBTC\n         JBTC\n         JBTC\n\nCode# Prints texts\nprint(corp)\n\nCorpus consisting of 1,537 documents and 9 docvars.\ntext1 :\n\"Genre and Metagenre in Biomedical Research Writing ~ The use...\"\n\ntext2 :\n\"The Ethics of Delivering Bad News: Evaluating Impression Man...\"\n\ntext3 :\n\"Stasis and Matters of Concern: The Conviction of the L'Aquil...\"\n\ntext4 :\n\"Integrating Social Media Into Existing Work Environments The...\"\n\ntext5 :\n\"Practitioner Research Instruction A Neglected Curricular Are...\"\n\ntext6 :\n\"Legally Minded Technical Communicators: A Case Study of a Le...\"\n\n[ reached max_ndoc ... 1,531 more documents ]\n\n\nPlot descriptive statistics\nPlot metadata: tokens per text, by journal\n\nCode# get metadata\ntokeninfo &lt;- summary(corp, n = 1537)\n\n# plot\nif (require(ggplot2)) ggplot(data = tokeninfo, aes(x = publication_year, y = Tokens, group = abbreviation, color = abbreviation)) +\n  geom_point() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"Tokens per text over time\",\n  subtitle = \"Token counts by publication year\",\n  x = \"Publication Year\",\n  y = \"Tokens Count\"\n  ) +\n  theme_bw()\n\n\n\n\nPlot metadata: sentences per text, by journal\nWe can plot sentences in the title+abstract for each article (and color-coded by journal)\n\nCode# how many unique sentence values are there?\nunique(tokeninfo$Sentences)\n\n [1]  4  6  3  5  7  1 10  2  8 15  9 11 12 14 13 17\n\nCode# plot tokeninfo \nif (require(ggplot2)) ggplot(data = tokeninfo, aes(x = publication_year, y = Sentences, group = abbreviation, color = abbreviation)) +\n  geom_point() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"Sentences per text over time\",\n  subtitle = \"Sentence counts by publication year\",\n  x = \"Publication Year\",\n  y = \"Sentence Count\"\n  ) +\n  theme_bw()\n\n\n\n\nBut wait, where are all the JBTC articles?!\n\nCode# get a list of unique sentence lengths in JBTC title+abstracts\nunique(tokeninfo$Sentences[tokeninfo$abbreviation == \"JBTC\"])\n\n[1]  4  6  3  5  7  1 10  2  8\n\nCode# get a list of unique years among JBTC observations\nunique(tokeninfo$publication_year[tokeninfo$abbreviation == \"JBTC\"])\n\n [1] 2023 2022 2016 2009 2020 2012 2011 2010 2007 2021 2006 2014 2008 2019 2018\n[16] 2015 2005 2013 2017\n\n\nIt seems like we have JBTC articles of varying sentence length across multiple years…\nSubset a corpus\nLet’s investigate further. Use the corpus_subset function to keep only texts from JBTC.\n\nCode# corpus_subset(corp, abbreviation == \"JBTC\")\n\njbtc_tokeninfo &lt;- summary(corpus_subset(corp, abbreviation == \"JBTC\"), n = 300)\n\n\nThen visualize sentence counts again, this time for just JBTC…\n\nCodeif (require(ggplot2)) ggplot(data = jbtc_tokeninfo, aes(x = publication_year, y = Sentences)) +\n  geom_point() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"JBTC: Sentences per text over time\",\n  subtitle = \"Sentence counts in JBTC\",\n  x = \"Publication Year\",\n  y = \"Sentence Count\"\n  ) +\n  theme_bw()\n\n\n\n\nOk, so maybe it’s just a problem with the plot?\nAdd a layer: # of observations with x sentences\n\nCode# use geom_count instead of geom_point to let size reflect the count of the points\n  \nggplot(data = jbtc_tokeninfo, aes(x = publication_year, y = Sentences)) +\n  geom_count() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"JBTC: Sentences per text over time\",\n  subtitle = \"Sentence counts in JBTC\",\n  x = \"Publication Year\",\n  y = \"Sentence Count\"\n  ) +\n  theme_bw()\n\n\n\n\nAdd size to all journal sentence counts\n\nCodeggplot(data = tokeninfo, aes(x = publication_year, y = Sentences, group = abbreviation, color = abbreviation)) +\n  geom_count() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"Sentences per text over time\",\n  subtitle = \"Sentence counts by publication year\",\n  x = \"Publication Year\",\n  y = \"Sentence Count\"\n  ) +\n  theme_bw()\n\n\n\n\nIt’s not perfect, but definitely better."
  },
  {
    "objectID": "demos/week03/wk03_text_analysis.html#exploring-corpus-texts",
    "href": "demos/week03/wk03_text_analysis.html#exploring-corpus-texts",
    "title": "Wk 03: Exploring TC journals, (pt 3)",
    "section": "Exploring corpus texts",
    "text": "Exploring corpus texts\nBut token and sentence counts probably aren’t the most interesting aspect of the titles and abstracts…\nKWIC: search for patterns\nWe can search for patterns in multiple ways:\n\nsingle word: kwic(data_tokens, pattern = \"usability\")\n\nstring of characters: kwic(data_tokens, pattern = \"user-*\")\n\nphrase: kwic(data_tokens, phrase(\"social justice\"))\n\n\nKWIC for “usability”\nWe can search for usability and surrounding words.\n\n\n\n\n\n\nQuanteda Tokens object\n\n\n\nTokens: Each element of a tokens object typically represents a single word or a term. However, tokens can also represent larger text units such as sentences or paragraphs, depending on the tokenization process applied.\n\n\n\nCode# create a tokens object\ndata_tokens &lt;- tokens(corp)\n\n# data, pattern, number of tokens before and after \nkwic_usability &lt;- kwic(data_tokens, pattern = \"usability\", 5) \n\n# display the first 10 matches\nkwic_usability[0:10]\n\nKeyword-in-context with 10 matches.                                                                    \n  [text16, 102]           a practical influence on the | usability |\n   [text79, 97]           and technology; the cultural | usability |\n  [text126, 94]    answered questions about the tool's | usability |\n  [text165, 42]        transparency, learnability, and | usability |\n  [text165, 96] supported by task-based documentation, | usability |\n [text168, 117]             similar contexts: content, | usability |\n  [text264, 16]             Tracking as a Component of | Usability |\n   [text273, 6]               Listening to Students: A | Usability |\n  [text273, 79]             how students use feedback. | Usability |\n [text273, 107]              . This article reports on | usability |\n                                              \n of assembly instructions. This               \n research conducted and located accountability\n and communicative effectiveness, and         \n . Looking at questions asked                 \n problems were more prominent.                \n , and overall visual appeal                  \n and Sustainability ~ Framed around           \n Evaluation of Instructor Commentary ~        \n evaluation is ideally equipped for           \n testing of commentary provided to            \n\nCode# display the last 6 matches\ntail(kwic_usability)\n\nKeyword-in-context with 6 matches.                                                                 \n  [text1158, 6]      Revising the Online Classroom: | Usability |\n [text1158, 26]               by the authors to use | usability |\n [text1158, 59]   institutions can create their own | usability |\n [text1190, 41]         format. It investigates the | usability |\n [text1190, 62]          as YouTube analytics data, | usability |\n [text1306, 70] of argument effectiveness, document | usability |\n                                         \n Testing for Training Online Technical   \n testing as a component of               \n testing protocols for formative online  \n and design-implications of a live-action\n , and comprehension assessments.        \n , and professionalism. Three            \n\nCode# chart using kableExtra (for markdown to html version)\nlibrary(kableExtra)\nhead(kwic_usability) %&gt;%\n  kbl() %&gt;%\n  kable_minimal()\n\n\n\ndocname\nfrom\nto\npre\nkeyword\npost\npattern\n\n\n\ntext16\n102\n102\na practical influence on the\nusability\nof assembly instructions . This\nusability\n\n\ntext79\n97\n97\nand technology ; the cultural\nusability\nresearch conducted and located accountability\nusability\n\n\ntext126\n94\n94\nanswered questions about the tool's\nusability\nand communicative effectiveness , and\nusability\n\n\ntext165\n42\n42\ntransparency , learnability , and\nusability\n. Looking at questions asked\nusability\n\n\ntext165\n96\n96\nsupported by task-based documentation ,\nusability\nproblems were more prominent .\nusability\n\n\ntext168\n117\n117\nsimilar contexts : content ,\nusability\n, and overall visual appeal\nusability\n\n\n\n\n\nKWIC for user-x\n\nCodekwic_userx &lt;- kwic(data_tokens, pattern = \"user-*\", 3)\n\nhead(kwic_userx)\n\nKeyword-in-context with 6 matches.                                                       \n  [text4, 129] and repurposing their | user-generated |\n  [text37, 61]    tagging to compile | user-specific  |\n [text49, 119]  emphasis on creating | user-centered  |\n [text172, 51]  well acquainted with | user-centered  |\n [text172, 73] data collected within | user-centered  |\n [text190, 33]       as interactive, | user-generated |\n                            \n data.                      \n metadata on information    \n risk information that      \n design ( UCD               \n research and instead       \n documentation and describes\n\n\nKWIC for “social justice”\n\nCode# show context of the first six occurrences of 'social justice'\nkwic(data_tokens, pattern = phrase(\"social justice\")) %&gt;%\n    head()\n\nKeyword-in-context with 6 matches.                                                                            \n  [text25, 131:132]           It encourages a response to | social justice |\n   [text103, 12:13]   Feminist Scholarship Can Inform the | Social Justice |\n   [text103, 33:34]                    do, and can inform | social justice |\n   [text103, 45:46]            communication ( TPC ) even | social justice |\n   [text103, 68:69]           that are relevant to future | social justice |\n [text103, 124:125] methodologies and theories to enhance | social justice |\n                                       \n exigencies, invites participation from\n Turn ~ This article calls             \n work in technical and professional    \n work that is not explicitly           \n work: ( a )                           \n scholarship.                          \n\n\nThe tokens object\nLet’s dig into the tokens object.\n\nCodetokens &lt;- tokens(corp)\n\ntokens[1:3]\n\nTokens consisting of 3 documents and 9 docvars.\ntext1 :\n [1] \"Genre\"      \"and\"        \"Metagenre\"  \"in\"         \"Biomedical\"\n [6] \"Research\"   \"Writing\"    \"~\"          \"The\"        \"use\"       \n[11] \"of\"         \"reporting\" \n[ ... and 137 more ]\n\ntext2 :\n [1] \"The\"        \"Ethics\"     \"of\"         \"Delivering\" \"Bad\"       \n [6] \"News\"       \":\"          \"Evaluating\" \"Impression\" \"Management\"\n[11] \"Strategies\" \"in\"        \n[ ... and 118 more ]\n\ntext3 :\n [1] \"Stasis\"     \"and\"        \"Matters\"    \"of\"         \"Concern\"   \n [6] \":\"          \"The\"        \"Conviction\" \"of\"         \"the\"       \n[11] \"L'Aquila\"   \"Seven\"     \n[ ... and 146 more ]\n\n\nNotice what counts as a token by default.\nPreprocessing\nWe may want to remove certain words or characters that aren’t salient for our analysis\nRemove punctuation, separators, and numbers\n\nCode# create a tokens object without punctuation, separators, and numbers\ntokens &lt;- tokens(corp, remove_punct = TRUE,\n                 remove_separators = TRUE,\n                 remove_numbers = TRUE)\n\n# check the result\ntokens[1:3]\n\nTokens consisting of 3 documents and 9 docvars.\ntext1 :\n [1] \"Genre\"      \"and\"        \"Metagenre\"  \"in\"         \"Biomedical\"\n [6] \"Research\"   \"Writing\"    \"~\"          \"The\"        \"use\"       \n[11] \"of\"         \"reporting\" \n[ ... and 116 more ]\n\ntext2 :\n [1] \"The\"        \"Ethics\"     \"of\"         \"Delivering\" \"Bad\"       \n [6] \"News\"       \"Evaluating\" \"Impression\" \"Management\" \"Strategies\"\n[11] \"in\"         \"Corporate\" \n[ ... and 103 more ]\n\ntext3 :\n [1] \"Stasis\"     \"and\"        \"Matters\"    \"of\"         \"Concern\"   \n [6] \"The\"        \"Conviction\" \"of\"         \"the\"        \"L'Aquila\"  \n[11] \"Seven\"      \"~\"         \n[ ... and 127 more ]\n\n\nRemove stopwords and more\n\nCode# see list of stopwords\nhead(stopwords(\"en\"), 15)\n\n [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n[11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n\nCode# remove stopwords\ntokens &lt;- tokens %&gt;%\n  tokens_remove(stopwords(\"en\")) %&gt;%\n  tokens_remove(\"~\")\n\n# check the result\ntokens[1:3]\n\nTokens consisting of 3 documents and 9 docvars.\ntext1 :\n [1] \"Genre\"          \"Metagenre\"      \"Biomedical\"     \"Research\"      \n [5] \"Writing\"        \"use\"            \"reporting\"      \"guidelines\"    \n [9] \"established\"    \"yet\"            \"still-evolving\" \"practice\"      \n[ ... and 69 more ]\n\ntext2 :\n [1] \"Ethics\"     \"Delivering\" \"Bad\"        \"News\"       \"Evaluating\"\n [6] \"Impression\" \"Management\" \"Strategies\" \"Corporate\"  \"Financial\" \n[11] \"Reporting\"  \"Business\"  \n[ ... and 76 more ]\n\ntext3 :\n [1] \"Stasis\"     \"Matters\"    \"Concern\"    \"Conviction\" \"L'Aquila\"  \n [6] \"Seven\"      \"October\"    \"six\"        \"scientists\" \"one\"       \n[11] \"civil\"      \"servant\"   \n[ ... and 67 more ]\n\n\nCreating a document-feature matrix\nQuanteda uses a data structure called a document-feature matrix:\n\nDocument: In a DFM, each row represents a document or text unit. This can be a single document, a sentence, a paragraph, or any other defined text unit. In our case, it’s a title + abstract.\nFeature: Each column represents a feature, typically a word or a term that appears in the documents. These features are usually extracted from the text through tokenization, and they can be single words or multi-word phrases.\nMatrix: The DFM is a two-dimensional matrix where the rows correspond to documents, and the columns correspond to features. The values in the matrix represent the frequency of each feature in each document, but they can also be transformed into other measures such as term frequency-inverse document frequency (TF-IDF) scores.\n\n\nCode# create the dfm\ndfm &lt;- tokens(corp, remove_punct = TRUE,\n              remove_separators = TRUE,\n              remove_numbers = TRUE) %&gt;%\n  tokens_remove(stopwords(\"en\")) %&gt;%\n  tokens_remove(\"~\") %&gt;%\n  dfm()\n\n# view the dfm\nprint(dfm)\n\nDocument-feature matrix of: 1,537 documents, 12,399 features (99.56% sparse) and 9 docvars.\n       features\ndocs    genre metagenre biomedical research writing use reporting guidelines\n  text1     1         2          2        4       2   3         2          3\n  text2     0         0          0        0       0   1         2          0\n  text3     0         0          0        0       0   0         0          0\n  text4     0         0          0        1       0   2         0          0\n  text5     0         0          0        7       0   0         0          0\n  text6     0         0          0        0       3   0         0          0\n       features\ndocs    established yet\n  text1           1   1\n  text2           0   0\n  text3           0   0\n  text4           0   0\n  text5           0   1\n  text6           0   0\n[ reached max_ndoc ... 1,531 more documents, reached max_nfeat ... 12,389 more features ]\n\n\n\n\n\n\n\n\nSparse matrices\n\n\n\nIn this context, “sparse” refers to a type of data structure used to efficiently work with large data. In a sparse matrix, the majority of the elements have a value of zero. If your dfm is 99.56% sparse, it means that only .04% of the entries are something other than 0.\n\n\nCreate a wordcloud\n\nCode# simple wordcloud\ntextplot_wordcloud(dfm)\n\n\n\nCode# wordcloud with parameters\nset.seed(100)\ntextplot_wordcloud(dfm, \n                   min_count = 20, # include word only if it occurs at least n times in data set \n                   random_order = FALSE, \n                   rotation = 0.25,\n    color = RColorBrewer::brewer.pal(8, \"Dark2\"))\n\n\n\n\nView top features\n\nCode# 20 most frequent words\ntopfeatures(dfm, 20)\n\n    technical communication       writing         study       article \n         1546          1475          1377           960           913 \n     students      research      analysis       results           can \n          772           751           577           485           483 \n professional           use        design    rhetorical          work \n          468           457           453           445           427 \n       social       content          data     practices         using \n          412           388           386           360           356 \n\n\nDFM: Grouping by variables\n\nCode# group the dfm by a variable (docvar): journal\ndfm_journ &lt;- dfm %&gt;%\n  dfm_group(groups = abbreviation)\n\n# sort features by frequency and then view\ndfm_sort(dfm_journ)\n\nDocument-feature matrix of: 4 documents, 12,399 features (54.05% sparse) and 3 docvars.\n      features\ndocs   technical communication writing study article students research analysis\n  JBTC       244           375     158   236     212      131      119      102\n  TC         716           508     102   225     144      127      192      157\n  TCQ        561           531     149   175     297      154      183      110\n  WC          25            61     968   324     260      360      257      208\n      features\ndocs   results can\n  JBTC      84 107\n  TC       270 193\n  TCQ       39  95\n  WC        92  88\n[ reached max_nfeat ... 12,389 more features ]\n\n\nCreate a comparison cloud\n\nCode# cloud that compares top features for each journal\ncomparison_cloud &lt;- dfm_journ %&gt;%\n  dfm_trim(min_termfreq = 25,\n           verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\n\n\nIt’s common joke in computational humanities and social scientists that “colleagues don’t let colleagues make wordclouds,” but maybe this one can help us generate or refine some RQs?\nPlot relative frequencies\nQuanteda’s texstat_frequency allows to plot the most frequent words in terms of relative frequency by group\n\nCode#help(\"dfm_weight\")\n#help(\"textstat_frequency\")\n\n# calculate the proportional weight (the proportion of the feature count relative to total feature count)\ndfm_weight_journ &lt;- dfm_journ %&gt;%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by journal\nfreq_weight &lt;- textstat_frequency(dfm_weight_journ, n = 15, \n                                  groups = dfm_weight_journ$abbreviation)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n     geom_point() +\n     facet_wrap(~ group, scales = \"free\") +\n     coord_flip() +\n     scale_x_continuous(breaks = nrow(freq_weight):1,\n                        labels = freq_weight$feature) +\n     labs(x = NULL, \n          y = \"Relative frequency\",\n          title = \"Most frequent terms as a proportion of terms in the journal\")\n\n\n\n\nIt seems like some of these terms are parts of phrases, no?"
  },
  {
    "objectID": "demos/week03/wk03_text_analysis.html#more-hastily",
    "href": "demos/week03/wk03_text_analysis.html#more-hastily",
    "title": "Wk 03: Exploring TC journals, (pt 3)",
    "section": "More, hastily",
    "text": "More, hastily\nPlot “keyness” in TCQ and TC\nKeyness is a score for top features that occur differentially across categories or groups.\n\nCode# get info on \"keyness\"\nhelp(\"textstat_keyness\")\n\n\n# Subset initial corpus to retain TCQ and TC \ntc_v_tcq_corpus &lt;- corpus_subset(corp, \n                            abbreviation %in% c(\"TCQ\", \"TC\"))\n\n# Create a dfm grouped by journal (abbreviation)\ntc_v_tcq_dfm &lt;- tokens(tc_v_tcq_corpus, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_remove(\"~\") %&gt;%\n  tokens_group(groups = abbreviation) %&gt;%\n  dfm()\n\n# Calculate keyness and determine TC as target group\nresult_keyness &lt;- textstat_keyness(tc_v_tcq_dfm, target = \"TC\")\n\n# Plot estimated word keyness\ntextplot_keyness(result_keyness) \n\n\n\n\nNgrams\nYou can generate n-grams from a tokens object using tokens_ngrams().\n\nCode# creating a tokens object from a corpus\ntoks &lt;- tokens(corp, remove_punct = TRUE,\n              remove_separators = TRUE,\n              remove_numbers = TRUE) %&gt;%\n  tokens_remove(stopwords(\"en\")) %&gt;%\n  tokens_remove(\"~\")\n\n\n# generating ngrams from a tokens object\n\ntoks_ngram &lt;- tokens_ngrams(toks, n = 2) # specify combinations, e.g. 2 and 3 word combos -&gt; (toks, n = 2:3)\n\n\n# view result: first 10 ngrams in the first article\nhead(toks_ngram[[1]], 10)\n\n [1] \"Genre_Metagenre\"        \"Metagenre_Biomedical\"   \"Biomedical_Research\"   \n [4] \"Research_Writing\"       \"Writing_use\"            \"use_reporting\"         \n [7] \"reporting_guidelines\"   \"guidelines_established\" \"established_yet\"       \n[10] \"yet_still-evolving\"    \n\nCode# view result: last 10 ngrams in the first article\ntail(toks_ngram[[1]], 10)\n\n [1] \"dynamic_promote\"          \"promote_strategic\"       \n [3] \"strategic_intervention\"   \"intervention_upholding\"  \n [5] \"upholding_traditional\"    \"traditional_principles\"  \n [7] \"principles_standards\"     \"standards_evidence-based\"\n [9] \"evidence-based_research\"  \"research_communication\"  \n\n\nDFM with bigrams\n\nCode# create dfm from bigrams tokens object and group by journal \ndfm_bigrams &lt;- toks_ngram %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = abbreviation)\n\n# another wordcloud...why not!\nset.seed(101)\ntextplot_wordcloud(dfm_bigrams, \n                   min_count = 20, # include word only if it occurs at least n times in data set \n                   random_order = FALSE, \n                   rotation = 0.25,\n    color = RColorBrewer::brewer.pal(8, \"Dark2\"))\n\n\n\nCode# cloud that compares top bigrams for each journal\ndfm_bigrams %&gt;%\n  dfm_trim(min_termfreq = 15,\n           verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)"
  },
  {
    "objectID": "demos/week02/wk02_wos_journals.html",
    "href": "demos/week02/wk02_wos_journals.html",
    "title": "Wk 02: Exploring TC journals (pt 1)",
    "section": "",
    "text": "Our source data is split between two CSV files, one for the journal “Technical Communication Quarterly” and one for “Technical Communication.”\n\nCode# install and load packages\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(readr)\n\n# read in both csv files\ntcq_data_raw &lt;- read_csv(\"data/tcq_wos_data.csv\")\ntc_data_raw &lt;- read_csv(\"data/tc_wos_data.csv\")"
  },
  {
    "objectID": "demos/week02/wk02_wos_journals.html#install-packages-and-read-in-data",
    "href": "demos/week02/wk02_wos_journals.html#install-packages-and-read-in-data",
    "title": "Wk 02: Exploring TC journals (pt 1)",
    "section": "",
    "text": "Our source data is split between two CSV files, one for the journal “Technical Communication Quarterly” and one for “Technical Communication.”\n\nCode# install and load packages\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\nlibrary(readr)\n\n# read in both csv files\ntcq_data_raw &lt;- read_csv(\"data/tcq_wos_data.csv\")\ntc_data_raw &lt;- read_csv(\"data/tc_wos_data.csv\")"
  },
  {
    "objectID": "demos/week02/wk02_wos_journals.html#data-preparation",
    "href": "demos/week02/wk02_wos_journals.html#data-preparation",
    "title": "Wk 02: Exploring TC journals (pt 1)",
    "section": "Data preparation",
    "text": "Data preparation\nLet’s take a quick look at the data and clean up any issues. Some questions we might answer:\n\nHow many rows and columns are in each dataset?\nWhat do the rows and columns correspond to?\n\n\nCode# shows dimensions (number of rows and columns)\ndim(tcq_data_raw) \n\n[1] 582  22\n\nCodedim(tc_data_raw)\n\n[1] 332  69\n\nCodeglimpse(tcq_data_raw)\n\nRows: 582\nColumns: 22\n$ `Publication Type`           &lt;chr&gt; \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"…\n$ Authors                      &lt;chr&gt; \"Fuglsby, BJ; Veeramoothoo, S\", \"Frost, E…\n$ `Author Full Names`          &lt;chr&gt; \"Fuglsby, Brandi J.; Veeramoothoo, Saveen…\n$ `Article Title`              &lt;chr&gt; \"Regulating Emotions for Social Action: E…\n$ `Source Title`               &lt;chr&gt; \"TECHNICAL COMMUNICATION QUARTERLY\", \"TEC…\n$ `Document Type`              &lt;chr&gt; \"Article\", \"Article\", \"Article\", \"Article…\n$ Abstract                     &lt;chr&gt; \"This article describes students' emotion…\n$ `Cited References`           &lt;chr&gt; \"Agboka Godwin Y., 2018, CITIZENSHIP ADVO…\n$ `Cited Reference Count`      &lt;dbl&gt; 32, 86, 54, 28, 66, 22, 27, 38, 5, 20, 50…\n$ `Times Cited, WoS Core`      &lt;dbl&gt; 0, 0, 1, 7, 7, 10, 11, 8, 0, 0, 8, 0, 9, …\n$ `Times Cited, All Databases` &lt;dbl&gt; 0, 0, 1, 7, 7, 10, 11, 8, 0, 0, 8, 0, 9, …\n$ `Publication Date`           &lt;chr&gt; \"JAN 2\", NA, NA, NA, NA, NA, NA, NA, NA, …\n$ `Publication Year`           &lt;dbl&gt; 2023, 2021, 2020, 2015, 2014, 2013, 2010,…\n$ Volume                       &lt;dbl&gt; 32, 30, 29, 24, 23, 22, 19, 18, 14, 29, 2…\n$ Issue                        &lt;dbl&gt; 1, 1, 2, 3, 3, 2, 3, 2, 1, 2, 4, 4, 1, 4,…\n$ `Start Page`                 &lt;chr&gt; \"98\", \"48\", \"136\", \"217\", \"184\", \"172\", \"…\n$ `End Page`                   &lt;chr&gt; \"113\", \"62\", \"154\", \"234\", \"206\", \"190\", …\n$ `Article Number`             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DOI                          &lt;chr&gt; \"10.1080/10572252.2022.2079725\", \"10.1080…\n$ `DOI Link`                   &lt;chr&gt; \"http://dx.doi.org/10.1080/10572252.2022.…\n$ `Early Access Date`          &lt;chr&gt; \"JUN 2022\", NA, NA, NA, NA, NA, NA, NA, N…\n$ `Web of Science Record`      &lt;chr&gt; \"View Full Record in Web of Science\", \"Vi…\n\nCodeglimpse(tc_data_raw)\n\nRows: 332\nColumns: 69\n$ `Publication Type`           &lt;chr&gt; \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"…\n$ Authors                      &lt;chr&gt; \"Britt, BC; Britt, RK\", \"Davis, C\", \"Colt…\n$ `Author Full Names`          &lt;chr&gt; \"Britt, Brian C.; Britt, Rebecca K.\", \"Da…\n$ `Book Author Full Names`     &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Group Authors`              &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Article Title`              &lt;chr&gt; \"The Roles of Medium and Narrative Believ…\n$ `Source Title`               &lt;chr&gt; \"TECHNICAL COMMUNICATION\", \"TECHNICAL COM…\n$ `Book Series Title`          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Book Series Subtitle`       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Language                     &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Document Type`              &lt;chr&gt; \"Article\", \"Article\", \"Article\", \"Article…\n$ `Conference Title`           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Conference Date`            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Conference Location`        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Conference Sponsor`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Conference Host`            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Author Keywords`            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Keywords Plus`              &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Abstract                     &lt;chr&gt; \"Purpose: This study investigates the rol…\n$ Addresses                    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Affiliations                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Reprint Addresses`          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Email Addresses`            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Researcher Ids`             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ORCIDs                       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Funding Orgs`               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Funding Name Preferred`     &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Funding Text`               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Cited References`           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Cited Reference Count`      &lt;dbl&gt; 82, 24, 73, 46, 30, 28, 12, 20, 27, 24, 7…\n$ `Times Cited, WoS Core`      &lt;dbl&gt; 1, 0, 4, 10, 4, 11, 0, 2, 19, 3, 3, 3, 0,…\n$ `Times Cited, All Databases` &lt;dbl&gt; 1, 0, 4, 10, 4, 11, 0, 2, 19, 3, 3, 3, 0,…\n$ `180 Day Usage Count`        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Since 2013 Usage Count`     &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Publisher                    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Publisher City`             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Publisher Address`          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ISSN                         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ eISSN                        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ISBN                         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Journal Abbreviation`       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Journal ISO Abbreviation`   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Publication Date`           &lt;chr&gt; \"AUG\", \"AUG\", \"FEB\", \"AUG\", \"NOV\", \"FEB\",…\n$ `Publication Year`           &lt;dbl&gt; 2021, 2019, 2019, 2018, 2014, 2012, 2011,…\n$ Volume                       &lt;dbl&gt; 68, 66, 66, 65, 61, 59, 58, 54, 54, 54, 6…\n$ Issue                        &lt;dbl&gt; 3, 3, 1, 3, 4, 1, 4, 4, 3, 1, 4, 3, 3, 1,…\n$ `Part Number`                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Supplement                   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Special Issue`              &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Meeting Abstract`           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Start Page`                 &lt;chr&gt; \"76\", \"272\", \"53\", \"293\", \"215\", \"1\", \"11…\n$ `End Page`                   &lt;chr&gt; \"96\", \"283\", \"67\", \"308\", \"231\", \"7\", \"13…\n$ `Article Number`             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DOI                          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `DOI Link`                   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Book DOI`                   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Early Access Date`          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Number of Pages`            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `WoS Categories`             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Web of Science Index`       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Research Areas`             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `IDS Number`                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Pubmed Id`                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Open Access Designations`   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Highly Cited Status`        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Hot Paper Status`           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Date of Export`             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `UT (Unique WOS ID)`         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Web of Science Record`      &lt;chr&gt; \"View Full Record in Web of Science\", \"Vi…\n\n\nCleaning names\nWe’ll start by cleaning up the names with the janitor package. The convention is lowercase and joined by underscores.\n\nCode#install.packages(\"janitor\")\nlibrary(janitor)\n\n# use help(\"library_name\") for a description\nhelp(\"janitor\") \n\n\n# Syntax method 1 (on TCQ)\ntcq_data_raw &lt;- clean_names(tcq_data_raw)\n\nglimpse(tcq_data_raw)\n\nRows: 582\nColumns: 22\n$ publication_type          &lt;chr&gt; \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\",…\n$ authors                   &lt;chr&gt; \"Fuglsby, BJ; Veeramoothoo, S\", \"Frost, EA\",…\n$ author_full_names         &lt;chr&gt; \"Fuglsby, Brandi J.; Veeramoothoo, Saveena (…\n$ article_title             &lt;chr&gt; \"Regulating Emotions for Social Action: Emot…\n$ source_title              &lt;chr&gt; \"TECHNICAL COMMUNICATION QUARTERLY\", \"TECHNI…\n$ document_type             &lt;chr&gt; \"Article\", \"Article\", \"Article\", \"Article\", …\n$ abstract                  &lt;chr&gt; \"This article describes students' emotional …\n$ cited_references          &lt;chr&gt; \"Agboka Godwin Y., 2018, CITIZENSHIP ADVOCAC…\n$ cited_reference_count     &lt;dbl&gt; 32, 86, 54, 28, 66, 22, 27, 38, 5, 20, 50, 4…\n$ times_cited_wo_s_core     &lt;dbl&gt; 0, 0, 1, 7, 7, 10, 11, 8, 0, 0, 8, 0, 9, 0, …\n$ times_cited_all_databases &lt;dbl&gt; 0, 0, 1, 7, 7, 10, 11, 8, 0, 0, 8, 0, 9, 0, …\n$ publication_date          &lt;chr&gt; \"JAN 2\", NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ publication_year          &lt;dbl&gt; 2023, 2021, 2020, 2015, 2014, 2013, 2010, 20…\n$ volume                    &lt;dbl&gt; 32, 30, 29, 24, 23, 22, 19, 18, 14, 29, 28, …\n$ issue                     &lt;dbl&gt; 1, 1, 2, 3, 3, 2, 3, 2, 1, 2, 4, 4, 1, 4, 2,…\n$ start_page                &lt;chr&gt; \"98\", \"48\", \"136\", \"217\", \"184\", \"172\", \"300…\n$ end_page                  &lt;chr&gt; \"113\", \"62\", \"154\", \"234\", \"206\", \"190\", \"31…\n$ article_number            &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ doi                       &lt;chr&gt; \"10.1080/10572252.2022.2079725\", \"10.1080/10…\n$ doi_link                  &lt;chr&gt; \"http://dx.doi.org/10.1080/10572252.2022.207…\n$ early_access_date         &lt;chr&gt; \"JUN 2022\", NA, NA, NA, NA, NA, NA, NA, NA, …\n$ web_of_science_record     &lt;chr&gt; \"View Full Record in Web of Science\", \"View …\n\nCode# Syntax method 2 (on TC)\ntc_data_raw &lt;- tc_data_raw %&gt;%\n  clean_names()\n\nglimpse(tc_data_raw)\n\nRows: 332\nColumns: 69\n$ publication_type          &lt;chr&gt; \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\",…\n$ authors                   &lt;chr&gt; \"Britt, BC; Britt, RK\", \"Davis, C\", \"Colton,…\n$ author_full_names         &lt;chr&gt; \"Britt, Brian C.; Britt, Rebecca K.\", \"Davis…\n$ book_author_full_names    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ group_authors             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ article_title             &lt;chr&gt; \"The Roles of Medium and Narrative Believabi…\n$ source_title              &lt;chr&gt; \"TECHNICAL COMMUNICATION\", \"TECHNICAL COMMUN…\n$ book_series_title         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ book_series_subtitle      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ language                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ document_type             &lt;chr&gt; \"Article\", \"Article\", \"Article\", \"Article\", …\n$ conference_title          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conference_date           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conference_location       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conference_sponsor        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ conference_host           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ author_keywords           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ keywords_plus             &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ abstract                  &lt;chr&gt; \"Purpose: This study investigates the role o…\n$ addresses                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ affiliations              &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ reprint_addresses         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ email_addresses           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ researcher_ids            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ orci_ds                   &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ funding_orgs              &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ funding_name_preferred    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ funding_text              &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ cited_references          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ cited_reference_count     &lt;dbl&gt; 82, 24, 73, 46, 30, 28, 12, 20, 27, 24, 72, …\n$ times_cited_wo_s_core     &lt;dbl&gt; 1, 0, 4, 10, 4, 11, 0, 2, 19, 3, 3, 3, 0, 4,…\n$ times_cited_all_databases &lt;dbl&gt; 1, 0, 4, 10, 4, 11, 0, 2, 19, 3, 3, 3, 0, 4,…\n$ x180_day_usage_count      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ since_2013_usage_count    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ publisher                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ publisher_city            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ publisher_address         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ issn                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ e_issn                    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ isbn                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ journal_abbreviation      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ journal_iso_abbreviation  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ publication_date          &lt;chr&gt; \"AUG\", \"AUG\", \"FEB\", \"AUG\", \"NOV\", \"FEB\", \"N…\n$ publication_year          &lt;dbl&gt; 2021, 2019, 2019, 2018, 2014, 2012, 2011, 20…\n$ volume                    &lt;dbl&gt; 68, 66, 66, 65, 61, 59, 58, 54, 54, 54, 68, …\n$ issue                     &lt;dbl&gt; 3, 3, 1, 3, 4, 1, 4, 4, 3, 1, 4, 3, 3, 1, 2,…\n$ part_number               &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ supplement                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ special_issue             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ meeting_abstract          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ start_page                &lt;chr&gt; \"76\", \"272\", \"53\", \"293\", \"215\", \"1\", \"11\", …\n$ end_page                  &lt;chr&gt; \"96\", \"283\", \"67\", \"308\", \"231\", \"7\", \"13\", …\n$ article_number            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ doi                       &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ doi_link                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ book_doi                  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ early_access_date         &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ number_of_pages           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ wo_s_categories           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ web_of_science_index      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ research_areas            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ ids_number                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ pubmed_id                 &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ open_access_designations  &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ highly_cited_status       &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ hot_paper_status          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ date_of_export            &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ ut_unique_wos_id          &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ web_of_science_record     &lt;chr&gt; \"View Full Record in Web of Science\", \"View …\n\n\nSelect columns of interest\nNow we’ll use a function from a library called dplyr to retain only the columns we want. Let’s keep:\n\nauthor_full_names\narticle_title\nsource_title\nabstract\ncited_references\ncited_reference_count\npublication_year\npublication_type\n\nFirst on TCQ\n\nCodelibrary(dplyr)\n\n# the select function from dplyr on TCQ data\ntcq_clean &lt;- tcq_data_raw %&gt;%\n  select(author_full_names,\n         article_title,\n         source_title,\n         abstract,\n         cited_references,\n         cited_reference_count,\n         publication_year,\n         publication_type)\n\nglimpse(tcq_clean)\n\nRows: 582\nColumns: 8\n$ author_full_names     &lt;chr&gt; \"Fuglsby, Brandi J.; Veeramoothoo, Saveena (Chak…\n$ article_title         &lt;chr&gt; \"Regulating Emotions for Social Action: Emotiona…\n$ source_title          &lt;chr&gt; \"TECHNICAL COMMUNICATION QUARTERLY\", \"TECHNICAL …\n$ abstract              &lt;chr&gt; \"This article describes students' emotional inte…\n$ cited_references      &lt;chr&gt; \"Agboka Godwin Y., 2018, CITIZENSHIP ADVOCACY; […\n$ cited_reference_count &lt;dbl&gt; 32, 86, 54, 28, 66, 22, 27, 38, 5, 20, 50, 4, 55…\n$ publication_year      &lt;dbl&gt; 2023, 2021, 2020, 2015, 2014, 2013, 2010, 2009, …\n$ publication_type      &lt;chr&gt; \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\"…\n\n\nNow on TC\n\nCode# the select function from dplyr on TCQ data\ntc_clean &lt;- tc_data_raw %&gt;%\n  select(author_full_names,\n         article_title,\n         source_title,\n         abstract,\n         cited_references,\n         cited_reference_count,\n         publication_year,\n         publication_type)\n\nglimpse(tc_clean)\n\nRows: 332\nColumns: 8\n$ author_full_names     &lt;chr&gt; \"Britt, Brian C.; Britt, Rebecca K.\", \"Davis, Ca…\n$ article_title         &lt;chr&gt; \"The Roles of Medium and Narrative Believability…\n$ source_title          &lt;chr&gt; \"TECHNICAL COMMUNICATION\", \"TECHNICAL COMMUNICAT…\n$ abstract              &lt;chr&gt; \"Purpose: This study investigates the role of ne…\n$ cited_references      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ cited_reference_count &lt;dbl&gt; 82, 24, 73, 46, 30, 28, 12, 20, 27, 24, 72, 34, …\n$ publication_year      &lt;dbl&gt; 2021, 2019, 2019, 2018, 2014, 2012, 2011, 2007, …\n$ publication_type      &lt;chr&gt; \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\"…\n\n\nCombine the two sets\n\nCodeboth_data &lt;- rbind(tcq_clean, tc_clean)\n\ndim(both_data)\n\n[1] 914   8"
  },
  {
    "objectID": "demos/week02/wk02_wos_journals.html#exploring-the-data",
    "href": "demos/week02/wk02_wos_journals.html#exploring-the-data",
    "title": "Wk 02: Exploring TC journals (pt 1)",
    "section": "Exploring the data",
    "text": "Exploring the data\nSome quick ways to examine at a high level\n\nCode# provides overview of numeric variables\nsummary(both_data) \n\n author_full_names  article_title      source_title         abstract        \n Length:914         Length:914         Length:914         Length:914        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n cited_references   cited_reference_count publication_year publication_type  \n Length:914         Min.   :  0.00        Min.   :2005     Length:914        \n Class :character   1st Qu.: 18.00        1st Qu.:2008     Class :character  \n Mode  :character   Median : 34.00        Median :2014     Mode  :character  \n                    Mean   : 35.59        Mean   :2014                       \n                    3rd Qu.: 51.00        3rd Qu.:2019                       \n                    Max.   :169.00        Max.   :2023                       \n\nCode# overview of target column\nsummary(both_data$publication_year)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2005    2008    2014    2014    2019    2023 \n\nCode# for each column, shows data types and first few observations\nstr(both_data) \n\ntibble [914 × 8] (S3: tbl_df/tbl/data.frame)\n $ author_full_names    : chr [1:914] \"Fuglsby, Brandi J.; Veeramoothoo, Saveena (Chakrika)\" \"Frost, Erin A.\" \"Weber, Ryan\" \"Kreuter, Nate\" ...\n $ article_title        : chr [1:914] \"Regulating Emotions for Social Action: Emotional Intelligence's Role in TPC\" \"Ultrasound, Gender, and Consent: An Apparent Feminist Analysis of Medical Imaging Rhetorics\" \"The News from Mars\" \"The US Intelligence Community's Mathematical Ideology of Technical Communication\" ...\n $ source_title         : chr [1:914] \"TECHNICAL COMMUNICATION QUARTERLY\" \"TECHNICAL COMMUNICATION QUARTERLY\" \"TECHNICAL COMMUNICATION QUARTERLY\" \"TECHNICAL COMMUNICATION QUARTERLY\" ...\n $ abstract             : chr [1:914] \"This article describes students' emotional intelligence (EI) development when participating in the Trans-Atlant\"| __truncated__ \"This article uses an apparent feminist approach to engage a two-part research question: First, does gender affe\"| __truncated__ \"Bruno Latour advocates for portrayals of science in the making but does not explain how the public can access t\"| __truncated__ \"Reading historical intelligence community documents primarily through the lens of Kenneth Burke's essay ''Seman\"| __truncated__ ...\n $ cited_references     : chr [1:914] \"Agboka Godwin Y., 2018, CITIZENSHIP ADVOCACY; [Anonymous], 2007, SOCIAL JUSTICE THEOR; [Anonymous], 2020, T ATL\"| __truncated__ \"American Cancer Society, 2019, REC PROST CANC EARL; American College of Obstetricians and Gynecologists, 2019, \"| __truncated__ \"[Anonymous], 2007, J TECH WRIT COMMUN, DOI DOI 10.2190/TW.37.3.C; [Anonymous], 2001, WHAT SHOULD WE TEACH; [Ano\"| __truncated__ \"[Anonymous], 2005, REP PRES US; Barnard I, 2010, COLL COMPOS COMMUN, V61, P434; Best J., 2001, DAMNED LIES STAT\"| __truncated__ ...\n $ cited_reference_count: num [1:914] 32 86 54 28 66 22 27 38 5 20 ...\n $ publication_year     : num [1:914] 2023 2021 2020 2015 2014 ...\n $ publication_type     : chr [1:914] \"J\" \"J\" \"J\" \"J\" ...\n\nCode# shows first few rows\nhead(both_data)\n\n# A tibble: 6 × 8\n  author_full_names         article_title source_title abstract cited_references\n  &lt;chr&gt;                     &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;           \n1 Fuglsby, Brandi J.; Veer… Regulating E… TECHNICAL C… This ar… Agboka Godwin Y…\n2 Frost, Erin A.            Ultrasound, … TECHNICAL C… This ar… American Cancer…\n3 Weber, Ryan               The News fro… TECHNICAL C… Bruno L… [Anonymous], 20…\n4 Kreuter, Nate             The US Intel… TECHNICAL C… Reading… [Anonymous], 20…\n5 Buehl, Jonathan           Toward an Et… TECHNICAL C… Over th… ANDERSON C, 199…\n6 Lauer, Claire             Examining th… TECHNICAL C… This ar… [Anonymous], SE…\n# ℹ 3 more variables: cited_reference_count &lt;dbl&gt;, publication_year &lt;dbl&gt;,\n#   publication_type &lt;chr&gt;\n\nCode# shows the first n rows\n\nhead(both_data, n = 10)\n\n# A tibble: 10 × 8\n   author_full_names        article_title source_title abstract cited_references\n   &lt;chr&gt;                    &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;           \n 1 Fuglsby, Brandi J.; Vee… Regulating E… TECHNICAL C… This ar… Agboka Godwin Y…\n 2 Frost, Erin A.           Ultrasound, … TECHNICAL C… This ar… American Cancer…\n 3 Weber, Ryan              The News fro… TECHNICAL C… Bruno L… [Anonymous], 20…\n 4 Kreuter, Nate            The US Intel… TECHNICAL C… Reading… [Anonymous], 20…\n 5 Buehl, Jonathan          Toward an Et… TECHNICAL C… Over th… ANDERSON C, 199…\n 6 Lauer, Claire            Examining th… TECHNICAL C… This ar… [Anonymous], SE…\n 7 Ding, Huiling            Technical Co… TECHNICAL C… In this… [Anonymous], 20…\n 8 Kynell, Teresa; Tebeaux… The Associat… TECHNICAL C… This ar… Anderson P., 19…\n 9 Kitalong, Karla Saari    Working with… TECHNICAL C… &lt;NA&gt;     ALLEN N, 2002, …\n10 Pihlaja, Beau            Rhetoric, Te… TECHNICAL C… &lt;NA&gt;     Agboka Godwin, …\n# ℹ 3 more variables: cited_reference_count &lt;dbl&gt;, publication_year &lt;dbl&gt;,\n#   publication_type &lt;chr&gt;\n\nCode#shows last few rows\ntail(both_data)\n\n# A tibble: 6 × 8\n  author_full_names         article_title source_title abstract cited_references\n  &lt;chr&gt;                     &lt;chr&gt;         &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;           \n1 Wagner, Christian; Schro… Capabilities… TECHNICAL C… Purpose… &lt;NA&gt;            \n2 Lentz, Leo; De Jong, Men… How Do Exper… TECHNICAL C… Discuss… &lt;NA&gt;            \n3 Rife, Martine Courant     Technical co… TECHNICAL C… Maintai… &lt;NA&gt;            \n4 Mott, Richard K.; Ford, … The converge… TECHNICAL C… States … &lt;NA&gt;            \n5 Thrush, Emily A.; Hooper… Industry and… TECHNICAL C… Details… &lt;NA&gt;            \n6 Theofanos, MF; Redish, J  Helping low-… TECHNICAL C… This is… &lt;NA&gt;            \n# ℹ 3 more variables: cited_reference_count &lt;dbl&gt;, publication_year &lt;dbl&gt;,\n#   publication_type &lt;chr&gt;\n\nCode# creates a frequency table for a categorical variable \ntable(both_data$publication_year) \n\n\n2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 \n  68   53   69   49   42   39   27   42   34   35   41   41   48   52   60   58 \n2021 2022 2023 \n  56   53   47 \n\n\nVisualize\nLet’s create some exploratory visualizations.\nVisualize the articles published by year\n\nCode# count of articles by year\nggplot(both_data, aes(x = publication_year)) +\n  geom_bar() +\n  labs(title = \"Articles per Year in TC and TCQ\",\n       X = \"Publication Year\",\n       Y = \"Number of Articles\") +\n  theme_light()\n\n\n\n\nAdd another variable to the display\n\nCode# count of articles by year, by journal\nggplot(both_data, aes(x = publication_year, fill = source_title)) +\n  geom_bar() +\n  labs(title = \"Articles per Year in TC and TCQ\",\n       X = \"Publication Year\",\n       Y = \"Number of Articles\") +\n  theme_light()\n\n\n\n\nCreate two displays using facet wrap\n\nCode# count of articles by year, by journal\nggplot(both_data, aes(x = publication_year)) +\n  geom_bar() +\n  labs(title = \"Articles per Year in TC and TCQ\",\n       X = \"Publication Year\",\n       Y = \"Number of Articles\") +\n   facet_wrap(~ source_title, ncol = 2) # Change ncol as needed\n\n\n\n\nSpice things up with a theme from ggthemes\nCheck out this ggthemes gallery\n\nCode#install.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n# count of articles by year, by journal\nggplot(both_data, aes(x = publication_year)) +\n  geom_bar() +\n  labs(title = \"Articles per Year in TC and TCQ\",\n       X = \"Publication Year\",\n       Y = \"Number of Articles\") +\n   facet_wrap(~ source_title, ncol = 1) +\n  theme_economist()\n\n\n\n\n\nCode# count of articles by year, by journal\nyear_plot &lt;- ggplot(both_data, aes(x = publication_year)) +\n  geom_bar() +\n  labs(title = \"Articles per Year in TC and TCQ\",\n       X = \"Publication Year\",\n       Y = \"Number of Articles\") +\n   facet_wrap(~ source_title, ncol = 1) +\n  theme_economist_white()\n\nyear_plot\n\n\n\n\nAdd labels above the bars\n\nCode# add labels above the bars\nyear_plot &lt;- year_plot +\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5, size = 4)\n\nyear_plot\n\n\n\n\nSave the chart\n\nCodehelp(ggsave)\nggsave(\"plots/articles_barplot.png\", \n       plot = year_plot, \n       width = 8, \n       height = 6, \n       dpi = 300,\n       bg = NULL)\n\n\nSave our dataset\n\nCode# save as csv\n\nwrite.csv(both_data, \"data_out/both_data.csv\")\n\nsave(both_data, file = \"data_out/both_data.RData\")"
  },
  {
    "objectID": "demos/week02/wk02_wos_journals.html#what-about-other-journals",
    "href": "demos/week02/wk02_wos_journals.html#what-about-other-journals",
    "title": "Wk 02: Exploring TC journals (pt 1)",
    "section": "What about other journals?",
    "text": "What about other journals?\nGather and add to our visuals all articles published after 2005 from Web of Science in these journals:\n\nJournal of Business and Technical Communication\nWritten Communication"
  },
  {
    "objectID": "demos/week03/wk03_wos_journals.html",
    "href": "demos/week03/wk03_wos_journals.html",
    "title": "Wk 03: Exploring TC journals, (pt 2)",
    "section": "",
    "text": "Last week we started with a CSV of results for TC and a separate CSV of results for TCQ. We cleaned, explored, merged, and visualized the data before saving it out in two formats: an .RData file and a .csv.\nThis week we’ll add two more journals…"
  },
  {
    "objectID": "demos/week03/wk03_wos_journals.html#overview",
    "href": "demos/week03/wk03_wos_journals.html#overview",
    "title": "Wk 03: Exploring TC journals, (pt 2)",
    "section": "",
    "text": "Last week we started with a CSV of results for TC and a separate CSV of results for TCQ. We cleaned, explored, merged, and visualized the data before saving it out in two formats: an .RData file and a .csv.\nThis week we’ll add two more journals…"
  },
  {
    "objectID": "demos/week03/wk03_wos_journals.html#load-libraries-and-data",
    "href": "demos/week03/wk03_wos_journals.html#load-libraries-and-data",
    "title": "Wk 03: Exploring TC journals, (pt 2)",
    "section": "Load libraries and data",
    "text": "Load libraries and data\n\nCodelibrary(tidyverse)\n\nload(\"data/both_data.RData\")\n\n\nLet’s confirm the data is what we expect.\n\nCodeglimpse(both_data)\n\nRows: 914\nColumns: 8\n$ author_full_names     &lt;chr&gt; \"Fuglsby, Brandi J.; Veeramoothoo, Saveena (Chak…\n$ article_title         &lt;chr&gt; \"Regulating Emotions for Social Action: Emotiona…\n$ source_title          &lt;chr&gt; \"TECHNICAL COMMUNICATION QUARTERLY\", \"TECHNICAL …\n$ abstract              &lt;chr&gt; \"This article describes students' emotional inte…\n$ cited_references      &lt;chr&gt; \"Agboka Godwin Y., 2018, CITIZENSHIP ADVOCACY; […\n$ cited_reference_count &lt;dbl&gt; 32, 86, 54, 28, 66, 22, 27, 38, 5, 20, 50, 4, 55…\n$ publication_year      &lt;dbl&gt; 2023, 2021, 2020, 2015, 2014, 2013, 2010, 2009, …\n$ publication_type      &lt;chr&gt; \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\"…\n\n\nAnd recreate our plot.\n\nCodelibrary(ggthemes)\n\n# count of articles by year, by journal\nyear_plot &lt;- ggplot(both_data, aes(x = publication_year)) +\n  geom_bar() +\n  labs(title = \"Articles per Year in TC and TCQ\",\n       x = \"Publication Year\",\n       y = \"Number of Articles\") +\n   facet_wrap(~ source_title, ncol = 1) +\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5, size = 4)+\n  theme_economist_white()\n\nyear_plot"
  },
  {
    "objectID": "demos/week03/wk03_wos_journals.html#read-in-new-data",
    "href": "demos/week03/wk03_wos_journals.html#read-in-new-data",
    "title": "Wk 03: Exploring TC journals, (pt 2)",
    "section": "Read in new data",
    "text": "Read in new data\nI’ve gone to web of science and queried for articles published 2005 to present in two journals:\n\nJournal of Business and Technical Communication\nWritten Communication\n\nBoth are .xls files located in my data folder.\n\nCodelibrary(readxl)\n\n# read in JBTC data\njbtc &lt;- read_excel(\"data/wos_jbtc.xls\")\n\n# read in Written Comm data\nwc &lt;- read_excel(\"data/wos_wc.xls\")"
  },
  {
    "objectID": "demos/week03/wk03_wos_journals.html#join-datasets",
    "href": "demos/week03/wk03_wos_journals.html#join-datasets",
    "title": "Wk 03: Exploring TC journals, (pt 2)",
    "section": "Join datasets",
    "text": "Join datasets\nOur goal is to add our two new datasets into the TC and TCQ data we already created.\nCheck data\nUpon checking the dimensions of the data, I see I have a lot of data columns I don’t need.\n\nCodedim(jbtc)\n\n[1] 300  72\n\nCodedim(wc)\n\n[1] 323  72\n\n\nClean names\nBefore we can join the datasets, we have to clean names.\n\nCodelibrary(janitor)\n\n# clean names of jbtc and wc \njbtc &lt;- clean_names(jbtc)\n\nwc &lt;- clean_names(wc)\n\n\nCombine jbtc and wc with bind_rows\nBecause they have the same variables, we can use bind_rows\n\nCode# create a combined dataset with bind rows\njbtc_wc &lt;- bind_rows(wc, jbtc)\n\n# check dimensions\ndim(jbtc_wc)\n\n[1] 623  72\n\n\nSelect target columns\nWe’ll reuse our code from week 2 to select the columns of interest.\n\nCodejbtc_wc &lt;- jbtc_wc %&gt;%\n  select(author_full_names,\n         article_title,\n         source_title,\n         abstract,\n         cited_references,\n         cited_reference_count,\n         publication_year,\n         publication_type)\n\n\nJoin the two combined sets\nBefore we join them, let’s do some math to see how many articles we should end up with.\n\nCode# number of articles (rows) in both_data (TC and TCQ)\ntc_and_tcq &lt;- nrow(both_data)\n\n# number of articles (rows) in jbtc_wc (JBTC and WC)\njbtc_and_wc &lt;- nrow(jbtc_wc)\n\n# The sum of articles in each\nprint(tc_and_tcq + jbtc_and_wc)\n\n[1] 1537\n\n\nNow, let’s join them and see how many articles we actually end up with.\n\nCode# combine the two sets (tcq+tc and jbtc+wc)\nfull_data &lt;- bind_rows(both_data, jbtc_wc)\n\n# check the dimensions\ndim(full_data)\n\n[1] 1537    8"
  },
  {
    "objectID": "demos/week03/wk03_wos_journals.html#explore-the-full-data",
    "href": "demos/week03/wk03_wos_journals.html#explore-the-full-data",
    "title": "Wk 03: Exploring TC journals, (pt 2)",
    "section": "Explore the full data",
    "text": "Explore the full data\n\nCode# provides overview of numeric variables\nsummary(full_data) \n\n author_full_names  article_title      source_title         abstract        \n Length:1537        Length:1537        Length:1537        Length:1537       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n cited_references   cited_reference_count publication_year publication_type  \n Length:1537        Min.   :  0.00        Min.   :2005     Length:1537       \n Class :character   1st Qu.: 18.00        1st Qu.:2009     Class :character  \n Mode  :character   Median : 34.00        Median :2015     Mode  :character  \n                    Mean   : 35.59        Mean   :2014                       \n                    3rd Qu.: 51.00        3rd Qu.:2019                       \n                    Max.   :169.00        Max.   :2023                       \n                    NA's   :623                                              \n\n\nDo we have all four journals represented?\n\nCodeunique(full_data$source_title)\n\n[1] \"TECHNICAL COMMUNICATION QUARTERLY\"              \n[2] \"TECHNICAL COMMUNICATION\"                        \n[3] \"WRITTEN COMMUNICATION\"                          \n[4] \"JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\"\n\n\nAdd journal abbreviation column\nFor visualization purposes, let’s add a new column of journal abbreviations\n\nCode# this dataframe maps each full name to an abbreviation\njournal_abbreviations &lt;- data.frame(\n  full_name = c(\"TECHNICAL COMMUNICATION QUARTERLY\", \"TECHNICAL COMMUNICATION\", \"WRITTEN COMMUNICATION\", \"JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\"),\n  abbreviation = c(\"TCQ\", \"TC\", \"WC\", \"JBTC\")\n)\n\njournal_abbreviations\n\n                                        full_name abbreviation\n1               TECHNICAL COMMUNICATION QUARTERLY          TCQ\n2                         TECHNICAL COMMUNICATION           TC\n3                           WRITTEN COMMUNICATION           WC\n4 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION         JBTC\n\n\nNow we can merge the full data with the abbreviations\n\nCode# Merge the data frames to add the 'abbreviation' column\nfull_data &lt;- merge(full_data, journal_abbreviations, by.x = \"source_title\", by.y = \"full_name\", all.x = TRUE)\n\nunique(full_data$abbreviation)\n\n[1] \"JBTC\" \"TC\"   \"TCQ\"  \"WC\"  \n\nCodefull_data[sample(1:nrow(full_data), 20, replace = FALSE),]\n\n                                        source_title\n1290                           WRITTEN COMMUNICATION\n330                          TECHNICAL COMMUNICATION\n149  JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n1321                           WRITTEN COMMUNICATION\n545                          TECHNICAL COMMUNICATION\n1277                           WRITTEN COMMUNICATION\n979                TECHNICAL COMMUNICATION QUARTERLY\n189  JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n864                TECHNICAL COMMUNICATION QUARTERLY\n536                          TECHNICAL COMMUNICATION\n1002               TECHNICAL COMMUNICATION QUARTERLY\n103  JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n1493                           WRITTEN COMMUNICATION\n578                          TECHNICAL COMMUNICATION\n1090               TECHNICAL COMMUNICATION QUARTERLY\n1067               TECHNICAL COMMUNICATION QUARTERLY\n781                TECHNICAL COMMUNICATION QUARTERLY\n220  JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n1195               TECHNICAL COMMUNICATION QUARTERLY\n443                          TECHNICAL COMMUNICATION\n                                                  author_full_names\n1290           Ellison, Tisha Lewis; Robinson, Bradley; Qiu, Tairan\n330                                                Shirley, Beth J.\n149                              McNair, Lisa D.; Paretti, Marie C.\n1321                                                 Hoglund, Heidi\n545                                 Boettger, Ryan K.; Friess, Erin\n1277 Salas, Nayme; Pascual, Mariona; Birello, Marilisa; Cross, Anna\n979                                              Tebeaux, Elizabeth\n189                      Johnson-Eilola, Johndan; Selber, Stuart A.\n864                                             Williams, Miriam F.\n536                                                  Tirdatov, Ilya\n1002                                                  Ren, Jingfang\n103                        Petersen, Emily January; Walton, Rebecca\n1493                           Olive, Thierry; Barbier, Marie-Laure\n578                            Thielsch, Meinald T.; Perabo, Isabel\n1090                                               Beliwoar, Hannah\n1067                                               Graham, S. Scott\n781                                                Sapienza, Filipp\n220                                                Hasrati, Mostafa\n1195                                             Kelly, Ashley Rose\n443                                     Voss, Dan; Flammia, Madelyn\n                                                                                                                                      article_title\n1290                        Examining African American Girls' Literate Intersectional Identities Through Journal Entries and Discussions About STEM\n330                                                    Post-Fact Fact Sheets: Dissociative Framing as a Strategy to Work Past Climate Change Denial\n149  Activity Theory, Speech Acts, and the Doctrine of Infelicity'': Connecting Language and Technology in Globally Networked Learning Environments\n1321                                                                             The Heartbeat of Poetry: Student Videomaking in Response to Poetry\n545                                Content and Authorship Patterns in Technical Communication Journals (1996-2017): A Quantitative Content Analysis\n1277                                                                      Embedding Explicit Linguistic Instruction in an SRSD Writing Intervention\n979                            English Agriculture and Estate Management Instructions, 1200-1700: From Orality to Textuality to Modern Instructions\n189                                                                          Strange Days: Creating Flexible Pedagogies for Technical Communication\n864               Gun Control and Gun Rights: A Conceptual Framework for Analyzing Public Policy Issues in Technical and Professional Communication\n536                                                                                                    Web-Based Crowd Funding: Rhetoric of Success\n1002                                                                                                                  Becoming a Writing Researcher\n103                                                       Bridging Analysis and Action: How Feminist Scholarship Can Inform the Social Justice Turn\n1493                          Processing Time and Cognitive Effort of Longhand Note Taking When Reading and Summarizing a Structured or Linear Text\n578                                                                                                     Use and Evaluation of Presentation Software\n1090                                                               Everyday Matters: Reception and Use as Productive Design of Health-Related Texts\n1067                                                   Agency and the Rhetoric of Medicine: Biomedical Brain Scans and the Ontology of Fibromyalgia\n781                                                                                    A Rhetorical Approach to Single-Sourcing Via Intertextuality\n220                 Material and Credentialing Incentives as Symbolic Violence: Local Engagement and Global Participation Through Joint Publication\n1195                                                               On the Frontier of Science: An American Rhetoric of Exploration and Exploitation\n443                                 Ethical and intercultural challenges for technical communicators and managers in a shrinking global marketplace\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          abstract\n1290                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This article examines how three African American girls, ages 10 to 18, used journaling and interviews to better understand science, technology, engineering, and mathematics (STEM) as part of their literate identities. Drawing on prior work about literate identities, the authors introduce the concept of literate intersectional identities, which describes how participants' diverse histories, literacies, and identities traverse categories, communities, genres, and modes of meaning within the context of a STEAM workshop. The authors employed open and thematic coding to analyze the girls' journal entries in an effort to answer a question: In what ways do African American girls' journal writings and interviews about STEM reflect and influence their literate identities in a digital app coding workshop? Findings reveal how their writings about race, access, and the underrepresentation of women of color in STEM helped them make sense of their self-assurance, self-awareness, and agency as girls of color interested in STEM careers.\n330  Purpose: This article presents a new rhetorical model for science and technical communication-specifically climate change communication-which the author is calling dissociative framing, in which climate change can be dissociated from the behaviors necessary to mitigate the human contribution to climate change, while positive associations are formed with those behaviors. This model serves as an alternative to the knowledge deficit model still in use in much science communication and is applicable both for students and practitioners of technical communication. Method: The model was developed by examining Matthew Nisbet's work on framing in conjunction with Perelman and Olbrechts-Tyteca's work on dissociation. I conducted a coded rhetorical analysis of two fact sheets produced by the Utah State University Extension Office with information on how their audience can change personal behaviors to mitigate their personal impact on climate change. I suggest how a dissociative frame would present the information more effectively. Results: A dissociative framing model can provide practitioners in technical and professional communication (TPC) a way to work around science skepticism and motivate action, especially when working with short, community-based genres, and can provide teachers of technical communication with a heuristic for instructing students on how to best engage a skeptical audience. Conclusion: While rural communities in the United States are especially prone to climate skepticism, it is important that they be informed and empowered to make the necessary behavioral changes to mitigate the human impact on climate change. Fact sheets published by extension services provide an excellent opportunity to inform and empower. A dissociative framing model provides a clear way to empower these communities with knowledge of how to mitigate their impact on climate change without diving into the political issues embroiled in climate science.\n149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            This article draws on activity theory, politics of the artifact, and speech act theory to analyze how language practices and technology interplay in establishing the social relationships necessary for globally networked teams. Specifically, it uses activity theory to examine how linguistic infelicities and the politics of communication technologies interplay in virtual meetings, thereby demonstrating the importance of grounding professional communication instruction in social as well as technical effectiveness. That is, students must learn not only how to communicate technical concepts clearly and concisely and recognize cultural differences but also how to use language and choose media in ways that produce the social conditions necessary for effective collaboration in globally networked environments. The article analyzes two case studies-a workplace and a classroom-that illustrate how the mediating functions of language and the politics of technology intersect as mediating tools in globally networked activity systems. It then traces the implications of that intersection for professional communication theory and pedagogy.\n1321                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 This article contributes to an emerging body of scholarship on multimodal composition in the poetry classroom through a study of Finnish lower secondary students' digital videomaking in response to poetry. The study explores students' use of semiotic resources in their interpretive work in transmediating a poem into a digital video, with a particular interest in their use of sound elements. Based on social semiotic theory of multimodality, the analysis shows how the students in a variety of ways used sound elements, together with other semiotic resources, to explore their interpretation of the poetic text. Sound elements in particular became a key resource in the interpretive work, giving the students the opportunity to elaborate on topical issues of interest and importance to them while reinforcing their social agency. The study demonstrates the relevance of sound elements in students' digital composing and explorations of poetry. Furthermore, it reveals how the students showed a capacity as well as a willingness to act, to have influence, and to make substantiated claims for recognition regarding critical issues related to sexuality and society.\n545                                                                                                                                                                                                                                                     Purpose: The maturity of technical communication merits a comprehensive, longitudinal analysis of the content published in its leading journals and the scholars who produce this research. Although reflexive research is common in the sciences and social sciences, few studies have analyzed the body of research in technical communication. Clarity on content and authorship patterns can help position the field for future relevance and sustainability. Method: We conducted a quantitative content analysis on 672 articles published in five leading technical communication journals from 1996-2017. Articles were coded on nine content variables related to primary topic, primary audience, and authorship. We subsequently conducted a correspondence analysis on the variables to identify how specific content areas associated with the journals. Results: Content and authorship patterns were near identical to the patterns found in the field 30 years prior. The journals published content primarily focused on rhetoric, genre, pedagogy, and diversity. In contrast, field-defining topics-usability/UX, comprehension, design, and editing and style-appeared in the sample less than expected. A majority of research was single-authored and written by female first authors; further, a majority of the first authors had academic affiliations in the United States. Conclusion: Scholars must consider if these content and authorship patterns are the products of deliberate choices and, if so, if this is the field's inevitable trajectory for the next 30 years. We argue that certain topics are being overproduced while other topics that established the field are being underproduced and, in some cases, being assumed by other disciplines.\n1277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Teaching linguistic aspects relevant to text construction is an essential component of any thorough writing instruction program, despite the conflicting evidence regarding its effectiveness. In this study, 889 second- and fourth-grade students were assigned to one of three conditions: Self-Regulated Development (SRSD), SRSD-connectors (SRSD-C), and business-as-usual (BAU). The experimental conditions addressed planning and self-regulation strategies to write opinion essays, but only the SRSD condition included explicit teaching of connectors (e.g., because) and discourse markers (e.g., In conclusion). Children in both experimental conditions outscored children in the BAU condition across grades and outcome variables. In addition, the SRSD condition showed larger effect sizes on Grade 2 children's gains in text quality, number of genre-appropriate elements, and number of connectors than the SRSD-C condition. The study provides evidence of the effectiveness of explicitly teaching functionally motivated linguistic representations within a SRSD program. Theoretical and educational implications are discussed.\n979                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            This article discusses the history and development of English agriculture and estate management instructions, 1200-1700, as these shifted from oral to textual forms. Beginning with manuscript treatises that influenced important instruction books printed in the 16th century, the article shows how major agricultural writers developed instructions for a range of users. By the close of the 17th century, agricultural and estate management books exemplified increasingly modern presentation and style.\n189                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The COVID-19 pandemic created major disruptions in technical communication classrooms everywhere. Although technical communication instructors are used to teaching in a variety of contexts and settings, adopting a flexible approach in the first place will allow them to be better prepared for the changing dynamics of an unpredictable world. The authors present an approach that constructs pedagogical scaffolding to emphasize outcomes, interactions, relationships, and projects. These interrelated aspects form a coherent vision that can support both pedagogical planning and real-time decision making in specific instructional situations.\n864                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The author proposes Policy, Roles, Sites (PRS), a conceptual model to help technical communicators analyze high-stakes, long-debated public policy issues and reveal ways that technical and professional communication informs public policy development and implementation. The author demonstrates how the PRS model can be used to examine complex public policy issues from race and policing to gun rights and gun control, as well as policy issues that intersect these seemingly disparate issues.\n536                                                                                                                                                                                                                                         Purpose: To identify the main rhetorical techniques actually used to secure investors' support in some of the most successful (most-funded) Web-based crowd funding projects. The study serves to bridge the gap between theoretical research of rhetoric and the needs of business communication practitioners by identifying the means of persuasion that can be used by online crowd funding entrepreneurs. Method: Qualitative analysis of thirteen crowd funding project descriptions posted on a major Web site-www.kickstarter.com-was performed to identify specific rhetorical techniques via text coding. The sample included the most-funded projects to date, one from each of the thirteen project categories on Kickstarter. Aristotle's concept of ethos, pathos, and logos served as a basic framework for developing a more detailed classification of rhetorical means of persuasion used in the projects. Results: The most-funded projects have been found to contain all three types of rhetorical appeals (ethos, pathos, and logos), subdivided into a total of twelve specific subtypes most commonly encountered in the descriptions from the sample. The subtype definitions have been developed and refined over the course of several reviews. Conclusion: The research data made it possible to create a rhetorical profile of a successful crowd funding project description representing a summary of the rhetorical techniques identified during the study. Although this summary reflects a hypothetical all-inclusive case, it can be used as a benchmark when drafting crowd funding project descriptions. The study also identified specific directions for future research that could determine the influence of project description rhetoric on donor decisions.\n1002                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          &lt;NA&gt;\n103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   This article calls for recognition of ways in which feminisms have, do, and can inform social justice work in technical and professional communication (TPC)even social justice work that is not explicitly feminist. The authors distill some areas of feminist TPC scholarship that are relevant to future social justice work: (a) epistemological contributions, ways of knowing and methods for discovering them and (b) reclamations of dominant topics, groundwork laid by feminist research on technology and science. They close with nine recommendations to inspire scholars with specific ways to use feminist methodologies and theories to enhance social justice scholarship.\n1493                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We examined longhand note taking strategies when reading and summarizing a source text that was formatted with bullets or that was presented in a single paragraph. We analyzed cognitive effort when reading the source text, when jotting notes, when reading the notes, and when composing the summary, as well as time spent in these activities and the content of the notes and the summaries. With a formatted text, students' perceived comprehension difficulty was lower and they expended less cognitive effort, spent less time reading the text, and noted more ideas. While composing the summary, only those students who read the formatted source text continued selecting ideas in their notes. Finally, the summaries were unaffected by the formatting of the source text. The study shows that formatting a source text with bullets facilitates note taking by helping students to grasp its structure and by reducing the cognitive effort of reading.\n578                                                                                                                                                                                                                                                                  Purpose: Although computer-based presentations are nowadays an expected standard, empirical research on them is still surprisingly rare: Little knowledge exists about general attitudes toward presentation software or users' functional demands other than editing texts and images. Therefore, we focus in our explorative study on users' handling and evaluation of such software, including a comparison between educational and business users. Method: A total of 1014 participants (51% female, 49% male) took part in a web-based study. Among them were 444 students and 570 employees from different fields. The online questionnaire consisted of 67 questions in three parts and was based on the current literature and ratings of five experts. Results: Our results show a strong preference for using Microsoft PowerPoint, which led to rather satisfied users. Computer-based presentations are mainly used in educational settings, talks, and meetings. Differences between students and employees were identified, with the latter showing a broader use. Furthermore, independent of occupation, participants stressed the importance of usability aspects such as ease of use, compatibility, or loading speed; however, they equally desired more creativity in computer-based presentations and better speakers. Conclusions: The process of slide generation seems to be patchwork, and a large amount of time is spent on design and animation; thus we recommend measures to reduce the time spent on matters of visual style. In addition, current presentation software still suffers from several usability issues. Generally, the central function of the speaker and the supporting role of the presentation software are to be stressed.\n1090                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   This article uses research in cultural-historic activity theory, exploring patients' use of technical health care texts to produce knowledge and design their choices related to their bodies and health. Drawing on a case study of Meagan, who dealt with colitis and complications due to pregnancy, the author argues that we should consider reception and use as multisemiotic acts of repurposing, inscription, and reproduction alongside the research of the production of texts by professionals.\n1067                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Recent agency scholarship has provided compelling accounts of how individuals can strategically occupy authoritative positions, in order to instantiate change. This article explores the discursive mechanisms of this type of agency in the legitimization of disease. Drawing on ethnographic research, this article investigates how a non-human agent (brain scans) contributed to fibromyalgia's acceptance within the highly regulated discourses of western biomedicine.\n781                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     A recent technique called single-sourcing has evolved to handle complex documents that involve content replication. Current rhetorical theories are insufficient to analyze this technique. This essay offers a background rooted in the poetic movements of Anglo-American Imagism and Russian Acmeism. Through developing an intertextuality of induction, rhetorical structure, and emphasis on craft, the poetic traditions inform examples of how these concepts apply to pedagogical and paradigmatic approaches to single-sourcing.\n220                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       This article reports the results of a qualitative study on the joint publication of research articles by a group of supervisors and graduate students in an Iranian university. The results indicate that the ministry-regulated incentive system for publication had increased the research output of the participants. It argues that material and credentialing incentives for supervisors can be regarded as symbolic violence in the exercise of disciplinary power, which required that the participants form local communities of practice and interconnect with international journal reviewers to get their articles published.\n1195                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          &lt;NA&gt;\n443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Examines ethical issues in intercultural communication, focusing on privacy, legality, teamwork, social responsibility, and cultural sensitivity. Offers specific suggestions for avoiding stereotyping, tokenism, and ethnocentrism. Concludes with guidelines for technical communicators and suggestions for managers.\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         cited_references\n1290                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &lt;NA&gt;\n330                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n1321                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &lt;NA&gt;\n545                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n1277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &lt;NA&gt;\n979                                                                                                                                                                                                                                                                                                                                                                                                                                                                           [Anonymous], 1996, GOD SPEED PLOUGH REP; [Anonymous], COUNTRY CONTENTMENTS; [Anonymous], TECHNICAL COMMUNICAT; [Anonymous], 1968, SIGNIFICANCE NEWTO N; [Anonymous], 1996, EMERGENCE STANDARD E; [Anonymous], 1968, LITERACY TRADITIONAL; [Anonymous], 1997, TECH COMMUN Q, DOI DOI 10.1207/S15427625TCQ0603_2; Bennett H. S., 1970, ENGLISH BOOKS READER; BROCKMANN RJ, 1998, MILLWRIGHTS SHIPWRIG; BUSH D, 1962, ENGLISH LIT EARLIER; Durack KT, 1997, TECH COMMUN, V44, P37; Fitzherbert J., 1530, BOKE OF HUSBANDRIE; GLENN C, 1993, COLL COMPOS COMMUN, V44, P497, DOI 10.2307/358385; GOODY J, 1963, COMP STUD SOC HIST, V5, P304, DOI 10.1017/S0010417500001730; Goody Jack., 1977, DOMESTICATION SAVAGE; Grosseteste W., 1971, WALTER HENLEY OTHER; HAGGE J, 1990, J TECH WRIT COMMUN, V20, P269, DOI 10.2190/VWCW-XKMV-949F-VLF7; Hill T., 1946, MOST BRIEFE PLEASAUN; Johnson CS, 2009, BAYWOODS TECH COMMUN, P1; LIPSON CS, 1982, J TECH WRIT COMMUN, V12, P243, DOI 10.2190/WRLQ-CGT7-28TL-5B93; Markham G., 1620, CHEAP GOOD HUSBANDRY; Markham G., 1620, FAREWELL HUSBANDRY; Mascall L., 1572, BOOK ARTE MANER PLAN; McDonald Donald, 1908, AGR WRITERS SIR WALT; Meager L., 1688, ENGLISH GARDNER SURE; MILLER EH, 1959, PROFESSIONAL WRITER; Mullet C.F., 1944, ISIS, V35, P106; Ong W. J., 2002, ORALITY LITERACY TEC, V2nd; ONG WJ, 1984, NEW LITERARY HIST, V16, P1, DOI 10.2307/468772; Oschinsky D., 1971, WALTER HENLEY OTHER; Ovitt Jr George, 1987, CREATIVITY IMAGINATI, V3, P34; Pollard W., 1976, SHORT TITLE CATALOGU, V1-2; Pollard W., 1986, SHORT TITLE CATALOGU, V1-2; Poynter F.N., 1962, BIBLIO GERVASE MARKH; RICHARDSON HG, 1941, AM HIST REV, V46, P259; RICHARDSON M, 1984, RHETORICA, V2, P207, DOI 10.1525/rh.1984.2.3.207; Scot R., 1574, PERFITE PLATFORM HOP; Shirk Henrietta Nickels, 1997, TECHNICAL COMMUNICAT, V6, P293; Stock Brian, 1983, IMPLICATIONS LITERAC; Stock Brian, 1990, LISTENING TEXT USES; Tebeaux E, 2004, J BUS TECH COMMUN, V18, P165, DOI 10.1177/1050651903260738; TEBEAUX E, 1993, WRIT COMMUN, V10, P164, DOI 10.1177/0741088393010002002; Tebeaux E., 1992, IEEE Transactions on Professional Communications, V35, P196, DOI 10.1109/47.180280; TEBEAUX E, 1991, WRIT COMMUN, V8, P411, DOI 10.1177/0741088391008004001; Tebeaux E., 1993, J BUSINESS TECHNICAL, V7, P322, DOI DOI 10.1177/1050651993007003003; Tebeaux E., 1990, ISSUES WRITING, V3, P41; Tebeaux E., 2008, J TECHNICAL WRITING, V28, P3; Tebeaux E., 1995, J ADV COMPOSITION, V15, P53; TEBEAUX E, 1999, J TECHNICAL WRITING, V29, P209; Tebeaux E., 1999, 3 KEYS PAST HIST TEC, P105; Tebeaux Elizabeth., 1997, WOMEN SCI MED 1500 1, P29; Tebeaux Elizabeth, 1997, EMERGENCE TRADITION; Tebeaux Elizabeth, 2000, J TECH WRIT COMMUN, V30, P307; Tusser T, 1557, 100 GOOD POINTERS HU; Tusser Thomas., 1573, 500 POINTS GOOD HUSB; Wing Donald G., 1972, SHORT TITLE CATALOGU; Worlidge J., 1681, SYSTEMA AGR MYSTERY; Wright Louis B., 1935, MIDDLE CLASS CULTURE\n189                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n864  Agboka Godwin, 2014, J TECH WRIT COMMUN, V44, P297, DOI 10.2190/TW.44.3.e; [Anonymous], 2014, SOCIAL MEDIA DISASTE; [Anonymous], 2000, TECH COMMUN Q, DOI DOI 10.1080/10572250009364682; [Anonymous], 2005, WRITING PUBLIC POLIC; [Anonymous], 2003, RULEMAKING GOVT AGEN; [Anonymous], 2002, RHETORIC RISK TECHNI; Balzhiser D, 2019, TECH COMMUN Q, V28, P3, DOI 10.1080/10572252.2018.1539192; BATFE Bureau of Alcohol Tobacco Firearms and Explosives, 2016, CRIM BACKGR CHECK 44; Birrell ND., 1985, PRACTICAL HDB SOFTWA, DOI 10.1017/CBO9780511624223; Bowdon MA, 2014, TECH COMMUN Q, V23, P35, DOI 10.1080/10572252.2014.850853; Bridgewater M, 2018, TECH COMMUN-STC, V65, P422; Cagle LE, 2015, TECH COMMUN Q, V24, P147, DOI 10.1080/10572252.2015.1001296; Cargile-Cook K, 2000, TECH COMMUN Q, V9, P56; Collins & Yaffe-Bellany , 2020, NEW YORK TIMES; Colton Jared S., 2018, J TECH WRIT COMMUN, V48, P4, DOI [10.1177/0047281616647803, DOI 10.1177/0047281616647803]; Ding H., 2014, RHETORIC GLOBAL EPID; Dorpenyo I, 2018, TECH COMMUN-STC, V65, P349; Dorpenyo IK, 2019, TECH COMMUN Q, V28, P361, DOI 10.1080/10572252.2019.1610502; Dragga S, 2017, TECH COMMUN-STC, V64, P277; Dragga S, 2014, TECH COMMUN-STC, V61, P76; Gibson K, 2008, TECH COMMUN Q, V18, P1, DOI 10.1080/10572250802437234; Givens D., 2020, BLACK ENTER; Gonzales L, 2018, SWEETLAND DIG RHET C, P1, DOI 10.3998/mpub.9952377; Good N, 2018, DATA HUMANS VALUE; Grabill J.T., 2000, TECHNICAL COMMUNICAT, V9, P29, DOI [10.1080/10572250009364684, DOI 10.1080/10572250009364684]; Grabill J. T., 1998, TECH COMMUN Q, V7, P415, DOI [10.1080/10572259809364640, DOI 10.1080/10572259809364640]; Haas AM, 2018, KEY THEORETICAL FRAMEWORKS: TEACHING TECHNICAL COMMUNICATION IN THE TWENTY-FIRST CENTURY, P3, DOI 10.7330/9781607327585.c000; Haas AM, 2012, J BUS TECH COMMUN, V26, P277, DOI 10.1177/1050651912439539; Johnson-Sheehan R, 2008, TECH COMMUN Q, V18, P9, DOI 10.1080/10572250802437283; Jones N., 2016, J TECH WRIT COMMUN, V46, P471, DOI [10.1177/0047281616653, DOI 10.1177/0047281616653489]; Jones NN, 2018, TECH COMMUN-STC, V65, P371; Jones NN, 2016, TECH COMMUN Q, V25, P211, DOI 10.1080/10572252.2016.1224655; Medina C, 2018, RACIAL SHORTHAND COD; Merry MK, 2018, POLICY STUD J, V46, P747, DOI 10.1111/psj.12255; Moore Kristen R., 2017, Communication Design Quarterly Review, V5, P52, DOI 10.1145/3188387.3188392; Moore KR, 2017, TECH COMMUN-STC, V64, P237; Nugent J, 2018, TECH COMMUN-STC, V65, P411; Olman C.Lynda, 2019, ROUTLEDGE HDB LANGUA, P279; Pew Research Center, 2017, AM COMPL REL GUNS; Ranney FrancesJ., 2000, TECHNICAL COMMUNICAT, V9, P9, DOI [10.1080/10572250009364683, DOI 10.1080/10572250009364683]; Sackey DJ, 2020, TECH COMMUN Q, V29, P33, DOI 10.1080/10572252.2019.1634767; Sanchez F, 2018, TECH COMMUN-STC, V65, P354; Sano-Franchini J, 2018, TECH COMMUN-STC, V65, P387; Sidler M, 2008, TECH COMMUN Q, V18, P28, DOI 10.1080/10572250802437317; Smith, 2000, TECH COMMUN Q, V9, P77, DOI [10.1080/10572250009364686, DOI 10.1080/10572250009364686]; Spoel P, 2008, TECH COMMUN Q, V18, P49, DOI 10.1080/10572250802437382; Walton R, 2019, J TECH WRIT COMMUN; Williams, 2007, ALLYN BACON TECHNICA; Williams MF, 2010, BAYWOODS TECH COMMUN, P1; Williams MF, 2008, TECH COMMUN Q, V18, P82, DOI 10.1080/10572250802437515; Williams MF, 2009, J BUS TECH COMMUN, V23, P448, DOI 10.1177/1050651909338809\n536                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n1002                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Blakeslee A., 2007, BECOMING WRITING RES\n103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n1493                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 &lt;NA&gt;\n578                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n1090                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [Anonymous], RHETORIC IDEOLOGY GE; [Anonymous], 1988, TASK TALK TEXT OPERA; [Anonymous], WRITING SELVES WRITI; [Anonymous], ACTIVITY THEORY SOCI; [Anonymous], 2003, RISKY RHETORIC AIDS; [Anonymous], 2005, POLITICA SOC; [Anonymous], 1998, WRITINGDISCIPLINARIT; BabyCenter LLC, 2009, POPP SEED PUMPK BIG; Bakhtin M. M., 1981, DIALOGIC IMAGINATION; Barnum C. M., 2001, USABILITY TESTING RE; Barton E, 2004, J BUS TECH COMMUN, V18, P67, DOI 10.1177/1050651903258127; Bowdon M., 2004, TECHNICAL COMMUNICAT, V13, P325, DOI [10.1207/s15427625tcq1303_6, DOI 10.1207/S15427625TCQ1303_6]; DAUTERMANN J, 1997, WRITING GOOD HOPE ST; Grabill J. T., 2007, WRITING COMMUNITY CH; Hamilton Heidi E, 2004, Commun Med, V1, P59, DOI 10.1515/come.2004.006; Hartouni Valerie, 1998, VISIBLE WOMAN IMAGIN, P198; Kimball MA, 2006, TECH COMMUN Q, V15, P67, DOI 10.1207/s15427625tcq1501_6; Koerber A, 2006, TECH COMMUN Q, V15, P87, DOI 10.1207/s15427625tcq1501_7; Kohan D., 2000, ACTING OUT TELEVISI; Murkoff Heidi, 2008, WHAT EXPECT YOU ARE; Russell DR, 1997, WRIT COMMUN, V14, P504, DOI 10.1177/0741088397014004004; Sarangi Srikant, 2004, Commun Med, V1, P1, DOI 10.1515/come.2004.002; Simmons W. Michelle., 2007, PARTICIPATION POWER; Spafford MM, 2006, J BUS TECH COMMUN, V20, P121, DOI 10.1177/1050651905284396; Spinuzzi C., 2008, NETWORK THEORIZING K; Vegni Elena, 2005, Commun Med, V2, P69, DOI 10.1515/come.2005.2.1.69; Weithorn M.J., 2001, THE KING OF QUEENS\n1067                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          American Academy of Rheumatology, 1990 CRIT CLASS FIBR; [Anonymous], 1993, J BUS TECH COMMUN, DOI DOI 10.1177/1050651993007004001; [Anonymous], LENIN PHILOS OTHER E; [Anonymous], 1990, THEORIES SCI SOC; [Anonymous], 2002, GLOBE MAIL, pH1; [Anonymous], 2007, CHEM DRUGGIST, V17; [Anonymous], 1998, BRIEF HIST TIME; Arnold LM, 2006, ARTHRITIS RES THER, V8, DOI 10.1186/ar1971; Berenson A., 2008, NEW YORK TIMES, p1A; Blythe S, 2008, J BUS TECH COMMUN, V22, P272, DOI 10.1177/1050651908315973; Clauw DJ, 2003, BEST PRACT RES CL RH, V17, P685, DOI 10.1016/S1521-6942(03)00035-4; Crofford LJ, 2005, ARTHRITIS RHEUM-US, V52, P1264, DOI 10.1002/art.20983; Crofford LJ, 2002, ARTHRITIS RHEUM-US, V46, P1136, DOI 10.1002/art.10217; Dumit Joseph, 2004, PICTURING PERSONHOOD; Foucault M., 2002, ARCHAEOLOGY KNOWLEDG; Goff K. G., 2002, WASHINGTON TIMES, pD1; Gomperz Heinrich, 1908, WELTANSCHAUUNGSLEHRE; Greene RW, 2004, PHILOS RHETORIC, V37, P188, DOI 10.1353/par.2004.0020; Griffing George T, 2008, Medscape J Med, V10, P47; Haraway D.J., 1997, MODEST WITNESS; HEISS WD, 1985, ATLAS POSITRONEN EMI; Held S., 2008, NY TIMES, P30; Herndl Carl G., 2007, COMMUNICATIVE PRACTI, P133; Koerber A, 2006, TECH COMMUN Q, V15, P87, DOI 10.1207/s15427625tcq1501_7; Kuhn T.S., 1996, STRUCTURE SCIENTIFC; Latour B., 1987, SCI ACTION FOLLOW SC; Latour B, 1999, PANDORAS HOPE ESSAYS; Latour Bruno., 1986, LAB LIFE CONSTRUCTIO; Latour Bruno., 1992, SHAPING TECHNOLOGY B, P225; McCullough M., 2000, PHILADELPHIA IN 0424; Miller Carolyn R., 2007, RHETOR SOC Q, V37, P137, DOI DOI 10.1080/02773940601021197; Mishra R., 2004, BOSTON GLOBE, pC3; PEIRCE CS, 1978, PHILOS RHETORIC, V11, P147; Perrot Serge, 2008, Pain Pract, V8, P177, DOI 10.1111/j.1533-2500.2008.00190.x; Pies R., 2008, NY TIMES, P30; Roland P E, 1985, Res Publ Assoc Res Nerv Ment Dis, V63, P87; Smith Paul, 1988, DISCERNING SUBJECT; TERPOGOSSIAN MM, 1975, RADIOLOGY, V114, P89, DOI 10.1148/114.1.89; Underwood Anne, 2003, Newsweek, V141, P53; Williams DA, 2006, ARTHRITIS RES THER, V8, DOI 10.1186/ar2094; Winsor D, 2006, TECH COMMUN Q, V15, P411, DOI 10.1207/s15427625tcq1504_1; WITTE SP, 1992, WRIT COMMUN, V9, P237, DOI 10.1177/0741088392009002003; Woolgar Steve, 1991, SOCIOLOGY MONSTERS E, P58\n781                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Albers MJ, 2003, TECH COMMUN, V50, P335; Ament K., 2003, SINGLE SOURCING BUIL; [Anonymous], 1985, HDB RUSSIAN LIT; [Anonymous], MANDELSTAM READER; [Anonymous], 1981, ARRESTING EYE RHETOR; Bolter J. D, 1993, RHETORICAL MEMORY DE, P97; COE RM, 1987, COLL ENGL, V49, P13, DOI 10.2307/377786; DICK K, 2000, XML MANAGERS GUIDE; Greene Thomas M., 1982, LIGHT TROY IMITATION; Hall WP, 2001, TECH COMMUN, V48, P235; HARTDAVIDSON W, 2002, MODELING FLEXIBLE DO; Lancaster A., 2005, RE DITA DARWIN INFO; LANDOW George, 1992, HYPERTEXT CONVERGENC; Lanham Richard., 1993, THE ELECT WORD; Mandel'shtam O., 1990, SOCHINENIIA DVUKH TO, V2; Mandelshtam Osip, 1979, CRITICAL PROSE LETT; MILLER CR, 1984, Q J SPEECH, V70, P151, DOI 10.1080/00335638409383686; Nichols C., 2002, ENCULTURATION; Ronen O., 1983, APPROACH MANDELSTAM; Rusinko Elaine, 1978, ULBANDUS REV, V1, P37; Sapienza Filipp, 2002, J TECHNICAL WRITING, V32, P155; Sellars Roy, 1997, MEMORY LIT INTERTEXT; SOUTHAM B. C., 1996, GUIDE SELECTED POEMS; Williams J., 2004, STYLE 10 LESSONS CLA; Williams JD, 2003, TECH COMMUN, V50, P321\n220                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n1195                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             CECCARELLI L, 2013, FRONTIERE SCIENCE AM\n443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  &lt;NA&gt;\n     cited_reference_count publication_year publication_type abbreviation\n1290                    NA             2020                J           WC\n330                     56             2021                J           TC\n149                     NA             2010                J         JBTC\n1321                    NA             2022                J           WC\n545                     72             2020                J           TC\n1277                    NA             2023                J           WC\n979                     58             2010                J          TCQ\n189                     NA             2021                J         JBTC\n864                     51             2022                J          TCQ\n536                     25             2014                J           TC\n1002                     1             2012                J          TCQ\n103                     NA             2018                J         JBTC\n1493                    NA             2017                J           WC\n578                     41             2012                J           TC\n1090                    27             2012                J          TCQ\n1067                    43             2009                J          TCQ\n781                     25             2007                J          TCQ\n220                     NA             2013                J         JBTC\n1195                     1             2015                J          TCQ\n443                     35             2007                J           TC"
  },
  {
    "objectID": "demos/week03/wk03_wos_journals.html#visualize-the-full-data",
    "href": "demos/week03/wk03_wos_journals.html#visualize-the-full-data",
    "title": "Wk 03: Exploring TC journals, (pt 2)",
    "section": "Visualize the full data",
    "text": "Visualize the full data\nWe’ll try a few different displays.\nStacked bar\n\nCode# count of articles by year, by journal\nggplot(full_data, aes(x = publication_year, fill = abbreviation)) +\n  geom_bar() +\n  labs(title = \"Articles per Year in TC and TCQ\",\n       x = \"Publication Year\",\n       y = \"Number of Articles\") +\n  theme_light()\n\n\n\n\nFaceted bar\n\nCode# count of articles by year, by journal\nggplot(full_data, aes(x = publication_year)) +\n  geom_bar() +\n  labs(title = \"Articles per Year\",\n       x = \"Publication Year\",\n       y = \"Number of Articles\") +\n  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.5, size = 4) +\n   facet_wrap(~ abbreviation, ncol = 1) + # Change ncol as needed\n   theme_fivethirtyeight()\n\n\n\n\nLine and points\nFor a line graph, we first have to create a new dataframe that includes article counts\n\nCode# Summarize the data to count the number of articles by source_title and publication_year\narticle_counts &lt;- full_data %&gt;%\n  group_by(abbreviation, publication_year) %&gt;%\n  summarise(article_count = n())\n\nglimpse(article_counts)\n\nRows: 76\nColumns: 3\nGroups: abbreviation [4]\n$ abbreviation     &lt;chr&gt; \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC…\n$ publication_year &lt;dbl&gt; 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,…\n$ article_count    &lt;int&gt; 15, 13, 15, 16, 14, 16, 17, 14, 17, 15, 17, 14, 15, 1…\n\nCodehead(article_counts)\n\n# A tibble: 6 × 3\n# Groups:   abbreviation [1]\n  abbreviation publication_year article_count\n  &lt;chr&gt;                   &lt;dbl&gt;         &lt;int&gt;\n1 JBTC                     2005            15\n2 JBTC                     2006            13\n3 JBTC                     2007            15\n4 JBTC                     2008            16\n5 JBTC                     2009            14\n6 JBTC                     2010            16\n\nCodetail(article_counts)\n\n# A tibble: 6 × 3\n# Groups:   abbreviation [1]\n  abbreviation publication_year article_count\n  &lt;chr&gt;                   &lt;dbl&gt;         &lt;int&gt;\n1 WC                       2018            16\n2 WC                       2019            18\n3 WC                       2020            16\n4 WC                       2021            17\n5 WC                       2022            24\n6 WC                       2023            30\n\n\nNow we can create the plot.\n\nCodeline &lt;- ggplot(article_counts, aes(x = publication_year, y = article_count, color = abbreviation))+\n  geom_line() +\n  geom_point() +\n  labs(title = \"Number of Articles Published Each Year by Journal\",\n       x = \"Publication Year\",\n       y = \"Number of Articles\") +\n  theme_tufte()+\n  scale_color_discrete(name = \"Journal\") # change the label name of the color variable\n\nline\n\n\n\n\nSave a plot\n\nCodehelp(ggsave)\nggsave(\"plots/full_data_line_plot.png\", \n       plot = line, \n       width = 8, \n       height = 6, \n       dpi = 300,\n       bg = \"white\")\n\n\nSave out data\n\nCode# save as csv\n\nwrite_csv(full_data, \"data_out/full_data.csv\")\n\n\nsave(full_data, file = \"data_out/full_data.RData\")"
  },
  {
    "objectID": "demos/week05/wk05_tc-titles.html",
    "href": "demos/week05/wk05_tc-titles.html",
    "title": "Wk 05: Titles in TC Journals, pt. 1",
    "section": "",
    "text": "Here, we prepare a dataset consisting of article metadata for five major tc journals:\n\nJournals: Technical Communication Quarterly (TCQ), Technical Communication (TC), Journal of Technical Writing and Communication (JTWC) , Communication Design Quarterly (CDQ), and Journal of Business and Technical Communication (JBTC)\nYears: 2005 - 2023\nArticle Metadata: Journal, Authors, Title, Abstract, Publication Year"
  },
  {
    "objectID": "demos/week05/wk05_tc-titles.html#overview",
    "href": "demos/week05/wk05_tc-titles.html#overview",
    "title": "Wk 05: Titles in TC Journals, pt. 1",
    "section": "",
    "text": "Here, we prepare a dataset consisting of article metadata for five major tc journals:\n\nJournals: Technical Communication Quarterly (TCQ), Technical Communication (TC), Journal of Technical Writing and Communication (JTWC) , Communication Design Quarterly (CDQ), and Journal of Business and Technical Communication (JBTC)\nYears: 2005 - 2023\nArticle Metadata: Journal, Authors, Title, Abstract, Publication Year"
  },
  {
    "objectID": "demos/week05/wk05_tc-titles.html#prepare-data",
    "href": "demos/week05/wk05_tc-titles.html#prepare-data",
    "title": "Wk 05: Titles in TC Journals, pt. 1",
    "section": "Prepare data",
    "text": "Prepare data\nWe need to drop written comm from our “full data” and read in and clean bibtex data from Communication Design Quarterly and Journal of Technical Writing and Communication\nLoad libraries\n\nCode#install.packages(\"bib2df\")\nlibrary(bib2df)\nlibrary(janitor)\nlibrary(tidyverse)\n\n\nFirst, we’ll read in CDQ.\n\nCode# CDQ: read in and clean names\ncdq_raw &lt;- bib2df(\"data/acm-cdq.bib\")%&gt;%\n  clean_names()\n\nhead(cdq_raw)\n\n# A tibble: 6 × 33\n  category bibtexkey    address annote author booktitle chapter crossref edition\n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;  &lt;list&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;  \n1 ARTICLE  10.1145/356… New Yo… &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n2 ARTICLE  10.1145/350… New Yo… &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n3 ARTICLE  10.1145/350… New Yo… &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n4 ARTICLE  10.1145/350… New Yo… &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n5 ARTICLE  10.1145/350… New Yo… &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n6 ARTICLE  10.1145/348… New Yo… &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n# ℹ 24 more variables: editor &lt;list&gt;, howpublished &lt;chr&gt;, institution &lt;chr&gt;,\n#   journal &lt;chr&gt;, key &lt;chr&gt;, month &lt;chr&gt;, note &lt;chr&gt;, number &lt;chr&gt;,\n#   organization &lt;chr&gt;, pages &lt;chr&gt;, publisher &lt;chr&gt;, school &lt;chr&gt;,\n#   series &lt;chr&gt;, title &lt;chr&gt;, type &lt;chr&gt;, volume &lt;chr&gt;, year &lt;dbl&gt;,\n#   issue_date &lt;chr&gt;, url &lt;chr&gt;, doi &lt;chr&gt;, abstract &lt;chr&gt;, numpages &lt;chr&gt;,\n#   keywords &lt;chr&gt;, issn &lt;chr&gt;\n\n\nSecond, we’ll read in JWTC.\n\nCode# JWTC: read in and clean names\njtwc_raw &lt;- bib2df(\"data/jtwc.bib\")%&gt;%\n  clean_names()\n\nhead(jtwc_raw)\n\n# A tibble: 6 × 30\n  category bibtexkey    address annote author booktitle chapter crossref edition\n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;  &lt;list&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;  \n1 ARTICLE  17004768420… &lt;NA&gt;    &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n2 ARTICLE  17004768320… &lt;NA&gt;    &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n3 ARTICLE  17004768220… &lt;NA&gt;    &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n4 ARTICLE  17004768720… &lt;NA&gt;    &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n5 ARTICLE  17004768520… &lt;NA&gt;    &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n6 ARTICLE  17004768620… &lt;NA&gt;    &lt;NA&gt;   &lt;chr&gt;  &lt;NA&gt;      &lt;NA&gt;    &lt;NA&gt;     &lt;NA&gt;   \n# ℹ 21 more variables: editor &lt;list&gt;, howpublished &lt;chr&gt;, institution &lt;chr&gt;,\n#   journal &lt;chr&gt;, key &lt;chr&gt;, month &lt;chr&gt;, note &lt;chr&gt;, number &lt;chr&gt;,\n#   organization &lt;chr&gt;, pages &lt;chr&gt;, publisher &lt;chr&gt;, school &lt;chr&gt;,\n#   series &lt;chr&gt;, title &lt;chr&gt;, type &lt;chr&gt;, volume &lt;chr&gt;, year &lt;dbl&gt;,\n#   abstract &lt;chr&gt;, issn &lt;chr&gt;, keywords &lt;chr&gt;, url &lt;chr&gt;\n\n\nThird, we’ll read in our full data and drop WC\n\nCode# read in full data and drop Written Comm entries\nfullish &lt;- read_csv(\"data/full_data.csv\") %&gt;%\n  filter(abbreviation != \"WC\") %&gt;%\n  select(source_title,\n         author_full_names,\n         article_title,\n         abstract,\n         publication_year,\n         abbreviation)\n\n#head(fullish)\n\n#glimpse(fullish)\n\n#unique(fullish$abbreviation)\n\n\nUnify the dataset\nFor this analysis, we want consistent data and column names for each journal in the dataset. Here are the datapoints, using the column names from our preexisting data:\n\nsource_title\nauthor_full_names\narticle_title\nabstract\npublication_year\nabbreviation\n\nLet’s select the target fields, add the journal abbreviation, and rename the columns to harmonize the data with our preexisting set.\n\nCode#glimpse(cdq_raw)\n\n# select fields to keep; add an abbreviation column; rename columns\ncdq_select &lt;- cdq_raw %&gt;%\n  select(journal,\n         author,\n         title,\n         abstract,\n         year) %&gt;%\n  mutate(abbreviation = \"CDQ\")%&gt;%\n  rename(source_title = journal,\n         author_full_names = author,\n         article_title = title,\n         abstract = abstract,\n         publication_year = year,\n         abbreviation = abbreviation)%&gt;%\n  mutate(author_full_names = map_chr(author_full_names, ~ paste(.x, collapse = \"; \")))\n\nglimpse(cdq_select)\n\nRows: 336\nColumns: 6\n$ source_title      &lt;chr&gt; \"Commun. Des. Q. Rev\", \"Commun. Des. Q. Rev\", \"Commu…\n$ author_full_names &lt;chr&gt; \"Carter, Daniel\", \"York, Eric J.\", \"Davis, Katlynne;…\n$ article_title     &lt;chr&gt; \"Constructing Structured Content on WordPress: Emerg…\n$ abstract          &lt;chr&gt; \"Web content management systems (WCMSs) are widely u…\n$ publication_year  &lt;dbl&gt; 2023, 2022, 2022, 2022, 2022, 2022, 2021, 2021, 2020…\n$ abbreviation      &lt;chr&gt; \"CDQ\", \"CDQ\", \"CDQ\", \"CDQ\", \"CDQ\", \"CDQ\", \"CDQ\", \"CD…\n\n\nNow the same for JTWC!\n\nCode#glimpse(jtwc_raw)\n\n# select fields to keep; add an abbreviation column; rename columns\njtwc_select &lt;- jtwc_raw %&gt;%\n  select(journal,\n         author,\n         title,\n         abstract,\n         year) %&gt;%\n  mutate(abbreviation = \"JTWC\")%&gt;%\n  rename(source_title = journal,\n         author_full_names = author,\n         article_title = title,\n         abstract = abstract,\n         publication_year = year,\n         abbreviation = abbreviation)%&gt;%\n  mutate(author_full_names = map_chr(author_full_names, ~ paste(.x, collapse = \"; \")))\n\nglimpse(jtwc_select)\n\nRows: 452\nColumns: 6\n$ source_title      &lt;chr&gt; \"Journal of Technical Writing & Communication\", \"Jou…\n$ author_full_names &lt;chr&gt; \"Getto, Guiseppe;  Flanagan, Suzan;  Labriola, Jack\"…\n$ article_title     &lt;chr&gt; \"Introduction to Special Issue: The People, Practice…\n$ abstract          &lt;chr&gt; \"This special issue of the Journal of Technical Writ…\n$ publication_year  &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023…\n$ abbreviation      &lt;chr&gt; \"JTWC\", \"JTWC\", \"JTWC\", \"JTWC\", \"JTWC\", \"JTWC\", \"JTW…\n\n\nNow we can join the three\n\nCodetc_journals &lt;- bind_rows(fullish, cdq_select, jtwc_select)\n\nglimpse(tc_journals)\n\nRows: 2,002\nColumns: 6\n$ source_title      &lt;chr&gt; \"JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\", \"…\n$ author_full_names &lt;chr&gt; \"Wickman, Chad\", \"DeJeu, Emily Barrow\", \"DeVasto, Da…\n$ article_title     &lt;chr&gt; \"Genre and Metagenre in Biomedical Research Writing\"…\n$ abstract          &lt;chr&gt; \"The use of reporting guidelines is an established y…\n$ publication_year  &lt;dbl&gt; 2023, 2022, 2016, 2009, 2009, 2020, 2016, 2012, 2011…\n$ abbreviation      &lt;chr&gt; \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBT…"
  },
  {
    "objectID": "demos/week05/wk05_tc-titles.html#check-work-plot-the-data",
    "href": "demos/week05/wk05_tc-titles.html#check-work-plot-the-data",
    "title": "Wk 05: Titles in TC Journals, pt. 1",
    "section": "Check work: Plot the data",
    "text": "Check work: Plot the data\n\nCode# Plot 1: Count of observations by journal abbreviation\ntc_journals %&gt;%\n  count(abbreviation) %&gt;%\n  ggplot(aes(x = reorder(abbreviation, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Articles published (2005-2023) in major TC Journals\", x = \"Journal\", y = \"Articles\")\n\n\n\n\n\nCode# Plot 2: Bar chart of the number of articles published per year, colored by journal abbreviation\ntc_journals %&gt;%\n  group_by(publication_year, abbreviation) %&gt;%\n  summarise(num_articles = n()) %&gt;%\n  ggplot(aes(x = publication_year, y = num_articles, fill = abbreviation)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Articles Published per Year, by Journal Abbreviation\", x = \"Publication Year\", y = \"Number of Articles\") +\n  scale_fill_discrete(name = \"Journal Abbreviation\")\n\n\n\n\n\nCode# Plot 3: Dot plot of the number of articles published per year, colored by journal abbreviation with trend lines\ntc_journals %&gt;%\n  group_by(publication_year, abbreviation) %&gt;%\n  summarise(num_articles = n()) %&gt;%\n  ggplot(aes(x = publication_year, y = num_articles, color = abbreviation)) +\n  geom_point(size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, size = .7, aes(group = abbreviation), linetype = \"dashed\") +\n  labs(title = \"Articles Published per Year, by Journal Abbreviation\", x = \"Publication Year\", y = \"Number of Articles\") +\n  scale_color_discrete(name = \"Journal Abbreviation\")\n\n\n\n\n\nCode# Plot 4: Bar chart of articles per year\ntc_journals %&gt;%\n  group_by(publication_year) %&gt;%\n  summarise(num_articles = n()) %&gt;%\n  ggplot(aes(x = publication_year, y = num_articles)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Total Articles per Year across Major TC Journals\", x = \"Publication Year\", y = \"Number of Articles\")"
  },
  {
    "objectID": "demos/week05/wk05_tc-titles.html#write-out-new-data",
    "href": "demos/week05/wk05_tc-titles.html#write-out-new-data",
    "title": "Wk 05: Titles in TC Journals, pt. 1",
    "section": "Write out new data",
    "text": "Write out new data\n\nCode# save as csv\n\nwrite_csv(tc_journals, \"data/tc_journals.csv\")\n\n\nsave(tc_journals, file = \"data/tc_journals.RData\")"
  },
  {
    "objectID": "demos/week08/wk08_sentiment-analysis.html",
    "href": "demos/week08/wk08_sentiment-analysis.html",
    "title": "Wk 08: Sentiment Analysis",
    "section": "",
    "text": "Here is some sample code to get sentiment data from bing and nrc dictionaries."
  },
  {
    "objectID": "demos/week08/wk08_sentiment-analysis.html#overview",
    "href": "demos/week08/wk08_sentiment-analysis.html#overview",
    "title": "Wk 08: Sentiment Analysis",
    "section": "",
    "text": "Here is some sample code to get sentiment data from bing and nrc dictionaries."
  },
  {
    "objectID": "demos/week08/wk08_sentiment-analysis.html#the-data",
    "href": "demos/week08/wk08_sentiment-analysis.html#the-data",
    "title": "Wk 08: Sentiment Analysis",
    "section": "The data",
    "text": "The data\n\nCodelibrary(tidyverse)\nload(\"data/tmm_comments.Rdata\")\n\nglimpse(data)\n\nRows: 38,661\nColumns: 6\n$ `Letter #`          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ `Organization Name` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ `Letter Text`       &lt;chr&gt; \"The proposal to renew expired mineral leases on t…\n$ ...4                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...5                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ ...6                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…"
  },
  {
    "objectID": "demos/week08/wk08_sentiment-analysis.html#sentiment-analysis",
    "href": "demos/week08/wk08_sentiment-analysis.html#sentiment-analysis",
    "title": "Wk 08: Sentiment Analysis",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\n\nCodelibrary(janitor)\ndata &lt;- data %&gt;%\n  clean_names()\n\nglimpse(data)\n\nRows: 38,661\nColumns: 6\n$ letter_number     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1…\n$ organization_name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ letter_text       &lt;chr&gt; \"The proposal to renew expired mineral leases on the…\n$ x4                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ x5                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ x6                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n\n\nLetter text\n\nCodehead(data$letter_text, n = 1)\n\n[1] \"The proposal to renew expired mineral leases on the Superior National Forest near the Boundary Waters shows a disregard for science, economics, and public opinion. The U.S. Forest Service rejected these very leases in 2016 because allowing mining to proceed would be inconsistent with its obligation to manage and protect these lands and waters for future generations. The Trump Administration has reversed the U.S. Forest Services prior decisions and illegally put the project into the hands of the Interior Departments Bureau of Land Management. The U.S. Forest Service has an obligation to protect the Boundary Waters. The BLMs inadequate environmental review rejects the U.S. Forest Services authority and ignores the overwhelming science and economics that support long-term mining protections for this area. Refusing to renew the expired mineral leases is the way to ensure that the conservation and protection of the Boundary Waters continues. The BLM must at least prepare a full environmental impact statement that includes a no-lease-renewal alternative and thoroughly examines the significant impacts of mining should it decide to proceed with reckless and illegal lease renewal.Thank you.\"\n\n\n\nCodelibrary(tidytext)\n\n#install.packages(\"textdata\")\nlibrary(textdata)\n\n\nAFINN\n\nCode# get AFINN sentiment dictionary (you may have to download)\nget_sentiments(\"afinn\")\n\n# A tibble: 2,477 × 2\n   word       value\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# ℹ 2,467 more rows\n\n\nBing\n\nCode# get bing sentiment dictionary\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\n\nNRC\n\nCode# get nrc dictionary (you may have to download)\nget_sentiments(\"nrc\")\n\n# A tibble: 13,872 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# ℹ 13,862 more rows\n\n\nSetting up our data\n\nCodedata &lt;- data %&gt;%\n  select(letter_number, organization_name, letter_text)\n\n\n\nCodetokens &lt;- data %&gt;%\n  unnest_tokens(word, letter_text)\n\nhead(tokens)\n\n# A tibble: 6 × 3\n  letter_number organization_name word    \n          &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;   \n1             1 &lt;NA&gt;              the     \n2             1 &lt;NA&gt;              proposal\n3             1 &lt;NA&gt;              to      \n4             1 &lt;NA&gt;              renew   \n5             1 &lt;NA&gt;              expired \n6             1 &lt;NA&gt;              mineral \n\n\nGet bing sentiment scores\nIn the following code, we\n\nCodebing_sentiments &lt;- tokens %&gt;%\n  inner_join(get_sentiments(\"bing\"), by = c(word = \"word\")) %&gt;%\n  group_by(letter_number) %&gt;%\n  summarise(bing_sentiment = sum(sentiment == \"positive\") - sum(sentiment == \"negative\"))\n\n\nbing_sentiments\n\n# A tibble: 38,347 × 2\n   letter_number bing_sentiment\n           &lt;dbl&gt;          &lt;int&gt;\n 1             1             -5\n 2             2             -5\n 3             3             -5\n 4             4             -3\n 5             5             -5\n 6             6             -5\n 7             7             -5\n 8             8             -5\n 9             9             -5\n10            10             -5\n# ℹ 38,337 more rows\n\n\nGet NRC sentiment scores\n\nCode# Calculate sentiment scores using the NRC lexicon\nnrc_sentiments &lt;- tokens %&gt;%\n  inner_join(get_sentiments(\"nrc\"), by = c(word = \"word\")) %&gt;%\n  group_by(letter_number, sentiment) %&gt;%\n  summarise(sentiment_count = n()) %&gt;%  # Count the number of each sentiment\n  pivot_wider(names_from = sentiment, values_from = sentiment_count, values_fill = 0) %&gt;%\n  ungroup(.)\n\n# rename columns to include prefix \"nrc_\"\nnrc_sentiments &lt;- nrc_sentiments %&gt;%\n  rename_with(~ paste0(\"nrc_\", .), -letter_number)\n\nhead(nrc_sentiments)\n\n# A tibble: 6 × 11\n  letter_number nrc_anger nrc_anticipation nrc_disgust nrc_fear nrc_negative\n          &lt;dbl&gt;     &lt;int&gt;            &lt;int&gt;       &lt;int&gt;    &lt;int&gt;        &lt;int&gt;\n1             1         3                4           2        3            8\n2             2         3                4           2        3            8\n3             3         3                4           2        3            8\n4             4         2                1           0        1            2\n5             5         3                4           2        3            8\n6             6         3                4           2        3            8\n# ℹ 5 more variables: nrc_positive &lt;int&gt;, nrc_sadness &lt;int&gt;,\n#   nrc_surprise &lt;int&gt;, nrc_trust &lt;int&gt;, nrc_joy &lt;int&gt;\n\n\n\nCode# Join the sentiment scores with the original DataFrame\nresults &lt;- data %&gt;%\n  left_join(bing_sentiments, by = \"letter_number\") %&gt;%\n  left_join(nrc_sentiments, by = \"letter_number\")\n\n# View the resulting DataFrame\nhead(results)\n\n# A tibble: 6 × 14\n  letter_number organization_name letter_text           bing_sentiment nrc_anger\n          &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;                          &lt;int&gt;     &lt;int&gt;\n1             1 &lt;NA&gt;              The proposal to rene…             -5         3\n2             2 &lt;NA&gt;              The proposal to rene…             -5         3\n3             3 &lt;NA&gt;              The proposal to rene…             -5         3\n4             4 &lt;NA&gt;              The renewal of these…             -3         2\n5             5 &lt;NA&gt;              The proposal to rene…             -5         3\n6             6 &lt;NA&gt;              The proposal to rene…             -5         3\n# ℹ 9 more variables: nrc_anticipation &lt;int&gt;, nrc_disgust &lt;int&gt;,\n#   nrc_fear &lt;int&gt;, nrc_negative &lt;int&gt;, nrc_positive &lt;int&gt;, nrc_sadness &lt;int&gt;,\n#   nrc_surprise &lt;int&gt;, nrc_trust &lt;int&gt;, nrc_joy &lt;int&gt;\n\n\nWrite out results for later use\n\nCodesave(results, file = \"out/tmm_sentiment_results.Rdata\")"
  },
  {
    "objectID": "demos/week08/wk08_sentiment-analysis.html#explore-sentiment-results",
    "href": "demos/week08/wk08_sentiment-analysis.html#explore-sentiment-results",
    "title": "Wk 08: Sentiment Analysis",
    "section": "Explore sentiment results",
    "text": "Explore sentiment results\n\nCode# Define the sentiment category you want to use (replace with the desired category)\nsentiment_category &lt;- \"nrc_trust\"\n\n# Extract the top 5 observations for the specified sentiment category\ntop_5_sentiment &lt;- results %&gt;%\n  arrange(desc({{sentiment_category}})) %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(letter_number, letter_text, {{sentiment_category}})\n\n# Print the top 5 observations\nprint(top_5_sentiment)\n\n# A tibble: 5 × 3\n  letter_number letter_text                                            nrc_trust\n          &lt;dbl&gt; &lt;chr&gt;                                                      &lt;int&gt;\n1             1 The proposal to renew expired mineral leases on the S…         6\n2             2 The proposal to renew expired mineral leases on the S…         6\n3             3 The proposal to renew expired mineral leases on the S…         6\n4             4 The renewal of these leases is arbitrary and done in …         1\n5             5 The proposal to renew expired mineral leases on the S…         6"
  },
  {
    "objectID": "demos/week08/wk08_sentiment-analysis.html#filter-out-repeat-letters-create-subsets",
    "href": "demos/week08/wk08_sentiment-analysis.html#filter-out-repeat-letters-create-subsets",
    "title": "Wk 08: Sentiment Analysis",
    "section": "Filter out repeat letters; create subsets",
    "text": "Filter out repeat letters; create subsets\n\nCode# Create a new df with unique 'letter_text' observations\nresults_filtered &lt;- results %&gt;%\n  distinct(letter_text, .keep_all = TRUE)\n\n# create subsets for exploration\nresults_anger &lt;- results_filtered %&gt;%\n  select(letter_number, letter_text, nrc_anger)\n\n\n\nCodelibrary(reactable)\n\n# create table (define columns, add parameters & formatting)\nreactable(results_anger, searchable = TRUE, filterable = TRUE)"
  },
  {
    "objectID": "demos/week09/wk09_tidy_networks.html",
    "href": "demos/week09/wk09_tidy_networks.html",
    "title": "Wk 09: Network Analysis",
    "section": "",
    "text": "Web of Science Queries (articles only)\n\nTechnical Communication Quarterly\nIEEE Transactions on Professional Communication\nJournal of Business and Technical Communication\nTechnical Communication\n\nOther resources:\n\nArticle that presents a tutorial for Bibliometrix"
  },
  {
    "objectID": "demos/week09/wk09_tidy_networks.html#overview-network-analysis",
    "href": "demos/week09/wk09_tidy_networks.html#overview-network-analysis",
    "title": "Wk 09: Network Analysis",
    "section": "",
    "text": "Web of Science Queries (articles only)\n\nTechnical Communication Quarterly\nIEEE Transactions on Professional Communication\nJournal of Business and Technical Communication\nTechnical Communication\n\nOther resources:\n\nArticle that presents a tutorial for Bibliometrix"
  },
  {
    "objectID": "demos/week09/wk09_tidy_networks.html#key-library-bibliometrix",
    "href": "demos/week09/wk09_tidy_networks.html#key-library-bibliometrix",
    "title": "Wk 09: Network Analysis",
    "section": "Key library: Bibliometrix",
    "text": "Key library: Bibliometrix\nBibliometrix\n\nCodelibrary(tidyverse)\nlibrary(janitor)\nlibrary(bibliometrix)\n\n\nTo use biblioshiny (Graphical interface built with RShiny)\n\nCode#library(bibliometrix)\n#biblioshiny()"
  },
  {
    "objectID": "demos/week09/wk09_tidy_networks.html#convert-.bib-to-dataframes",
    "href": "demos/week09/wk09_tidy_networks.html#convert-.bib-to-dataframes",
    "title": "Wk 09: Network Analysis",
    "section": "Convert .bib to dataframes",
    "text": "Convert .bib to dataframes\nSet file locations\n\nCodetcq &lt;- \"data/wos-tcq.bib\"\nieee &lt;- \"data/wos-ieee.bib\"\njbtc &lt;- \"data/wos-jbtc.bib\"\ntc &lt;- \"data/wos-tc.bib\"\n\n\nConvert each bib file to a dataframe\n\nCode#help(convert2df)\n\n\n# TCQ\ndf_tcq &lt;- convert2df(file = tcq,\n                  dbsource = \"wos\",\n                  format = \"bibtex\")\n\n\nConverting your wos collection into a bibliographic dataframe\n\n\nWarning:\nIn your file, some mandatory metadata are missing. Bibliometrix functions may not work properly!\n\nPlease, take a look at the vignettes:\n- 'Data Importing and Converting' (https://www.bibliometrix.org/vignettes/Data-Importing-and-Converting.html)\n- 'A brief introduction to bibliometrix' (https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html)\n\n\nMissing fields:  C1 \nDone!\n\nCode# IEEE\ndf_ieee &lt;- convert2df(file = ieee,\n                  dbsource = \"wos\",\n                  format = \"bibtex\")\n\n\nConverting your wos collection into a bibliographic dataframe\n\n\nWarning:\nIn your file, some mandatory metadata are missing. Bibliometrix functions may not work properly!\n\nPlease, take a look at the vignettes:\n- 'Data Importing and Converting' (https://www.bibliometrix.org/vignettes/Data-Importing-and-Converting.html)\n- 'A brief introduction to bibliometrix' (https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html)\n\n\nMissing fields:  C1 \nDone!\n\nCode# JBTC\ndf_jbtc &lt;- convert2df(file = jbtc,\n                  dbsource = \"wos\",\n                  format = \"bibtex\")\n\n\nConverting your wos collection into a bibliographic dataframe\n\n\nWarning:\nIn your file, some mandatory metadata are missing. Bibliometrix functions may not work properly!\n\nPlease, take a look at the vignettes:\n- 'Data Importing and Converting' (https://www.bibliometrix.org/vignettes/Data-Importing-and-Converting.html)\n- 'A brief introduction to bibliometrix' (https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html)\n\n\nMissing fields:  C1 \nDone!\n\nCode# TC\ndf_tc &lt;- convert2df(file = tc,\n                  dbsource = \"wos\",\n                  format = \"bibtex\")\n\n\nConverting your wos collection into a bibliographic dataframe\n\n\nWarning:\nIn your file, some mandatory metadata are missing. Bibliometrix functions may not work properly!\n\nPlease, take a look at the vignettes:\n- 'Data Importing and Converting' (https://www.bibliometrix.org/vignettes/Data-Importing-and-Converting.html)\n- 'A brief introduction to bibliometrix' (https://www.bibliometrix.org/vignettes/Introduction_to_bibliometrix.html)\n\n\nMissing fields:  C1 \nDone!\n\n\nCombine the resulting dataframes\n\nCodeM &lt;- bind_rows(\n  df_ieee,\n  df_jbtc,\n  df_tc,\n  df_tcq\n)\n\n#write_rds(M, \"data/comb.rds\")\n\n#install.packages(\"bib2df\")\n#library(bib2df)\n#df2bib(M, \"data/comb.bib\")\n\n#write_csv(M, \"data/comb.csv\")\n\n# get subset for analysis\n\nM &lt;- M %&gt;%\n  filter(PY &gt; 1992)\n\n\n\nCode# run a stock analysis (generates a list of dataframes)\nresults &lt;- biblioAnalysis(M, sep = \";\")\n\n# create a summary of the results\noptions(width=100)\nS &lt;- summary(object = results, k = 10, pause = FALSE)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              1994 : 2023 \n Sources (Journals, Books, etc)        4 \n Documents                             1866 \n Annual Growth Rate %                  8.76 \n Document Average Age                  11.8 \n Average citations per doc             12.12 \n Average citations per year per doc    1.022 \n References                            52452 \n \nDOCUMENT TYPES                     \n article                         1826 \n article; early access           30 \n article; proceedings paper      10 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    1476 \n Author's Keywords (DE)                4048 \n \nAUTHORS\n Authors                               2201 \n Author Appearances                    3370 \n Authors of single-authored docs       654 \n \nAUTHORS COLLABORATION\n Single-authored docs                  970 \n Documents per Author                  0.848 \n Co-Authors per Doc                    1.81 \n International co-authorships %        0 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    1994        7\n    1995       20\n    1996       17\n    1997       40\n    1998       42\n    1999       42\n    2000       51\n    2001       52\n    2002       40\n    2003       52\n    2004       46\n    2005       84\n    2006       63\n    2007       82\n    2008       76\n    2009       72\n    2010       80\n    2011       65\n    2012       72\n    2013       66\n    2014       60\n    2015       67\n    2016       73\n    2017       75\n    2018       81\n    2019       83\n    2020       80\n    2021      109\n    2022       89\n    2023       80\n\nAnnual Percentage Growth Rate 8.76 \n\n\nMost Productive Authors\n\n   Authors        Articles Authors        Articles Fractionalized\n1    CARLINER S         16 CARLINER S                       13.42\n2    DE JONG MDT        16 MACKIEWICZ J                     13.03\n3    MACKIEWICZ J       16 SWARTS J                         12.00\n4    SPINUZZI C         16 FRIESS E                         10.50\n5    LAM C              14 LAM C                            10.50\n6    MELONCON L         14 SPINUZZI C                        9.50\n7    FRIESS E           13 MELONCON L                        8.67\n8    SWARTS J           13 BATOVA T                          8.00\n9    WALTON R           12 MALONE EA                         7.50\n10   DE JONG M          11 VAN DER MEIJ H                    7.50\n\n\nTop manuscripts per citations\n\n                               Paper                                    DOI  TC TCperYear   NTC\n1  LOWRY PB, 2014, IEEE Trans. Prof. Commun.  10.1109/TPC.2014.2312452      964     96.40 33.18\n2  SPINUZZI C, 2005, Tech. Commun.            NA                            502     26.42 18.49\n3  SPINUZZI C, 2012, J. Bus. Tech. Commun.    10.1177/1050651912444070      339     28.25 16.72\n4  BOREN MT, 2000, IEEE Trans. Prof. Commun.  10.1109/47.867942             325     13.54 13.55\n5  KOCK N, 2005, IEEE Trans. Prof. Commun.    10.1109/TPC.2005.849649       227     11.95  8.36\n6  ROBERT LP, 2005, IEEE Trans. Prof. Commun. 10.1109/TPC.2004.843292       154      8.11  5.67\n7  RUPPEL CP, 2001, IEEE Trans. Prof. Commun. 10.1109/47.911131             147      6.39  6.41\n8  HAAS AM, 2012, J. Bus. Tech. Commun.       10.1177/1050651912439539      128     10.67  6.31\n9  ROBEY D, 2000, Tech. Commun.               10.1109/47.826416             126      5.25  5.25\n10 JONES NN, 2016, Tech. Commun. Q.-a         10.1080/10572252.2016.1224655 125     15.62 10.14\n\n\nMost Relevant Sources\n\n                                   Sources        Articles\n1 TECHNICAL COMMUNICATION                              545\n2 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION      460\n3 IEEE TRANSACTIONS ON PROFESSIONAL COMMUNICATION      454\n4 TECHNICAL COMMUNICATION QUARTERLY                    407\n\n\nMost Relevant Keywords\n\n   Author Keywords (DE)      Articles  Keywords-Plus (ID)     Articles\n1    COMMUNICATION                113 COMMUNICATION                165\n2    TECHNICAL COMMUNICATION       83 TECHNICAL COMMUNICATION       94\n3    COLLABORATION                 59 TECHNOLOGY                    81\n4    PEDAGOGY                      52 GENRE                         68\n5    USABILITY                     50 INFORMATION                   65\n6    ETHICS                        49 DESIGN                        61\n7    WRITING                       47 WORK                          58\n8    RHETORIC                      45 PERFORMANCE                   56\n9    SOCIAL MEDIA                  42 MODEL                         52\n10   SOCIAL JUSTICE                39 DISCOURSE                     42"
  },
  {
    "objectID": "demos/week09/wk09_tidy_networks.html#citation-analysis",
    "href": "demos/week09/wk09_tidy_networks.html#citation-analysis",
    "title": "Wk 09: Network Analysis",
    "section": "Citation analysis",
    "text": "Citation analysis\nMost cited articles\n\nCode# Get citations\nCR &lt;- citations(M, field = \"article\", sep = \";\")\n\n# Top 50 most cited articles\ncbind(CR$Cited[1:50])\n\n                                                                                                                                               [,1]\nANONYMOUS, TECHNICAL COMMUNICAT                                                                                                                 209\nMILLER CR, 1984, Q J SPEECH, V70, P151, DOI 10.1080/00335638409383686                                                                           118\nMILLER CR, 1979, COLL ENGL, V40, P610, DOI 10.2307/375964                                                                                        85\nJONES NN, 2016, TECH COMMUN Q, V25, P211, DOI 10.1080/10572252.2016.1224655                                                                      80\nJONES N. N., 2016, J TECH WRIT COMMUN, V46, P342, DOI DOI 10.1177/0047281616639472                                                               69\nRUSSELL DR, 1997, WRIT COMMUN, V14, P504, DOI 10.1177/0741088397014004004                                                                        66\nAGBOKA GY, 2013, TECH COMMUN Q, V22, P28, DOI 10.1080/10572252.2013.730966                                                                       62\nSWALES J, 1990, GENRE ANAL ENGLISH A                                                                                                             62\nHAAS AM, 2012, J BUS TECH COMMUN, V26, P277, DOI 10.1177/1050651912439539                                                                        61\nANONYMOUS, 1996, TECH COMMUN Q                                                                                                                   57\nRUDE CD, 2009, J BUS TECH COMMUN, V23, P174, DOI 10.1177/1050651908329562                                                                        56\nBAZERMAN CHARLES, 1988, SHAPING WRITTEN KNOW                                                                                                     53\nSPINUZZI C., 2003, TRACING GENRES ORG S                                                                                                          53\nBERKENKOTTER C., 1995, GENRE KNOWLEDGE DISC                                                                                                      52\nKATZ SB, 1992, COLL ENGL, V54, P255, DOI 10.2307/378062                                                                                          52\nSCHRIVER KA., 1997, DYNAMICS DOCUMENT DE                                                                                                         49\nANONYMOUS, 1998, TECH COMMUN Q, DOI 10.1080/10572259809364640, DOI 10.1080/10572259809364640                                                     48\nAGBOKA G. Y., 2014, J TECHNICAL WRITING, V44, P297                                                                                               44\nDAFT RL, 1986, MANAGE SCI, V32, P554, DOI 10.1287/MNSC.32.5.554                                                                                  42\nDRAGGA S, 2001, TECH COMMUN, V48, P265                                                                                                           42\nJOHNSON R. R., 1998, USER CTR TECHNOLOGY                                                                                                         42\nMILES M. B., 1994, QUALITATIVE DATA ANA                                                                                                          42\nANONYMOUS, NO TITLE CAPTURED                                                                                                                     41\nHOFT NANCY L, 1995, INT TECHNICAL COMMUN                                                                                                         41\nRAINEY KT, 2005, TECH COMMUN-STC, V52, P323                                                                                                      41\nANONYMOUS, 2000, SPURIOUS COIN HIST S                                                                                                            39\nKIMBALL MA, 2006, TECH COMMUN Q, V15, P67, DOI 10.1207/S15427625TCQ1501\\\\_6                                                                      38\nSPINUZZI C, 2007, TECH COMMUN Q, V16, P265, DOI 10.1080/10572250701290998                                                                        38\nHOFSTEDE G., 2001, CULTURES CONSEQUENCE                                                                                                          37\nSLACK JENNIFER DARYLL, 1993, J BUS TECH COMMUN, V7, P12, DOI 10.1177/1050651993007001002                                                         37\nANONYMOUS, 2004, TECH COMMUN Q                                                                                                                   36\nANONYMOUS, J TECHNICAL WRITING                                                                                                                   36\nWALTON R., 2019, TECHNICAL COMMUNICAT                                                                                                            36\nANONYMOUS, INTERCOM                                                                                                                              34\nJONES NN, 2018, KEY THEORETICAL FRAMEWORKS: TEACHING TECHNICAL COMMUNICATION IN THE TWENTY-FIRST CENTURY, P241, DOI 10.7330/9781607327585.C010   34\nSALDA├A┬▒A J., 2016, CODING MANUAL QUALIT                                                                                                        34\nSUN HT, 2006, TECH COMMUN Q, V15, P457, DOI 10.1207/S15427625TCQ1504\\\\_3                                                                         33\nFARKAS DK, 1999, TECH COMMUN, V46, P42                                                                                                           32\nANONYMOUS, J BUSINESS COMMUNICA                                                                                                                  31\nDURACK K. T., 1997, TECH COMMUN Q, V6, P249                                                                                                      31\nSPINUZZI C., 2008, NETWORK THEORIZING K                                                                                                          31\nYATES J, 1992, ACAD MANAGE REV, V17, P299, DOI 10.2307/258774                                                                                    31\nMISTRY ROHINTON, 2002, FAMILY MATTERS                                                                                                            30\nPARETTI MC, 2007, TECH COMMUN Q, V16, P327, DOI 10.1080/10572250701291087                                                                        29\nANONYMOUS, 2007, COMMUNICATIVE PRACTI                                                                                                            28\nANONYMOUS, PROFESSIONAL COMMUNI                                                                                                                  28\nBRUMBERGER E, 2015, TECH COMMUN-STC, V62, P224                                                                                                   28\nDAFT RL, 1987, MIS QUART, V11, P355, DOI 10.2307/248682                                                                                          28\nKOERBER A, 2006, TECH COMMUN Q, V15, P87, DOI 10.1207/S15427625TCQ1501\\\\_7                                                                       28\nSCHRIVER K, 1997, DYNAMICS DOCUMENT DE                                                                                                           28\n\n\nMost cited first authors\n\nCodeCR &lt;- citations(M, field = \"author\", sep = \";\")\n\ncbind(CR$Cited[1:25])\n\n                  [,1]\nANONYMOUS         7766\nSPINUZZI C         405\nMILLER CR          244\nNO TITLE CAPTURED  223\nJONES NN           191\nWALTON R           171\nCARLINER S         161\nHOFSTEDE G         143\nDRAGGA S           126\nWINSOR DA          123\nLATOUR B           119\nKOSTELNICK C       118\nMELONCON L         116\nBERKENKOTTER C     114\nNIELSEN J          111\nFREEDMAN A         109\nJONES N N          108\nBAZERMAN CHARLES   107\nVAN DER MEIJ H     107\nMACKIEWICZ J       102\nSWARTS J           102\nHAAS AM            101\nMIREL B             93\nTEBEAUX E           92\nKIMBALL MA          90\n\n\nLocal citations\nNot working :(\n\nCodeCR &lt;- localCitations(M, sep = \";\")\nCR$Authors[1:10, ]\n\n          Author LocalCitations\n1  [ANONYMOUS] A              0\n2       ABBOTT C              0\n3       ABBOTT L              0\n4        ABELL J              0\n5     ACHARYA KR              0\n6     ACKERMAN M              0\n7          ADA S              0\n8         ADAM C              0\n9        ADAMS A              0\n10      ADAMS AH              0\n\n\nNot working :(\n\nCodeCR$Papers[1:10,]\n\n                                       Paper                         DOI Year LCS GCS\n1    HOPKINS WE, 1994, J. Bus. Tech. Commun. 10.1177/1050651994008003005 1994   0   0\n2        ZAK MW, 1994, J. Bus. Tech. Commun. 10.1177/1050651994008003002 1994   0   7\n3  NICHOLSON JD, 1994, J. Bus. Tech. Commun. 10.1177/1050651994008003006 1994   0   0\n4     LIMAYE MR, 1994, J. Bus. Tech. Commun. 10.1177/1050651994008003007 1994   0   3\n5   GRIFFETH RW, 1994, J. Bus. Tech. Commun. 10.1177/1050651994008003003 1994   0   3\n6     KOSSEK EE, 1994, J. Bus. Tech. Commun. 10.1177/1050651994008003004 1994   0  12\n7   LIMAYE MR, 1994, J. Bus. Tech. Commun.-a 10.1177/1050651994008003001 1994   0   0\n8     THOMAS SG, 1995, J. Bus. Tech. Commun. 10.1177/1050651995009004004 1995   0   5\n9      GOLEN SP, 1995, J. Bus. Tech. Commun. 10.1177/1050651995009003003 1995   0   0\n10      MIREL B, 1995, J. Bus. Tech. Commun. 10.1177/1050651995009003001 1995   0   6\n\n\n\nCodeDF &lt;- dominance(results, k = 12)\nDF\n\n         Author Dominance Factor Tot Articles Single-Authored Multi-Authored First-Authored Rank by Articles Rank by DF\n1  MACKIEWICZ J        0.6000000           16              11              5              3                1          1\n2      FRIESS E        0.6000000           13               8              5              3                7          1\n3    SPINUZZI C        0.5555556           16               7              9              5                1          3\n4     GRAHAM SS        0.5555556           11               2              9              5                9          3\n5    CARLINER S        0.5000000           16              12              4              2                1          5\n6    MELONCON L        0.4444444           14               5              9              4                5          6\n7      WALTON R        0.4000000           12               2             10              4                8          7\n8     DE JONG M        0.3636364           11               0             11              4                9          8\n9     MALONE EA        0.3333333           11               5              6              2                9          9\n10        LAM C        0.2857143           14               7              7              2                5         10\n11  DE JONG MDT        0.2500000           16               0             16              4                1         11\n12   KARREMAN J        0.1818182           11               0             11              2                9         12\n\n\n\nCodetopAU &lt;- authorProdOverTime(M, k = 20, graph = TRUE)\n\n\n\n\n\nCodehead(topAU$dfAU)\n\n       Author year freq TC      TCpY\n1    BATOVA T 2010    1 11 0.7857143\n2    BATOVA T 2015    2 34 3.7777778\n3    BATOVA T 2018    3 25 4.1666667\n4    BATOVA T 2019    1  1 0.2000000\n5    BATOVA T 2021    2 13 4.3333333\n6 BOETTGER RK 2010    2 77 5.5000000"
  },
  {
    "objectID": "demos/week09/wk09_tidy_networks.html#co-citation",
    "href": "demos/week09/wk09_tidy_networks.html#co-citation",
    "title": "Wk 09: Network Analysis",
    "section": "Co-Citation",
    "text": "Co-Citation\n\nCodeNetMatrix &lt;- biblioNetwork(M, analysis = \"co-citation\", network = \"references\", sep = \";\")\n\n## Plot the network\nnet=networkPlot(NetMatrix,\n                n = 25, \n                Title = \"Co-Citation Network\", \n                type = \"fruchterman\", \n                size=T,\n                label.cex=TRUE,\n                label.color=TRUE,\n                halo=FALSE,\n                remove.multiple=FALSE, \n                labelsize=.7,\n                edgesize = 3,\n                cluster = \"none\",\n                community.repulsion = .3,\n                edges.min = 1)"
  }
]