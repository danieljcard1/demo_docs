{
  "hash": "9c5c0e7ae19fa764316eeb35cbfc8116",
  "result": {
    "markdown": "---\ntitle: \"Wk 03: Exploring TC journals, (pt 3)\"\n---\n\n\n## Overview\n\nWe start with data from four journals\n\n## Load libraries and data\n\nFor this analysis, we'll be using the R package [Quanteda: Quantitative Analysis of Textual Data](http://quanteda.io/index.html). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\n\n#install.packages(\"Rtools\") # you may need to install Rtools to install all the quanteda packages\n\n#install.packages(\"remotes\")\n\n#install.packages(\"quanteda\")\n#install.packages(\"readtext\")\n#install.packages(\"spacyr\")\n#install.packages(\"quanteda.textmodels\")\n#install.packages(\"quanteda.textstats\")\n#install.packages(\"quanteda.textplots\")\n#remotes::install_github(\"kbenoit/quanteda.dictionaries\")\n\nlibrary(quanteda)\nlibrary(quanteda.dictionaries)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)\n```\n:::\n\n\n\n## Load data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"here\")\nlibrary(here)\n\n# Use \"here\" to set the working directory \nhere::here()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"/Users/dcard/Documents/GitHub/computational rhetorics/demo_docs\"\n```\n:::\n\n```{.r .cell-code}\ngetwd()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"/Users/dcard/Documents/GitHub/computational rhetorics/demo_docs/demos/week03\"\n```\n:::\n\n```{.r .cell-code}\n# Use \"here\" to define the relative path to your data\ndata_file <- here(\"demos/week03/data_out/full_data.RData\")\n\nload(data_file)\n\nglimpse(full_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,537\nColumns: 9\n$ source_title          <chr> \"JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION…\n$ author_full_names     <chr> \"Wickman, Chad\", \"DeJeu, Emily Barrow\", \"DeVasto…\n$ article_title         <chr> \"Genre and Metagenre in Biomedical Research Writ…\n$ abstract              <chr> \"The use of reporting guidelines is an establish…\n$ cited_references      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ cited_reference_count <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ publication_year      <dbl> 2023, 2022, 2016, 2009, 2009, 2020, 2016, 2012, …\n$ publication_type      <chr> \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\", \"J\"…\n$ abbreviation          <chr> \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", \"JBTC\", …\n```\n:::\n:::\n\n\n## Creating a Corpus object\n\nA Quanteda Corpus is a special form of a character vector that includes metadata about the corpus and the \"documents\" within the corpus. In this case, our corpus includes all the articles results in our CSV and each article is a document. \n\n### Create a \"text\" column for analysis\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creates a new column in full_data from the title and abstract (separated by a tilde)\nfull_data$text <- paste(full_data$article_title, full_data$abstract, sep = \" ~ \")\n\n\n# check the column\nfull_data$text[2:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The Ethics of Delivering Bad News: Evaluating Impression Management Strategies in Corporate Financial Reporting ~ Business communication textbooks offer impression management (IM) strategies to help students learn how to soften bad news. But corporations sometimes use these strategies in ethically questionable ways. This article analyzes IM strategies in a landmark case of ethically dubious corporate financial reporting. Findings suggest that the company, Ivax, manipulated three standard IM strategies by overamplifying its power to fix a financial crisis, substantially downplaying bad news, and concealing damaging information. Ivax also used a fourth, less familiar strategy: It buried contradictory information in legal disclaimers. Instructors need to help students become ethical writers who avoid questionable IM strategies like these.\"                                                                                     \n[2] \"Stasis and Matters of Concern: The Conviction of the L'Aquila Seven ~ On October 22, 2012, six scientists and one civil servant were convicted of manslaughter for failing to properly warn the people of L'Aquila, Italy, of an impending earthquake that resulted in over 300 deaths and 1,500 injuries. This article investigates a key event leading up to this conviction: An emergency meeting of scientists, civil servants, and politicians to determine whether or not an advanced warning should be issued to the residents of L'Aquila. The following investigation of this emergency meeting uses functional stasis analysis to identify the primary breakdown in deliberation that ultimately led to a message of calm and reassurance immediately prior to the devastating earthquake. The results provide insights into not only the events in L'Aquila but also broader issues of risk, uncertainty, fact, and value in science-policy deliberation.\"\n```\n:::\n:::\n\n\n### Create the corpus\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# creates the corpus object\ncorp <- corpus(full_data)\n\n# summary of the corpus (including metadata for the texts)\nsummary(corp, n = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1537 documents, showing 3 documents:\n\n  Text Types Tokens Sentences                                    source_title\n text1    97    149         4 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n text2    94    130         6 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n text3   102    158         4 JOURNAL OF BUSINESS AND TECHNICAL COMMUNICATION\n                                       author_full_names\n                                           Wickman, Chad\n                                     DeJeu, Emily Barrow\n DeVasto, Danielle; Graham, S. Scott; Zamparutti, Louise\n                                                                                                   article_title\n                                                              Genre and Metagenre in Biomedical Research Writing\n The Ethics of Delivering Bad News: Evaluating Impression Management Strategies in Corporate Financial Reporting\n                                             Stasis and Matters of Concern: The Conviction of the L'Aquila Seven\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    abstract\n The use of reporting guidelines is an established yet still-evolving practice in the field of biomedicine. These documents are often linked to common methodologies (e.g., randomized clinical trials); they include multiple textual artifacts (e.g., checklists, flow diagrams) and have a history that is coextensive with the emergence and ongoing development of evidence-based medicine (e.g., as an epistemological orientation to research and decision making). Drawing on the concept of metagenre, this article examines how practitioners use reporting guidelines to define and regulate the boundaries of biomedical research and writing activity. The analysis, focusing on one prominent set of guidelines, shows how practitioners use the genre-metagenre dynamic to promote strategic intervention while upholding traditional principles and standards for evidence-based research and communication.\n                                                                                                                                                               Business communication textbooks offer impression management (IM) strategies to help students learn how to soften bad news. But corporations sometimes use these strategies in ethically questionable ways. This article analyzes IM strategies in a landmark case of ethically dubious corporate financial reporting. Findings suggest that the company, Ivax, manipulated three standard IM strategies by overamplifying its power to fix a financial crisis, substantially downplaying bad news, and concealing damaging information. Ivax also used a fourth, less familiar strategy: It buried contradictory information in legal disclaimers. Instructors need to help students become ethical writers who avoid questionable IM strategies like these.\n                              On October 22, 2012, six scientists and one civil servant were convicted of manslaughter for failing to properly warn the people of L'Aquila, Italy, of an impending earthquake that resulted in over 300 deaths and 1,500 injuries. This article investigates a key event leading up to this conviction: An emergency meeting of scientists, civil servants, and politicians to determine whether or not an advanced warning should be issued to the residents of L'Aquila. The following investigation of this emergency meeting uses functional stasis analysis to identify the primary breakdown in deliberation that ultimately led to a message of calm and reassurance immediately prior to the devastating earthquake. The results provide insights into not only the events in L'Aquila but also broader issues of risk, uncertainty, fact, and value in science-policy deliberation.\n cited_references cited_reference_count publication_year publication_type\n             <NA>                    NA             2023                J\n             <NA>                    NA             2022                J\n             <NA>                    NA             2016                J\n abbreviation\n         JBTC\n         JBTC\n         JBTC\n```\n:::\n\n```{.r .cell-code}\n# Prints texts\nprint(corp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 1,537 documents and 9 docvars.\ntext1 :\n\"Genre and Metagenre in Biomedical Research Writing ~ The use...\"\n\ntext2 :\n\"The Ethics of Delivering Bad News: Evaluating Impression Man...\"\n\ntext3 :\n\"Stasis and Matters of Concern: The Conviction of the L'Aquil...\"\n\ntext4 :\n\"Integrating Social Media Into Existing Work Environments The...\"\n\ntext5 :\n\"Practitioner Research Instruction A Neglected Curricular Are...\"\n\ntext6 :\n\"Legally Minded Technical Communicators: A Case Study of a Le...\"\n\n[ reached max_ndoc ... 1,531 more documents ]\n```\n:::\n:::\n\n\n### Plot descriptive statistics\n\n\n#### Plot metadata: tokens per text, by journal\n\n::: {.cell}\n\n```{.r .cell-code}\n# get metadata\ntokeninfo <- summary(corp, n = 1537)\n\n# plot\nif (require(ggplot2)) ggplot(data = tokeninfo, aes(x = publication_year, y = Tokens, group = abbreviation, color = abbreviation)) +\n  geom_point() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"Tokens per text over time\",\n  subtitle = \"Token counts by publication year\",\n  x = \"Publication Year\",\n  y = \"Tokens Count\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n#### Plot metadata: sentences per text, by journal\n\nWe can plot sentences in the title+abstract for each article (and color-coded by journal)\n\n::: {.cell}\n\n```{.r .cell-code}\n# how many unique sentence values are there?\nunique(tokeninfo$Sentences)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  4  6  3  5  7  1 10  2  8 15  9 11 12 14 13 17\n```\n:::\n\n```{.r .cell-code}\n# plot tokeninfo \nif (require(ggplot2)) ggplot(data = tokeninfo, aes(x = publication_year, y = Sentences, group = abbreviation, color = abbreviation)) +\n  geom_point() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"Sentences per text over time\",\n  subtitle = \"Sentence counts by publication year\",\n  x = \"Publication Year\",\n  y = \"Sentence Count\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nBut wait, where are all the JBTC articles?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get a list of unique sentence lengths in JBTC title+abstracts\nunique(tokeninfo$Sentences[tokeninfo$abbreviation == \"JBTC\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  4  6  3  5  7  1 10  2  8\n```\n:::\n\n```{.r .cell-code}\n# get a list of unique years among JBTC observations\nunique(tokeninfo$publication_year[tokeninfo$abbreviation == \"JBTC\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2023 2022 2016 2009 2020 2012 2011 2010 2007 2021 2006 2014 2008 2019 2018\n[16] 2015 2005 2013 2017\n```\n:::\n:::\n\nIt seems like we have JBTC articles of varying sentence length across multiple years...\n\n\n### Subset a corpus\n\nLet's investigate further. Use the `corpus_subset` function to keep only texts from JBTC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# corpus_subset(corp, abbreviation == \"JBTC\")\n\njbtc_tokeninfo <- summary(corpus_subset(corp, abbreviation == \"JBTC\"), n = 300)\n```\n:::\n\n\nThen visualize sentence counts again, this time for just JBTC...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (require(ggplot2)) ggplot(data = jbtc_tokeninfo, aes(x = publication_year, y = Sentences)) +\n  geom_point() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"JBTC: Sentences per text over time\",\n  subtitle = \"Sentence counts in JBTC\",\n  x = \"Publication Year\",\n  y = \"Sentence Count\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\nOk, so maybe it's just a problem with the plot?\n\n### Add a layer: # of observations with x sentences\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use geom_count instead of geom_point to let size reflect the count of the points\n  \nggplot(data = jbtc_tokeninfo, aes(x = publication_year, y = Sentences)) +\n  geom_count() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"JBTC: Sentences per text over time\",\n  subtitle = \"Sentence counts in JBTC\",\n  x = \"Publication Year\",\n  y = \"Sentence Count\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n### Add size to all journal sentence counts\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = tokeninfo, aes(x = publication_year, y = Sentences, group = abbreviation, color = abbreviation)) +\n  geom_count() + \n  scale_x_continuous(labels = c(seq(2005, 2023, 3)),\n    breaks = seq(2005, 2023, 3)) + \n  labs(\n  title = \"Sentences per text over time\",\n  subtitle = \"Sentence counts by publication year\",\n  x = \"Publication Year\",\n  y = \"Sentence Count\"\n  ) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\nIt's not perfect, but definitely better. \n\n## Exploring corpus texts\n\nBut token and sentence counts probably aren't the most interesting aspect of the titles and abstracts...\n\n### KWIC: search for patterns\n\nWe can search for patterns in multiple ways:\n\n* single word: `kwic(data_tokens, pattern = \"usability\")`\n* string of characters: `kwic(data_tokens, pattern = \"user-*\")`\n* phrase: `kwic(data_tokens, phrase(\"social justice\"))`\n\n\n#### KWIC for \"usability\"\n\nWe can search for usability and surrounding words. \n\n::: {.callout-note title=\"Quanteda Tokens object\"}\nTokens: Each element of a tokens object typically represents a single word or a term. However, tokens can also represent larger text units such as sentences or paragraphs, depending on the tokenization process applied.\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a tokens object\ndata_tokens <- tokens(corp)\n\n# data, pattern, number of tokens before and after \nkwic_usability <- kwic(data_tokens, pattern = \"usability\", 5) \n\n# display the first 10 matches\nkwic_usability[0:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKeyword-in-context with 10 matches.                                                                    \n  [text16, 102]           a practical influence on the | usability |\n   [text79, 97]           and technology; the cultural | usability |\n  [text126, 94]    answered questions about the tool's | usability |\n  [text165, 42]        transparency, learnability, and | usability |\n  [text165, 96] supported by task-based documentation, | usability |\n [text168, 117]             similar contexts: content, | usability |\n  [text264, 16]             Tracking as a Component of | Usability |\n   [text273, 6]               Listening to Students: A | Usability |\n  [text273, 79]             how students use feedback. | Usability |\n [text273, 107]              . This article reports on | usability |\n                                              \n of assembly instructions. This               \n research conducted and located accountability\n and communicative effectiveness, and         \n . Looking at questions asked                 \n problems were more prominent.                \n , and overall visual appeal                  \n and Sustainability ~ Framed around           \n Evaluation of Instructor Commentary ~        \n evaluation is ideally equipped for           \n testing of commentary provided to            \n```\n:::\n\n```{.r .cell-code}\n# display the last 6 matches\ntail(kwic_usability)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKeyword-in-context with 6 matches.                                                                 \n  [text1158, 6]      Revising the Online Classroom: | Usability |\n [text1158, 26]               by the authors to use | usability |\n [text1158, 59]   institutions can create their own | usability |\n [text1190, 41]         format. It investigates the | usability |\n [text1190, 62]          as YouTube analytics data, | usability |\n [text1306, 70] of argument effectiveness, document | usability |\n                                         \n Testing for Training Online Technical   \n testing as a component of               \n testing protocols for formative online  \n and design-implications of a live-action\n , and comprehension assessments.        \n , and professionalism. Three            \n```\n:::\n\n```{.r .cell-code}\n# chart using kableExtra (for markdown to html version)\nlibrary(kableExtra)\nhead(kwic_usability) %>%\n  kbl() %>%\n  kable_minimal()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-minimal\" style='font-family: \"Trebuchet MS\", verdana, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> docname </th>\n   <th style=\"text-align:right;\"> from </th>\n   <th style=\"text-align:right;\"> to </th>\n   <th style=\"text-align:left;\"> pre </th>\n   <th style=\"text-align:left;\"> keyword </th>\n   <th style=\"text-align:left;\"> post </th>\n   <th style=\"text-align:left;\"> pattern </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> text16 </td>\n   <td style=\"text-align:right;\"> 102 </td>\n   <td style=\"text-align:right;\"> 102 </td>\n   <td style=\"text-align:left;\"> a practical influence on the </td>\n   <td style=\"text-align:left;\"> usability </td>\n   <td style=\"text-align:left;\"> of assembly instructions . This </td>\n   <td style=\"text-align:left;\"> usability </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> text79 </td>\n   <td style=\"text-align:right;\"> 97 </td>\n   <td style=\"text-align:right;\"> 97 </td>\n   <td style=\"text-align:left;\"> and technology ; the cultural </td>\n   <td style=\"text-align:left;\"> usability </td>\n   <td style=\"text-align:left;\"> research conducted and located accountability </td>\n   <td style=\"text-align:left;\"> usability </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> text126 </td>\n   <td style=\"text-align:right;\"> 94 </td>\n   <td style=\"text-align:right;\"> 94 </td>\n   <td style=\"text-align:left;\"> answered questions about the tool's </td>\n   <td style=\"text-align:left;\"> usability </td>\n   <td style=\"text-align:left;\"> and communicative effectiveness , and </td>\n   <td style=\"text-align:left;\"> usability </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> text165 </td>\n   <td style=\"text-align:right;\"> 42 </td>\n   <td style=\"text-align:right;\"> 42 </td>\n   <td style=\"text-align:left;\"> transparency , learnability , and </td>\n   <td style=\"text-align:left;\"> usability </td>\n   <td style=\"text-align:left;\"> . Looking at questions asked </td>\n   <td style=\"text-align:left;\"> usability </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> text165 </td>\n   <td style=\"text-align:right;\"> 96 </td>\n   <td style=\"text-align:right;\"> 96 </td>\n   <td style=\"text-align:left;\"> supported by task-based documentation , </td>\n   <td style=\"text-align:left;\"> usability </td>\n   <td style=\"text-align:left;\"> problems were more prominent . </td>\n   <td style=\"text-align:left;\"> usability </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> text168 </td>\n   <td style=\"text-align:right;\"> 117 </td>\n   <td style=\"text-align:right;\"> 117 </td>\n   <td style=\"text-align:left;\"> similar contexts : content , </td>\n   <td style=\"text-align:left;\"> usability </td>\n   <td style=\"text-align:left;\"> , and overall visual appeal </td>\n   <td style=\"text-align:left;\"> usability </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n#### KWIC for user-x\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkwic_userx <- kwic(data_tokens, pattern = \"user-*\", 3)\n\nhead(kwic_userx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKeyword-in-context with 6 matches.                                                       \n  [text4, 129] and repurposing their | user-generated |\n  [text37, 61]    tagging to compile | user-specific  |\n [text49, 119]  emphasis on creating | user-centered  |\n [text172, 51]  well acquainted with | user-centered  |\n [text172, 73] data collected within | user-centered  |\n [text190, 33]       as interactive, | user-generated |\n                            \n data.                      \n metadata on information    \n risk information that      \n design ( UCD               \n research and instead       \n documentation and describes\n```\n:::\n:::\n\n\n#### KWIC for \"social justice\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# show context of the first six occurrences of 'social justice'\nkwic(data_tokens, pattern = phrase(\"social justice\")) %>%\n    head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKeyword-in-context with 6 matches.                                                                            \n  [text25, 131:132]           It encourages a response to | social justice |\n   [text103, 12:13]   Feminist Scholarship Can Inform the | Social Justice |\n   [text103, 33:34]                    do, and can inform | social justice |\n   [text103, 45:46]            communication ( TPC ) even | social justice |\n   [text103, 68:69]           that are relevant to future | social justice |\n [text103, 124:125] methodologies and theories to enhance | social justice |\n                                       \n exigencies, invites participation from\n Turn ~ This article calls             \n work in technical and professional    \n work that is not explicitly           \n work: ( a )                           \n scholarship.                          \n```\n:::\n:::\n\n\n\n\n### The tokens object\n\nLet's dig into the tokens object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokens <- tokens(corp)\n\ntokens[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 3 documents and 9 docvars.\ntext1 :\n [1] \"Genre\"      \"and\"        \"Metagenre\"  \"in\"         \"Biomedical\"\n [6] \"Research\"   \"Writing\"    \"~\"          \"The\"        \"use\"       \n[11] \"of\"         \"reporting\" \n[ ... and 137 more ]\n\ntext2 :\n [1] \"The\"        \"Ethics\"     \"of\"         \"Delivering\" \"Bad\"       \n [6] \"News\"       \":\"          \"Evaluating\" \"Impression\" \"Management\"\n[11] \"Strategies\" \"in\"        \n[ ... and 118 more ]\n\ntext3 :\n [1] \"Stasis\"     \"and\"        \"Matters\"    \"of\"         \"Concern\"   \n [6] \":\"          \"The\"        \"Conviction\" \"of\"         \"the\"       \n[11] \"L'Aquila\"   \"Seven\"     \n[ ... and 146 more ]\n```\n:::\n:::\n\n\nNotice what counts as a token by default.\n\n### Preprocessing\n\nWe may want to remove certain words or characters that aren't salient for our analysis\n\n\n#### Remove punctuation, separators, and numbers\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a tokens object without punctuation, separators, and numbers\ntokens <- tokens(corp, remove_punct = TRUE,\n                 remove_separators = TRUE,\n                 remove_numbers = TRUE)\n\n# check the result\ntokens[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 3 documents and 9 docvars.\ntext1 :\n [1] \"Genre\"      \"and\"        \"Metagenre\"  \"in\"         \"Biomedical\"\n [6] \"Research\"   \"Writing\"    \"~\"          \"The\"        \"use\"       \n[11] \"of\"         \"reporting\" \n[ ... and 116 more ]\n\ntext2 :\n [1] \"The\"        \"Ethics\"     \"of\"         \"Delivering\" \"Bad\"       \n [6] \"News\"       \"Evaluating\" \"Impression\" \"Management\" \"Strategies\"\n[11] \"in\"         \"Corporate\" \n[ ... and 103 more ]\n\ntext3 :\n [1] \"Stasis\"     \"and\"        \"Matters\"    \"of\"         \"Concern\"   \n [6] \"The\"        \"Conviction\" \"of\"         \"the\"        \"L'Aquila\"  \n[11] \"Seven\"      \"~\"         \n[ ... and 127 more ]\n```\n:::\n:::\n\n\n#### Remove stopwords and more\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# see list of stopwords\nhead(stopwords(\"en\"), 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\"        \n [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\"      \n[11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\"       \n```\n:::\n\n```{.r .cell-code}\n# remove stopwords\ntokens <- tokens %>%\n  tokens_remove(stopwords(\"en\")) %>%\n  tokens_remove(\"~\")\n\n# check the result\ntokens[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens consisting of 3 documents and 9 docvars.\ntext1 :\n [1] \"Genre\"          \"Metagenre\"      \"Biomedical\"     \"Research\"      \n [5] \"Writing\"        \"use\"            \"reporting\"      \"guidelines\"    \n [9] \"established\"    \"yet\"            \"still-evolving\" \"practice\"      \n[ ... and 69 more ]\n\ntext2 :\n [1] \"Ethics\"     \"Delivering\" \"Bad\"        \"News\"       \"Evaluating\"\n [6] \"Impression\" \"Management\" \"Strategies\" \"Corporate\"  \"Financial\" \n[11] \"Reporting\"  \"Business\"  \n[ ... and 76 more ]\n\ntext3 :\n [1] \"Stasis\"     \"Matters\"    \"Concern\"    \"Conviction\" \"L'Aquila\"  \n [6] \"Seven\"      \"October\"    \"six\"        \"scientists\" \"one\"       \n[11] \"civil\"      \"servant\"   \n[ ... and 67 more ]\n```\n:::\n:::\n\n\n\n### Creating a document-feature matrix\n\nQuanteda uses a data structure called a document-feature matrix:\n\n* Document: In a DFM, each row represents a document or text unit. This can be a single document, a sentence, a paragraph, or any other defined text unit. In our case, it's a title + abstract.\n\n* Feature: Each column represents a feature, typically a word or a term that appears in the documents. These features are usually extracted from the text through tokenization, and they can be single words or multi-word phrases.\n\n* Matrix: The DFM is a two-dimensional matrix where the rows correspond to documents, and the columns correspond to features. The values in the matrix represent the frequency of each feature in each document, but they can also be transformed into other measures such as term frequency-inverse document frequency (TF-IDF) scores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create the dfm\ndfm <- tokens(corp, remove_punct = TRUE,\n              remove_separators = TRUE,\n              remove_numbers = TRUE) %>%\n  tokens_remove(stopwords(\"en\")) %>%\n  tokens_remove(\"~\") %>%\n  dfm()\n\n# view the dfm\nprint(dfm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 1,537 documents, 12,399 features (99.56% sparse) and 9 docvars.\n       features\ndocs    genre metagenre biomedical research writing use reporting guidelines\n  text1     1         2          2        4       2   3         2          3\n  text2     0         0          0        0       0   1         2          0\n  text3     0         0          0        0       0   0         0          0\n  text4     0         0          0        1       0   2         0          0\n  text5     0         0          0        7       0   0         0          0\n  text6     0         0          0        0       3   0         0          0\n       features\ndocs    established yet\n  text1           1   1\n  text2           0   0\n  text3           0   0\n  text4           0   0\n  text5           0   1\n  text6           0   0\n[ reached max_ndoc ... 1,531 more documents, reached max_nfeat ... 12,389 more features ]\n```\n:::\n:::\n\n\n\n::: {.callout-note title=\"Sparse matrices\"}\nIn this context, \"sparse\" refers to a type of data structure used to efficiently work with large data. In a sparse matrix, the majority of the elements have a value of zero. If your dfm is 99.56% sparse, it means that only .04% of the entries are something other than 0.  \n:::\n\n\n#### Create a wordcloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simple wordcloud\ntextplot_wordcloud(dfm)\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# wordcloud with parameters\nset.seed(100)\ntextplot_wordcloud(dfm, \n                   min_count = 20, # include word only if it occurs at least n times in data set \n                   random_order = FALSE, \n                   rotation = 0.25,\n    color = RColorBrewer::brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n\n\n#### View top features\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 20 most frequent words\ntopfeatures(dfm, 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    technical communication       writing         study       article \n         1546          1475          1377           960           913 \n     students      research      analysis       results           can \n          772           751           577           485           483 \n professional           use        design    rhetorical          work \n          468           457           453           445           427 \n       social       content          data     practices         using \n          412           388           386           360           356 \n```\n:::\n:::\n\n\n### DFM: Grouping by variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# group the dfm by a variable (docvar): journal\ndfm_journ <- dfm %>%\n  dfm_group(groups = abbreviation)\n\n# sort features by frequency and then view\ndfm_sort(dfm_journ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument-feature matrix of: 4 documents, 12,399 features (54.05% sparse) and 3 docvars.\n      features\ndocs   technical communication writing study article students research analysis\n  JBTC       244           375     158   236     212      131      119      102\n  TC         716           508     102   225     144      127      192      157\n  TCQ        561           531     149   175     297      154      183      110\n  WC          25            61     968   324     260      360      257      208\n      features\ndocs   results can\n  JBTC      84 107\n  TC       270 193\n  TCQ       39  95\n  WC        92  88\n[ reached max_nfeat ... 12,389 more features ]\n```\n:::\n:::\n\n\n#### Create a comparison cloud\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cloud that compares top features for each journal\ncomparison_cloud <- dfm_journ %>%\n  dfm_trim(min_termfreq = 25,\n           verbose = FALSE) %>%\n  textplot_wordcloud(comparison = TRUE)\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nIt's common joke in computational humanities and social scientists that \"colleagues don't let colleagues make wordclouds,\" but maybe this one can help us generate or refine some RQs? \n\n#### Plot relative frequencies\n\nQuanteda's `texstat_frequency` allows to plot the most frequent words in terms of relative frequency by group\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#help(\"dfm_weight\")\n#help(\"textstat_frequency\")\n\n# calculate the proportional weight (the proportion of the feature count relative to total feature count)\ndfm_weight_journ <- dfm_journ %>%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by journal\nfreq_weight <- textstat_frequency(dfm_weight_journ, n = 15, \n                                  groups = dfm_weight_journ$abbreviation)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n     geom_point() +\n     facet_wrap(~ group, scales = \"free\") +\n     coord_flip() +\n     scale_x_continuous(breaks = nrow(freq_weight):1,\n                        labels = freq_weight$feature) +\n     labs(x = NULL, \n          y = \"Relative frequency\",\n          title = \"Most frequent terms as a proportion of terms in the journal\")\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nIt seems like some of these terms are parts of phrases, no?\n\n\n## More, hastily\n\n### Plot \"keyness\" in TCQ and TC\n\nKeyness is a score for top features that occur differentially across categories or groups. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get info on \"keyness\"\nhelp(\"textstat_keyness\")\n\n\n# Subset initial corpus to retain TCQ and TC \ntc_v_tcq_corpus <- corpus_subset(corp, \n                            abbreviation %in% c(\"TCQ\", \"TC\"))\n\n# Create a dfm grouped by journal (abbreviation)\ntc_v_tcq_dfm <- tokens(tc_v_tcq_corpus, remove_punct = TRUE) %>%\n  tokens_remove(stopwords(\"english\")) %>%\n  tokens_remove(\"~\") %>%\n  tokens_group(groups = abbreviation) %>%\n  dfm()\n\n# Calculate keyness and determine TC as target group\nresult_keyness <- textstat_keyness(tc_v_tcq_dfm, target = \"TC\")\n\n# Plot estimated word keyness\ntextplot_keyness(result_keyness) \n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n### Ngrams\n\nYou can generate n-grams from a tokens object using `tokens_ngrams()`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# creating a tokens object from a corpus\ntoks <- tokens(corp, remove_punct = TRUE,\n              remove_separators = TRUE,\n              remove_numbers = TRUE) %>%\n  tokens_remove(stopwords(\"en\")) %>%\n  tokens_remove(\"~\")\n\n\n# generating ngrams from a tokens object\n\ntoks_ngram <- tokens_ngrams(toks, n = 2) # specify combinations, e.g. 2 and 3 word combos -> (toks, n = 2:3)\n\n\n# view result: first 10 ngrams in the first article\nhead(toks_ngram[[1]], 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Genre_Metagenre\"        \"Metagenre_Biomedical\"   \"Biomedical_Research\"   \n [4] \"Research_Writing\"       \"Writing_use\"            \"use_reporting\"         \n [7] \"reporting_guidelines\"   \"guidelines_established\" \"established_yet\"       \n[10] \"yet_still-evolving\"    \n```\n:::\n\n```{.r .cell-code}\n# view result: last 10 ngrams in the first article\ntail(toks_ngram[[1]], 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"dynamic_promote\"          \"promote_strategic\"       \n [3] \"strategic_intervention\"   \"intervention_upholding\"  \n [5] \"upholding_traditional\"    \"traditional_principles\"  \n [7] \"principles_standards\"     \"standards_evidence-based\"\n [9] \"evidence-based_research\"  \"research_communication\"  \n```\n:::\n:::\n\n\n\n#### DFM with bigrams\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create dfm from bigrams tokens object and group by journal \ndfm_bigrams <- toks_ngram %>%\n  dfm() %>%\n  dfm_group(groups = abbreviation)\n\n# another wordcloud...why not!\nset.seed(101)\ntextplot_wordcloud(dfm_bigrams, \n                   min_count = 20, # include word only if it occurs at least n times in data set \n                   random_order = FALSE, \n                   rotation = 0.25,\n    color = RColorBrewer::brewer.pal(8, \"Dark2\"))\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# cloud that compares top bigrams for each journal\ndfm_bigrams %>%\n  dfm_trim(min_termfreq = 15,\n           verbose = FALSE) %>%\n  textplot_wordcloud(comparison = TRUE)\n```\n\n::: {.cell-output-display}\n![](wk03_text_analysis_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "wk03_text_analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}