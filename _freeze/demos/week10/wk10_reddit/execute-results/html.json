{
  "hash": "d8d8e3c7af52293271b41eb77d0559a7",
  "result": {
    "markdown": "---\ntitle: \"Wk 10: Reddit API\"\nexecute:\n  eval: false\n---\n\n\n## Overview\n\nUsing RedditExtractoR to collect data from the reddit API.\n\n### Reddit API\n\n### RedditExtractoR\n\n-   [RedditExtractoR documentation on Github](https://github.com/ivan-rivera/RedditExtractor)\n\n### Additional Resources\n\n-   [Youtube tutorial on RedditExtractoR (16min, James Cook)](https://www.youtube.com/watch?v=Snm0Azfi_hc&t=17s)\n\n## Collecting data\n\n### libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n```\n:::\n\n\n### Get thread metadata\n\nSome key arguments in find_thread_urls:\n\n-   **subreddit.** This allows us to find the thread URLs for a particular subreddit. Alternatively, we could have used `keywords = \"keyword\"` to get posts by keyword instead of subreddit.\n-   **sort_by.** Here we designate the sorting method we want to apply to our search.\n    -   keyword search sort options: relevance, comments, new, hot, top\n    -   non-keyword search sort options: hot, new, top, rising\n-   **period.** Timeframe of results. Options: hour, day, week, month, year, all\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_threads <- find_thread_urls(\n  subreddit = \"Professors\",\n  sort_by = \"new\",\n  period = \"week\"\n)\n\nglimpse(my_threads)\n```\n:::\n\n\n### Get thread comments\n\n::: callout-warning\n`get_thread_content()` can take a LONG time, depending on the number of URLs and the size of the corresponding threads.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_comments <- get_thread_content(\n  my_threads$url\n)\n\nglimpse(my_comments)\n```\n:::\n\n\nThe reddit API will limit how many comments we can grab from each thread. Let's check how many comments we have in our threads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(ggthemes)\n\n# Create a histogram\nggplot(my_threads, aes(x = comments)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribution of Number of Comments\", x = \"Number of Comments\", y = \"Frequency\") +\n  theme_minimal()\n```\n:::\n\n\n### Join my_threads and my_comments$comments\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_both <- my_threads %>%\n  left_join(my_comments$comments, by = \"url\")\n\nglimpse(my_both)\n```\n:::\n\n\n### trying more URLs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_threads_all <- find_thread_urls(\n  subreddit = \"Professors\",\n  sort_by = \"new\",\n  period = \"all\"\n)\n\nglimpse(my_threads_all)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_threads_profgpt <- find_thread_urls(\n  subreddit = \"Professors\",\n  keywords = \"chatgpt\",\n  sort_by = \"new\",\n  period = \"all\"\n)\n\nglimpse(my_threads_profsgpt)\n```\n:::\n\n\n### Finding subreddits\n\nIf we want multiple subreddits, there's a function called `find_subreddits()` that allows us to search for relevant subreddits based on keywords.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhelp(\"find_subreddits\")\nmy_subreddits <- find_subreddits(\"food stamps\")\n\nglimpse(my_subreddits)\n```\n:::\n\n\nWe can get a list of the subreddit names that the keyword search turned up:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_subreddits$subreddit\n```\n:::\n\n\nWe could in theory perform `find_thread_urls()` on all of them, but we'll just target a few:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget_subreddits <- c(\"Conservative\",\n                       \"Libertarian\",\n                       \"progressive\",\n                       \"democrats\",\n                       \"Liberal\")\ntarget_subreddits\n```\n:::\n\n\n## Getting a big dataset\n\nMaybe we can abide by the API limits and still get a pretty sizeable dataset? \n\n### Using purrr to `find_thread_urls`\n\nHere, we use the map function from the tidyverse library purr to politely iterate over a list of subreddits\n\nIn the code below, we:\n\n1. define a list of subreddits\n2. create a function that:\n  1. performs `find_thread_urls` on a target subreddit (with keywords, sort, and period parameters)\n  2. Prints to the console what subreddit the function is working on\n  3. defines an amount of time between 3 and 7 seconds and then waits that amount of time before proceeding to the next subreddit\n  4. returns the result\n3. we use purrr::map to apply that function to each subreddit in our list of target subreddits\n4. finally, we combine our results for each subreddit into a single dataframe\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\n\n# Define the vector of subreddit names\ntarget_subreddits <- c(\"Conservative\", \"Libertarian\", \"progressive\", \"democrats\", \"Liberal\")\n\n# Initialize an empty list to store the results\nresults_list <- list()\n\n# Define the function to process a single subreddit \nprocess_subreddit <- function(subreddit) {\n  \n  find_threads <- find_thread_urls(\n    subreddit = subreddit,\n    keywords = \"food stamps\",\n    sort_by = \"new\",\n    period = \"all\"\n  )\n  \n  cat(\"Processing subreddit:\", subreddit, \"\\n\")\n  \n  # Generate a random sleep duration between 3 and 7 seconds\n  sleep_duration <- runif(1, min = 6, max = 7)\n  cat(\"Sleeping for\", sleep_duration, \"seconds\\n\")\n  Sys.sleep(sleep_duration)\n  \n  # Return the result\n  return(find_threads)\n}\n\n# Use purrr::map to apply the function to each subreddit name\nresults_list <- map(target_subreddits, process_subreddit)\n\n# Combine the results into a single dataframe\ntarget_results <- bind_rows(results_list)\n```\n:::\n\n\nNow target_results contains the results for all our target subreddits in a single dataframe. We can save these out for later use, if needed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save out as needed\n#write_csv(target_results, \"data/food_stamps/ideology_foodstamps.csv\")\n#saveRDS(target_results, \"data/food_stamps/ideology_foodstamps.rds\")\n```\n:::\n\n\n\nNow we can visualize the results to see how many threads we collected from each subreddit in our list\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the total number of threads for each subreddit\nsubreddit_totals <- target_results %>%\n  group_by(subreddit) %>%\n  summarise(total_threads = n())\n\n# Create a bar plot with totals\nggplot(subreddit_totals, aes(x = subreddit, y = total_threads)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = total_threads), vjust = -0.5, size = 3) +\n  labs(x = \"Subreddit\", y = \"Number of Threads\") +\n  ggtitle(\"Number of Threads Collected from Each Subreddit\") +\n  theme_minimal()\n```\n:::\n\n\n\n\n### Using Purrr to `get_thread_content`\n\nIf you need the data and have the time to collect it, we can use purr again. Instead of collecting all the threads for a given subreddit, pausing, and moving on to the next subreddit, this time we'll collect all the comments for a thread, pause, and then move on to the next thread. \n\n::: callout-warning\n`get_thread_content()` can take a LONG time, depending on the number of URLs and the size of the corresponding threads.\n:::\n\nJust how big are the threads we're about to collect?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a histogram\nggplot(target_results, aes(x = comments)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribution of Number of Comments\", x = \"Number of Thread Comments\", y = \"Frequency\") +\n  theme_minimal()\n```\n:::\n\n\nAgain, this next one would take a LONG time, e.g. over two hours for me to grab the comments for 835 threads. Instead, we'll start by testing on a subset.\n\n## Subset the data for testing\n\n### Generate a sample threads list\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # Set a seed for reproducibility\n\n\n# Sample a specific number of rows from the data frame (e.g., 2%)\nsample_size <- 0.02 * nrow(target_results)\n\n# Create a new data frame with the random sample\ntr_sample <- target_results %>%\n  sample_n(size = sample_size, replace = FALSE)\n\nglimpse(tr_sample)\n```\n:::\n\n\n### Define the function to collect comments\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the function to collect comments for a given URL\ncollect_comments <- function(url) {\n  \n  # Call the get_thread_content function with the URL\n  content <- get_thread_content(url)\n  \n  # Check if content$comments is empty or NULL, and skip if it is\n  if (is.null(content$comments) || nrow(content$comments) == 0) {\n    cat(\"No comments found for URL:\", url, \"\\n\")\n    return(NULL)\n  }\n  \n  # Add the \"thread_url\" column with the URL value to content$comments\n  content$comments <- content$comments %>% mutate(thread_url = url)\n  \n  # Convert \"comment_id\" to character to ensure consistent data types\n  content$comments <- content$comments %>% mutate(comment_id = as.character(comment_id))\n  \n  # Introduce a random pause between 6-7 seconds\n  sleep_duration <- runif(1, min = 6, max = 7)\n  Sys.sleep(sleep_duration)\n  \n  # Return content$comments\n  return(content$comments)\n}\n\n# Use purrr::map to apply the function to each URL with a progress bar\nsamp_comments_list <- map(tr_sample$url, collect_comments, .progress = \"getting thread comments\")\n\n# Filter out NULL elements (URLs with no comments)\nsamp_comments_list <- samp_comments_list[!sapply(samp_comments_list, is.null)]\n\n# Combine all the comments data frames into a single data frame\ncombined_comments <- bind_rows(samp_comments_list)\n\n# Now, combined_comments contains all the comments from different URLs\n\n# Filter out NULL elements (URLs with no comments)\nsamp_comments_list <- samp_comments_list[!sapply(samp_comments_list, is.null)]\n\n# Combine all the comments data frames into a single data frame\ncombined_comments <- bind_rows(samp_comments_list)\n\n# Now, combined_comments contains all the comments from different URLs\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the function to collect comments for a given URL\ncollect_comments <- function(url) {\n  \n  # Call the get_thread_content function with the URL\n  content <- get_thread_content(url)\n  \n  \n  # Check if content$comments is empty or NULL, and skip if it is\n  if (is.null(content$comments) || nrow(content$comments) == 0) {\n    cat(\"No comments found for URL:\", url, \"\\n\")\n    return(NULL)\n  }\n  \n  # Add the \"thread_url\" column with the URL value\n  content$threads <- content$threads %>% mutate(thread_url = url)\n  content$comments <- content$comments %>% mutate(thread_url = url)\n  \n   # Perform an inner join based on the \"thread_url\" column\n  combined_data <- content$threads %>%\n    left_join(content$comments, by = \"thread_url\")\n  \n  # Generate a filename based on the \"thread_url\"\n  filename <- gsub(\"[^[:alnum:].]\", \"_\", content$threads$thread_url)  # Clean the URL for a valid filename\n  \n  # Save the combined data as an RDS file with the thread_url as the filename\n  filename_with_extension <- paste0(filename, \".rds\")\n  saveRDS(combined_data, file.path(\"output\", filename_with_extension))\n  \n  # Introduce a random pause between 6-7 seconds\n  sleep_duration <- runif(1, min = 6, max = 7)\n  Sys.sleep(sleep_duration)\n  \n  \n  # Return the combined data frame\n  return(combined_data)\n\n}\n\n# Use purrr::map to apply the function to each URL with a progress bar\nsamp_comments_list <- map(tr_sample$url, collect_comments, .progress = \"getting thread comments\")\n\n# Filter out NULL elements (URLs with no comments)\nsamp_comments_list_nonull <- samp_comments_list[!sapply(samp_comments_list, is.null)]\n\n# Combine all the results into a single data frame\ncombined_data_sample <- bind_rows(samp_comments_list_nonull)\n\n\nwrite_rds(samp_comments_list, \"data/food_stamps_sample.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set your output directory\noutput_directory <- \"output\"\n\n# List all RDS files in the output directory\nrds_files <- list.files(output_directory, pattern = \"\\\\.rds$\", full.names = TRUE)\n\n# Initialize an empty list to store the data frames\ndata_list <- list()\n\n# Loop through the RDS files and read them into data frames\nfor (file in rds_files) {\n  data <- readRDS(file)\n  \n  # Convert \"comment_id\" to character\n  data <- data %>% mutate(comment_id = as.character(comment_id))\n  \n  data_list <- c(data_list, list(data))\n}\n\n# Combine all the data frames into a single data frame\ncombined_data <- bind_rows(data_list)\n\n# Now, combined_data contains all the data from the RDS files in the output_directory\nIn this modified code, we use mutate and as.character to ensure that the \"comment_id\" column is consistently of type character in all data frames before combining them with bind_rows. This should resolve the data type mismatch error.\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}